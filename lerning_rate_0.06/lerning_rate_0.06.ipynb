{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637c7bf8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f59e76",
   "metadata": {},
   "source": [
    "## Import the pandas library and perform data reading for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63a6dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', sep='\\t', header=None)\n",
    "test = pd.read_csv('test.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9137ac",
   "metadata": {},
   "source": [
    "## Adding column names to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f90815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = [\"text\",'labels']\n",
    "test.columns = [\"text\",'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbef43",
   "metadata": {},
   "source": [
    "## The first 10 texts of the training set are read in the format \"text, labels\". The labels may contain multiple sentiment categories, each sentiment is separated by ','."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f350e950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>8,20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0  My favourite food is anything I didn't have to...     27\n",
       "1  Now if he does off himself, everyone will thin...     27\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING      2\n",
       "3                        To make her feel threatened     14\n",
       "4                             Dirty Southern Wankers      3\n",
       "5  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...     26\n",
       "6  Yes I heard abt the f bombs! That has to be wh...     15\n",
       "7  We need more boards and to create a bit more s...   8,20\n",
       "8  Damn youtube and outrage drama is super lucrat...      0\n",
       "9  It might be linked to the trust factor of your...     27"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76da08ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m really sorry about your situation :( Altho...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's wonderful because it's awful. At not with.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kings fan here, good luck to you guys! Will be...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didn't know that, thank you for teaching me ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They got bored from haunting earth for thousan...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you for asking questions and recognizing...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You’re welcome</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100%! Congrats on your job too!</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I’m sorry to hear that friend :(. It’s for the...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Girlfriend weak as well, that jump was pathetic.</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0  I’m really sorry about your situation :( Altho...     25\n",
       "1    It's wonderful because it's awful. At not with.      0\n",
       "2  Kings fan here, good luck to you guys! Will be...     13\n",
       "3  I didn't know that, thank you for teaching me ...     15\n",
       "4  They got bored from haunting earth for thousan...     27\n",
       "5  Thank you for asking questions and recognizing...     15\n",
       "6                                     You’re welcome     15\n",
       "7                    100%! Congrats on your job too!     15\n",
       "8  I’m sorry to hear that friend :(. It’s for the...     24\n",
       "9   Girlfriend weak as well, that jump was pathetic.     25"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c68bb0",
   "metadata": {},
   "source": [
    "# Import related libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "842978b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8810e",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf320c",
   "metadata": {},
   "source": [
    "## For the 28 micro-sentiment multi-label classification scenario, where a sentence may correspond to multiple sentiment category labels, the sentiment labels of the dataset need to be transformed using One-Hot coding first, with \"0\" indicating absence and \"1\" indicating presence for each sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e354f",
   "metadata": {},
   "source": [
    "### Create sentiment label mapping relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a3dfdb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = {\n",
    "    0: \"admiration\",\n",
    "    1: \"amusement\",\n",
    "    2: \"anger\",\n",
    "    3: \"annoyance\",\n",
    "    4: \"approval\",\n",
    "    5: \"caring\",\n",
    "    6: \"confusion\",\n",
    "    7: \"curiosity\",\n",
    "    8: \"desire\",\n",
    "    9: \"disappointment\",\n",
    "    10: \"disapproval\",\n",
    "    11: \"disgust\",\n",
    "    12: \"embarrassment\",\n",
    "    13: \"excitement\",\n",
    "    14: \"fear\",\n",
    "    15: \"gratitude\",\n",
    "    16: \"grief\",\n",
    "    17: \"joy\",\n",
    "    18: \"love\",\n",
    "    19: \"nervousness\",\n",
    "    20: \"optimism\",\n",
    "    21: \"pride\",\n",
    "    22: \"realization\",\n",
    "    23: \"relief\",\n",
    "    24: \"remorse\",\n",
    "    25: \"sadness\",\n",
    "    26: \"surprise\",\n",
    "    27: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee3ff",
   "metadata": {},
   "source": [
    "### Customize the dataset, read the data file, create the dataset and define the data type as MapDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bf5f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "# Clear invalid characters\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# Define the read data set function\n",
    "def read_custom_data(filepath, is_one_hot=True):\n",
    "    f = open(filepath)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        # One-hot processing for 28 types of micro sentiment tags\n",
    "        if is_one_hot:\n",
    "            labels = [float(1) if str(i) in data[1].split(',') else float(0) for i in range(28)]  # 28 types\n",
    "        else:\n",
    "            labels = [int(d) for d in data[1].split(',')]\n",
    "        yield {\"text\": clean_text(data[0]), \"labels\": labels}\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e190e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset() to Create dataset.\n",
    "# lazy=False，The dataset is returned as a MapDataset type.\n",
    "# Pre-processing of training and validation sets.\n",
    "train_ds = load_dataset(read_custom_data, filepath='train.csv', lazy=False) \n",
    "test_ds = load_dataset(read_custom_data, filepath='test.csv', lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675b782",
   "metadata": {},
   "source": [
    "### Print dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fc0c6514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatype: <class 'paddlenlp.datasets.dataset.MapDataset'>\n",
      "training dataset example: {'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}\n",
      "testing dataset example: {'text': 'I’m really sorry about your situation :( Although I love the names Sapphira, Cirilla, and Scarlett!', 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"datatype:\", type(train_ds))\n",
    "print(\"training dataset example:\", train_ds[0])\n",
    "print(\"testing dataset example:\", test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b4596",
   "metadata": {},
   "source": [
    "## Load Chinese ERNIE 3.0 pre-training model and word splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0edba76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-22 16:42:15,075] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:42:15,082] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 16,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-04-22 16:42:18,100] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieForSequenceClassification: ['ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.linear1.bias']\n",
      "- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-04-22 16:42:18,101] [ WARNING]\u001b[0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['ernie.pooler.dense.bias', 'classifier.weight', 'ernie.pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:42:18,101] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:42:18,101] [    INFO]\u001b[0m - Already cached /Users/wubowei/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:42:18,112] [    INFO]\u001b[0m - tokenizer config file saved in /Users/wubowei/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:42:18,113] [    INFO]\u001b[0m - Special tokens file saved in /Users/wubowei/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-medium-zh\"   # ERNIE3.0 model\n",
    "num_classes = 28  # 28 classification mission\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320374a4",
   "metadata": {},
   "source": [
    "## Process the raw data into a model-acceptable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "464fa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# Data pre-processing function to convert text into integer sequences using a word splitter.\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=64)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# function is constructed to extend the different length sequences to the maximum length of the data in the batch, and then stack the data.\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define the BatchSampler, select the batch size and whether to randomly jumble the DataLoader.\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=16, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919dd05d",
   "metadata": {},
   "source": [
    "## Define model validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d7025e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from paddle.metric import Metric\n",
    "\n",
    "# Customize MultiLabelReport evaluation metrics.\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='MultiLabelReport', average='micro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def f1_score(self, y_prob):\n",
    "        '''\n",
    "        Returns the f1 score by searching the best threshhold\n",
    "        '''\n",
    "        best_score = 0\n",
    "        for threshold in [i * 0.01 for i in range(100)]:\n",
    "            self.y_pred = y_prob > threshold\n",
    "            score = sklearn.metrics.f1_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                precison = precision_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "                recall = recall_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        return best_score, precison, recall\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        auc = roc_auc_score(\n",
    "            y_score=self.y_prob, y_true=self.y_true, average=self.average)\n",
    "        f1_score, precison, recall = self.f1_score(y_prob=self.y_prob)\n",
    "        return auc, f1_score, precison, recall\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968143c",
   "metadata": {},
   "source": [
    "# Building the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43683d",
   "metadata": {},
   "source": [
    "## Select an optimization strategy and run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "76696dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# AdamW optimizer, cross-entropy loss function, custom MultiLabelReport evaluation metrics.\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=6e-2, parameters=model.parameters(), weight_decay=0.01)\n",
    "criterion = paddle.nn.MSELoss()\n",
    "metric = MultiLabelReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ef4a4",
   "metadata": {},
   "source": [
    "## Model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5feb0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# Build the validation set evaluate function.\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        losses.append(loss.numpy())\n",
    "        metric.update(probs, labels)\n",
    "        if if_return_results:\n",
    "            probs = probs.tolist()\n",
    "            for prob in probs:\n",
    "                result = []\n",
    "                for c, pred in enumerate(prob):\n",
    "                    if pred > 0.5:\n",
    "                        result.append(label_vocab[c])\n",
    "                results.append(','.join(result))\n",
    "\n",
    "    auc, f1_score, precison, recall = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, auc: %.5f, f1 score: %.5f, precison: %.5f, recall: %.5f\" %\n",
    "          (np.mean(losses), auc, f1_score, precison, recall))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    if if_return_results:\n",
    "        return results\n",
    "    else:\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5f65d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 156.88049, auc: 0.53645, f1 score: 0.09235, speed: 0.26 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 97.19123, auc: 0.50663, f1 score: 0.08330, speed: 0.24 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 78.28904, auc: 0.50238, f1 score: 0.08199, speed: 0.23 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 14.44917, auc: 0.50678, f1 score: 0.08175, speed: 0.25 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-22 16:48:51,025] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 20.94166, auc: 0.41441, f1 score: 0.07997, precison: 0.04165, recall: 1.00000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-22 16:48:51,296] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:48:51,297] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 50, epoch: 1, batch: 50, loss: 3.16498, auc: 0.50748, f1 score: 0.08550, speed: 0.04 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 1.36867, auc: 0.50643, f1 score: 0.08181, speed: 0.25 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 1.11207, auc: 0.50599, f1 score: 0.08093, speed: 0.24 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 1.34504, auc: 0.50857, f1 score: 0.08204, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-22 16:55:17,187] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:55:17,356] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-22 16:55:17,357] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.35317, auc: 0.67353, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 90, epoch: 1, batch: 90, loss: 1.12372, auc: 0.49374, f1 score: 0.08293, speed: 0.04 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.98431, auc: 0.50918, f1 score: 0.08365, speed: 0.24 step/s\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.79660, auc: 0.51908, f1 score: 0.08338, speed: 0.23 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 1.08811, auc: 0.51731, f1 score: 0.08411, speed: 0.22 step/s\n",
      "eval loss: 0.06879, auc: 0.49512, f1 score: 0.08528, precison: 0.04557, recall: 0.66424\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.85344, auc: 0.51658, f1 score: 0.08515, speed: 0.04 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 1.43888, auc: 0.51495, f1 score: 0.08500, speed: 0.24 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.85115, auc: 0.52356, f1 score: 0.08629, speed: 0.23 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.60283, auc: 0.52280, f1 score: 0.08626, speed: 0.21 step/s\n",
      "eval loss: 0.06843, auc: 0.63974, f1 score: 0.26666, precison: 0.21107, recall: 0.36198\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.51754, auc: 0.53594, f1 score: 0.08938, speed: 0.04 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.90026, auc: 0.53822, f1 score: 0.09029, speed: 0.23 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.78677, auc: 0.54136, f1 score: 0.09090, speed: 0.23 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.81804, auc: 0.53228, f1 score: 0.08809, speed: 0.22 step/s\n",
      "eval loss: 0.07639, auc: 0.54259, f1 score: 0.10167, precison: 0.05454, recall: 0.74830\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.69256, auc: 0.54229, f1 score: 0.08608, speed: 0.04 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.68081, auc: 0.52963, f1 score: 0.08548, speed: 0.24 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.71969, auc: 0.52931, f1 score: 0.08645, speed: 0.23 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.56760, auc: 0.52688, f1 score: 0.08569, speed: 0.22 step/s\n",
      "eval loss: 0.06036, auc: 0.53876, f1 score: 0.09654, precison: 0.05179, recall: 0.71054\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.67234, auc: 0.53301, f1 score: 0.08878, speed: 0.04 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.60678, auc: 0.54286, f1 score: 0.09013, speed: 0.23 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.51543, auc: 0.54198, f1 score: 0.08893, speed: 0.22 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.55587, auc: 0.53331, f1 score: 0.08715, speed: 0.22 step/s\n",
      "eval loss: 0.08899, auc: 0.68302, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.65384, auc: 0.51026, f1 score: 0.08763, speed: 0.04 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.65333, auc: 0.51833, f1 score: 0.08742, speed: 0.23 step/s\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.53544, auc: 0.52504, f1 score: 0.08685, speed: 0.24 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.62310, auc: 0.52843, f1 score: 0.08750, speed: 0.22 step/s\n",
      "eval loss: 0.16120, auc: 0.63924, f1 score: 0.24897, precison: 0.19707, recall: 0.33797\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.54254, auc: 0.52307, f1 score: 0.08767, speed: 0.04 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.61481, auc: 0.52776, f1 score: 0.08838, speed: 0.23 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.57440, auc: 0.53114, f1 score: 0.08891, speed: 0.22 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.42046, auc: 0.53500, f1 score: 0.08930, speed: 0.23 step/s\n",
      "eval loss: 0.06692, auc: 0.64551, f1 score: 0.22965, precison: 0.18178, recall: 0.31174\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.54387, auc: 0.55401, f1 score: 0.09091, speed: 0.04 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.63365, auc: 0.54499, f1 score: 0.08872, speed: 0.23 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.35954, auc: 0.55173, f1 score: 0.09017, speed: 0.22 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.32481, auc: 0.55050, f1 score: 0.09083, speed: 0.21 step/s\n",
      "eval loss: 0.08374, auc: 0.66623, f1 score: 0.21136, precison: 0.13649, recall: 0.46816\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.35527, auc: 0.55486, f1 score: 0.09305, speed: 0.04 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.34036, auc: 0.55397, f1 score: 0.09257, speed: 0.24 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.38787, auc: 0.55084, f1 score: 0.09108, speed: 0.23 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.31225, auc: 0.54489, f1 score: 0.08986, speed: 0.22 step/s\n",
      "eval loss: 0.08044, auc: 0.65743, f1 score: 0.18818, precison: 0.12152, recall: 0.41681\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.39544, auc: 0.51575, f1 score: 0.08246, speed: 0.04 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.25098, auc: 0.53477, f1 score: 0.08720, speed: 0.23 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.34093, auc: 0.54493, f1 score: 0.08959, speed: 0.24 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.34546, auc: 0.54340, f1 score: 0.08910, speed: 0.23 step/s\n",
      "eval loss: 0.23711, auc: 0.50530, f1 score: 0.09343, precison: 0.04974, recall: 0.76774\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.26939, auc: 0.54335, f1 score: 0.09382, speed: 0.04 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.31536, auc: 0.54874, f1 score: 0.09172, speed: 0.24 step/s\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.27958, auc: 0.54714, f1 score: 0.09125, speed: 0.23 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.30766, auc: 0.54914, f1 score: 0.09242, speed: 0.22 step/s\n",
      "eval loss: 0.06433, auc: 0.54071, f1 score: 0.09779, precison: 0.05189, recall: 0.84547\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.35350, auc: 0.55309, f1 score: 0.09705, speed: 0.04 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.22588, auc: 0.53245, f1 score: 0.08644, speed: 0.22 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.32432, auc: 0.54663, f1 score: 0.09067, speed: 0.24 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.27818, auc: 0.54862, f1 score: 0.09089, speed: 0.24 step/s\n",
      "eval loss: 0.13484, auc: 0.66824, f1 score: 0.26666, precison: 0.21107, recall: 0.36198\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.25066, auc: 0.54505, f1 score: 0.09832, speed: 0.04 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.23450, auc: 0.55314, f1 score: 0.09635, speed: 0.24 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.20382, auc: 0.56142, f1 score: 0.09912, speed: 0.23 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.19655, auc: 0.55130, f1 score: 0.09571, speed: 0.22 step/s\n",
      "eval loss: 0.05930, auc: 0.64811, f1 score: 0.21433, precison: 0.14882, recall: 0.38284\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.19651, auc: 0.57995, f1 score: 0.10464, speed: 0.04 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.19236, auc: 0.56726, f1 score: 0.09742, speed: 0.23 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.15651, auc: 0.55949, f1 score: 0.09634, speed: 0.23 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.18779, auc: 0.56779, f1 score: 0.10253, speed: 0.22 step/s\n",
      "eval loss: 0.06408, auc: 0.38668, f1 score: 0.07997, precison: 0.04165, recall: 1.00000\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.13968, auc: 0.59543, f1 score: 0.13979, speed: 0.04 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.11000, auc: 0.57572, f1 score: 0.11390, speed: 0.24 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.13089, auc: 0.57395, f1 score: 0.10562, speed: 0.24 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.13995, auc: 0.57119, f1 score: 0.10387, speed: 0.23 step/s\n",
      "eval loss: 0.06673, auc: 0.65443, f1 score: 0.18097, precison: 0.11159, recall: 0.47843\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.18387, auc: 0.57087, f1 score: 0.09970, speed: 0.04 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.18016, auc: 0.58481, f1 score: 0.11324, speed: 0.23 step/s\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.11307, auc: 0.59045, f1 score: 0.11039, speed: 0.22 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.11489, auc: 0.58943, f1 score: 0.11269, speed: 0.22 step/s\n",
      "eval loss: 0.07728, auc: 0.58171, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 730, epoch: 1, batch: 730, loss: 0.13562, auc: 0.58459, f1 score: 0.10929, speed: 0.04 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.12886, auc: 0.57798, f1 score: 0.10886, speed: 0.24 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.09919, auc: 0.56787, f1 score: 0.10490, speed: 0.24 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.11510, auc: 0.56570, f1 score: 0.10580, speed: 0.23 step/s\n",
      "eval loss: 0.07630, auc: 0.62618, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.15072, auc: 0.59313, f1 score: 0.11210, speed: 0.04 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.14056, auc: 0.59310, f1 score: 0.11588, speed: 0.24 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.10762, auc: 0.58975, f1 score: 0.12288, speed: 0.21 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.10653, auc: 0.59233, f1 score: 0.12376, speed: 0.23 step/s\n",
      "eval loss: 0.05907, auc: 0.71075, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.10136, auc: 0.60494, f1 score: 0.11812, speed: 0.04 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.13217, auc: 0.56916, f1 score: 0.10567, speed: 0.23 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.10890, auc: 0.57697, f1 score: 0.10787, speed: 0.23 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.09569, auc: 0.57810, f1 score: 0.11517, speed: 0.23 step/s\n",
      "eval loss: 0.08888, auc: 0.51160, f1 score: 0.09560, precison: 0.05128, recall: 0.70359\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.08820, auc: 0.59227, f1 score: 0.15897, speed: 0.04 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.11333, auc: 0.59427, f1 score: 0.14994, speed: 0.25 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.10667, auc: 0.58927, f1 score: 0.14054, speed: 0.23 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.09439, auc: 0.58059, f1 score: 0.13574, speed: 0.23 step/s\n",
      "eval loss: 0.08471, auc: 0.60243, f1 score: 0.23872, precison: 0.18896, recall: 0.32406\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.09625, auc: 0.61371, f1 score: 0.12829, speed: 0.04 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.11630, auc: 0.59383, f1 score: 0.11463, speed: 0.24 step/s\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.08359, auc: 0.58578, f1 score: 0.11656, speed: 0.23 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.14499, auc: 0.59500, f1 score: 0.12480, speed: 0.23 step/s\n",
      "eval loss: 0.06091, auc: 0.61695, f1 score: 0.17791, precison: 0.11489, recall: 0.39406\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.09075, auc: 0.59097, f1 score: 0.13097, speed: 0.04 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.08336, auc: 0.58265, f1 score: 0.14229, speed: 0.24 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.07080, auc: 0.58254, f1 score: 0.14198, speed: 0.24 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.07406, auc: 0.58576, f1 score: 0.14479, speed: 0.23 step/s\n",
      "eval loss: 0.06600, auc: 0.60597, f1 score: 0.13421, precison: 0.07829, recall: 0.46990\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.06908, auc: 0.58524, f1 score: 0.11569, speed: 0.04 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.06654, auc: 0.59675, f1 score: 0.13562, speed: 0.23 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.06044, auc: 0.60059, f1 score: 0.13703, speed: 0.23 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.08260, auc: 0.59564, f1 score: 0.13598, speed: 0.23 step/s\n",
      "eval loss: 0.06359, auc: 0.64230, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.10600, auc: 0.57973, f1 score: 0.14305, speed: 0.04 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.13019, auc: 0.55683, f1 score: 0.13105, speed: 0.22 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.10064, auc: 0.55474, f1 score: 0.12581, speed: 0.23 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.07484, auc: 0.56331, f1 score: 0.12857, speed: 0.24 step/s\n",
      "eval loss: 0.05706, auc: 0.66891, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.08384, auc: 0.57966, f1 score: 0.14232, speed: 0.04 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.11150, auc: 0.57400, f1 score: 0.15565, speed: 0.24 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.10338, auc: 0.55709, f1 score: 0.15466, speed: 0.24 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.07919, auc: 0.55555, f1 score: 0.15296, speed: 0.24 step/s\n",
      "eval loss: 0.09222, auc: 0.59274, f1 score: 0.22965, precison: 0.18178, recall: 0.31174\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.10068, auc: 0.53495, f1 score: 0.15111, speed: 0.04 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.06853, auc: 0.54948, f1 score: 0.13670, speed: 0.24 step/s\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.06526, auc: 0.55464, f1 score: 0.13665, speed: 0.24 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.06384, auc: 0.56593, f1 score: 0.14100, speed: 0.23 step/s\n",
      "eval loss: 0.04981, auc: 0.61428, f1 score: 0.14044, precison: 0.08192, recall: 0.49170\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.06386, auc: 0.60070, f1 score: 0.13468, speed: 0.04 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.06976, auc: 0.60654, f1 score: 0.14529, speed: 0.24 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.07198, auc: 0.60744, f1 score: 0.14961, speed: 0.23 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.07700, auc: 0.62102, f1 score: 0.15780, speed: 0.23 step/s\n",
      "eval loss: 0.05039, auc: 0.60170, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.07923, auc: 0.60276, f1 score: 0.12992, speed: 0.04 step/s\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.06678, auc: 0.59692, f1 score: 0.13025, speed: 0.24 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.07968, auc: 0.59816, f1 score: 0.13417, speed: 0.23 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.10182, auc: 0.59176, f1 score: 0.13405, speed: 0.22 step/s\n",
      "eval loss: 0.08084, auc: 0.58522, f1 score: 0.14865, precison: 0.08671, recall: 0.52046\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.08012, auc: 0.55483, f1 score: 0.20813, speed: 0.04 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.10224, auc: 0.54681, f1 score: 0.16423, speed: 0.22 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.08467, auc: 0.56346, f1 score: 0.17461, speed: 0.23 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.06735, auc: 0.57679, f1 score: 0.15556, speed: 0.23 step/s\n",
      "eval loss: 0.07869, auc: 0.62543, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.09791, auc: 0.56377, f1 score: 0.21614, speed: 0.04 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.06293, auc: 0.53289, f1 score: 0.19135, speed: 0.24 step/s\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.13567, auc: 0.56198, f1 score: 0.18343, speed: 0.24 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.09076, auc: 0.56258, f1 score: 0.17532, speed: 0.24 step/s\n",
      "eval loss: 0.09125, auc: 0.56100, f1 score: 0.14826, precison: 0.08854, recall: 0.45552\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.13638, auc: 0.57915, f1 score: 0.18083, speed: 0.04 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.09893, auc: 0.55094, f1 score: 0.15850, speed: 0.24 step/s\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.07868, auc: 0.58211, f1 score: 0.16996, speed: 0.23 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.10598, auc: 0.57922, f1 score: 0.15823, speed: 0.23 step/s\n",
      "eval loss: 0.09385, auc: 0.70455, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.10923, auc: 0.60929, f1 score: 0.15743, speed: 0.04 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.13533, auc: 0.57008, f1 score: 0.15637, speed: 0.23 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.09670, auc: 0.58240, f1 score: 0.15812, speed: 0.23 step/s\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.08236, auc: 0.57339, f1 score: 0.15082, speed: 0.23 step/s\n",
      "eval loss: 0.10440, auc: 0.63910, f1 score: 0.20045, precison: 0.12945, recall: 0.44399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 1370, epoch: 1, batch: 1370, loss: 0.05892, auc: 0.58519, f1 score: 0.13504, speed: 0.04 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.09420, auc: 0.61046, f1 score: 0.15149, speed: 0.24 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.07190, auc: 0.60303, f1 score: 0.14789, speed: 0.23 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.11134, auc: 0.59691, f1 score: 0.15253, speed: 0.23 step/s\n",
      "eval loss: 0.15142, auc: 0.49630, f1 score: 0.08978, precison: 0.04780, recall: 0.73772\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.10355, auc: 0.54671, f1 score: 0.16945, speed: 0.04 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.11416, auc: 0.56975, f1 score: 0.19251, speed: 0.24 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.10303, auc: 0.58562, f1 score: 0.17645, speed: 0.22 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.08777, auc: 0.58470, f1 score: 0.16285, speed: 0.24 step/s\n",
      "eval loss: 0.06760, auc: 0.56653, f1 score: 0.17548, precison: 0.11332, recall: 0.38869\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.09839, auc: 0.61424, f1 score: 0.17884, speed: 0.04 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.08731, auc: 0.60418, f1 score: 0.18194, speed: 0.23 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.12168, auc: 0.59429, f1 score: 0.16889, speed: 0.23 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.08461, auc: 0.60362, f1 score: 0.16399, speed: 0.23 step/s\n",
      "eval loss: 0.15634, auc: 0.39112, f1 score: 0.07997, precison: 0.04165, recall: 1.00000\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.19168, auc: 0.57200, f1 score: 0.13012, speed: 0.04 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.08596, auc: 0.57448, f1 score: 0.12190, speed: 0.24 step/s\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.13712, auc: 0.56451, f1 score: 0.12279, speed: 0.24 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.10988, auc: 0.56386, f1 score: 0.12778, speed: 0.23 step/s\n",
      "eval loss: 0.11395, auc: 0.59490, f1 score: 0.18425, precison: 0.12794, recall: 0.32912\n",
      "global step 1530, epoch: 2, batch: 3, loss: 0.12355, auc: 0.57330, f1 score: 0.14908, speed: 0.04 step/s\n",
      "global step 1540, epoch: 2, batch: 13, loss: 0.27966, auc: 0.54343, f1 score: 0.12800, speed: 0.23 step/s\n",
      "global step 1550, epoch: 2, batch: 23, loss: 0.19024, auc: 0.55065, f1 score: 0.12249, speed: 0.22 step/s\n",
      "global step 1560, epoch: 2, batch: 33, loss: 0.12500, auc: 0.56104, f1 score: 0.12783, speed: 0.24 step/s\n",
      "eval loss: 0.07688, auc: 0.41262, f1 score: 0.08022, precison: 0.04184, recall: 0.96872\n",
      "global step 1570, epoch: 2, batch: 43, loss: 0.21647, auc: 0.54523, f1 score: 0.15707, speed: 0.04 step/s\n",
      "global step 1580, epoch: 2, batch: 53, loss: 0.30673, auc: 0.54767, f1 score: 0.14011, speed: 0.23 step/s\n",
      "global step 1590, epoch: 2, batch: 63, loss: 0.09568, auc: 0.56079, f1 score: 0.14345, speed: 0.24 step/s\n",
      "global step 1600, epoch: 2, batch: 73, loss: 0.11786, auc: 0.55627, f1 score: 0.13579, speed: 0.23 step/s\n",
      "eval loss: 0.13125, auc: 0.65896, f1 score: 0.26666, precison: 0.21107, recall: 0.36198\n",
      "global step 1610, epoch: 2, batch: 83, loss: 0.14418, auc: 0.57028, f1 score: 0.13802, speed: 0.04 step/s\n",
      "global step 1620, epoch: 2, batch: 93, loss: 0.32883, auc: 0.57148, f1 score: 0.13832, speed: 0.25 step/s\n",
      "global step 1630, epoch: 2, batch: 103, loss: 0.32058, auc: 0.56329, f1 score: 0.13776, speed: 0.23 step/s\n",
      "global step 1640, epoch: 2, batch: 113, loss: 0.14216, auc: 0.57224, f1 score: 0.13506, speed: 0.24 step/s\n",
      "eval loss: 0.08668, auc: 0.58136, f1 score: 0.16778, precison: 0.10835, recall: 0.37162\n",
      "global step 1650, epoch: 2, batch: 123, loss: 0.11364, auc: 0.57364, f1 score: 0.13266, speed: 0.04 step/s\n",
      "global step 1660, epoch: 2, batch: 133, loss: 0.16637, auc: 0.55559, f1 score: 0.12921, speed: 0.24 step/s\n",
      "global step 1670, epoch: 2, batch: 143, loss: 0.37638, auc: 0.53709, f1 score: 0.12993, speed: 0.24 step/s\n",
      "global step 1680, epoch: 2, batch: 153, loss: 0.54142, auc: 0.54784, f1 score: 0.13861, speed: 0.23 step/s\n",
      "eval loss: 0.53297, auc: 0.58289, f1 score: 0.12697, precison: 0.07089, recall: 0.60784\n",
      "global step 1690, epoch: 2, batch: 163, loss: 0.16617, auc: 0.58738, f1 score: 0.12992, speed: 0.04 step/s\n",
      "global step 1700, epoch: 2, batch: 173, loss: 0.19735, auc: 0.56832, f1 score: 0.12093, speed: 0.24 step/s\n",
      "global step 1710, epoch: 2, batch: 183, loss: 0.10374, auc: 0.57192, f1 score: 0.12336, speed: 0.23 step/s\n",
      "global step 1720, epoch: 2, batch: 193, loss: 0.32001, auc: 0.55429, f1 score: 0.12730, speed: 0.24 step/s\n",
      "eval loss: 0.43090, auc: 0.66549, f1 score: 0.26666, precison: 0.21107, recall: 0.36198\n",
      "global step 1730, epoch: 2, batch: 203, loss: 0.28873, auc: 0.53275, f1 score: 0.13183, speed: 0.04 step/s\n",
      "global step 1740, epoch: 2, batch: 213, loss: 0.13094, auc: 0.53743, f1 score: 0.13074, speed: 0.23 step/s\n",
      "global step 1750, epoch: 2, batch: 223, loss: 0.14830, auc: 0.55592, f1 score: 0.13021, speed: 0.24 step/s\n",
      "global step 1760, epoch: 2, batch: 233, loss: 0.36119, auc: 0.55614, f1 score: 0.12101, speed: 0.24 step/s\n",
      "eval loss: 0.23162, auc: 0.61551, f1 score: 0.15456, precison: 0.09016, recall: 0.54116\n",
      "global step 1770, epoch: 2, batch: 243, loss: 0.26447, auc: 0.55947, f1 score: 0.13633, speed: 0.04 step/s\n",
      "global step 1780, epoch: 2, batch: 253, loss: 0.23114, auc: 0.53995, f1 score: 0.12240, speed: 0.24 step/s\n",
      "global step 1790, epoch: 2, batch: 263, loss: 0.36884, auc: 0.53940, f1 score: 0.12304, speed: 0.23 step/s\n",
      "global step 1800, epoch: 2, batch: 273, loss: 0.24762, auc: 0.53822, f1 score: 0.13004, speed: 0.23 step/s\n",
      "eval loss: 0.25173, auc: 0.59713, f1 score: 0.22965, precison: 0.18178, recall: 0.31174\n",
      "global step 1810, epoch: 2, batch: 283, loss: 0.32866, auc: 0.52561, f1 score: 0.16282, speed: 0.04 step/s\n",
      "global step 1820, epoch: 2, batch: 293, loss: 0.36908, auc: 0.55129, f1 score: 0.14489, speed: 0.24 step/s\n",
      "global step 1830, epoch: 2, batch: 303, loss: 0.50540, auc: 0.55361, f1 score: 0.12914, speed: 0.23 step/s\n",
      "global step 1840, epoch: 2, batch: 313, loss: 0.79816, auc: 0.54581, f1 score: 0.12436, speed: 0.23 step/s\n",
      "eval loss: 0.30866, auc: 0.59593, f1 score: 0.22965, precison: 0.18178, recall: 0.31174\n",
      "global step 1850, epoch: 2, batch: 323, loss: 0.43171, auc: 0.51832, f1 score: 0.10272, speed: 0.04 step/s\n",
      "global step 1860, epoch: 2, batch: 333, loss: 0.64237, auc: 0.54176, f1 score: 0.10424, speed: 0.25 step/s\n",
      "global step 1870, epoch: 2, batch: 343, loss: 0.53993, auc: 0.54133, f1 score: 0.10793, speed: 0.24 step/s\n",
      "global step 1880, epoch: 2, batch: 353, loss: 0.36577, auc: 0.54466, f1 score: 0.11495, speed: 0.23 step/s\n",
      "eval loss: 0.45642, auc: 0.46269, f1 score: 0.09952, precison: 0.07877, recall: 0.13509\n",
      "global step 1890, epoch: 2, batch: 363, loss: 0.38262, auc: 0.55689, f1 score: 0.13431, speed: 0.04 step/s\n",
      "global step 1900, epoch: 2, batch: 373, loss: 0.27999, auc: 0.55459, f1 score: 0.11444, speed: 0.24 step/s\n",
      "global step 1910, epoch: 2, batch: 383, loss: 0.35610, auc: 0.55142, f1 score: 0.11389, speed: 0.23 step/s\n",
      "global step 1920, epoch: 2, batch: 393, loss: 0.32271, auc: 0.54831, f1 score: 0.10917, speed: 0.23 step/s\n",
      "eval loss: 0.35224, auc: 0.50348, f1 score: 0.09463, precison: 0.05099, recall: 0.65587\n",
      "global step 1930, epoch: 2, batch: 403, loss: 0.58537, auc: 0.55268, f1 score: 0.10958, speed: 0.04 step/s\n",
      "global step 1940, epoch: 2, batch: 413, loss: 0.55135, auc: 0.54756, f1 score: 0.10698, speed: 0.23 step/s\n",
      "global step 1950, epoch: 2, batch: 423, loss: 0.42077, auc: 0.54963, f1 score: 0.10715, speed: 0.24 step/s\n",
      "global step 1960, epoch: 2, batch: 433, loss: 0.49115, auc: 0.53974, f1 score: 0.10434, speed: 0.23 step/s\n",
      "eval loss: 0.38318, auc: 0.62875, f1 score: 0.14373, precison: 0.08384, recall: 0.50324\n",
      "global step 1970, epoch: 2, batch: 443, loss: 0.53752, auc: 0.59093, f1 score: 0.12213, speed: 0.04 step/s\n",
      "global step 1980, epoch: 2, batch: 453, loss: 0.79596, auc: 0.55665, f1 score: 0.11490, speed: 0.24 step/s\n",
      "global step 1990, epoch: 2, batch: 463, loss: 0.51030, auc: 0.54575, f1 score: 0.10661, speed: 0.24 step/s\n",
      "global step 2000, epoch: 2, batch: 473, loss: 0.45907, auc: 0.54846, f1 score: 0.10938, speed: 0.23 step/s\n",
      "eval loss: 0.48007, auc: 0.55812, f1 score: 0.13746, precison: 0.08209, recall: 0.42234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2010, epoch: 2, batch: 483, loss: 0.50941, auc: 0.51589, f1 score: 0.11404, speed: 0.04 step/s\n",
      "global step 2020, epoch: 2, batch: 493, loss: 0.47023, auc: 0.52253, f1 score: 0.10340, speed: 0.24 step/s\n",
      "global step 2030, epoch: 2, batch: 503, loss: 0.30310, auc: 0.52142, f1 score: 0.09496, speed: 0.23 step/s\n",
      "global step 2040, epoch: 2, batch: 513, loss: 0.86430, auc: 0.52154, f1 score: 0.09262, speed: 0.22 step/s\n",
      "eval loss: 0.70352, auc: 0.63356, f1 score: 0.18575, precison: 0.11996, recall: 0.41144\n",
      "global step 2050, epoch: 2, batch: 523, loss: 0.61276, auc: 0.54455, f1 score: 0.12252, speed: 0.04 step/s\n",
      "global step 2060, epoch: 2, batch: 533, loss: 0.98375, auc: 0.53985, f1 score: 0.09895, speed: 0.24 step/s\n",
      "global step 2070, epoch: 2, batch: 543, loss: 1.52096, auc: 0.53368, f1 score: 0.09091, speed: 0.23 step/s\n",
      "global step 2080, epoch: 2, batch: 553, loss: 1.11054, auc: 0.53974, f1 score: 0.09192, speed: 0.23 step/s\n",
      "eval loss: 0.97088, auc: 0.60601, f1 score: 0.13664, precison: 0.08160, recall: 0.41981\n",
      "global step 2090, epoch: 2, batch: 563, loss: 0.78549, auc: 0.53707, f1 score: 0.09110, speed: 0.04 step/s\n",
      "global step 2100, epoch: 2, batch: 573, loss: 1.82423, auc: 0.52960, f1 score: 0.09427, speed: 0.23 step/s\n",
      "global step 2110, epoch: 2, batch: 583, loss: 0.95833, auc: 0.53080, f1 score: 0.08838, speed: 0.23 step/s\n",
      "global step 2120, epoch: 2, batch: 593, loss: 0.93281, auc: 0.53112, f1 score: 0.08779, speed: 0.22 step/s\n",
      "eval loss: 1.07093, auc: 0.50331, f1 score: 0.09180, precison: 0.04972, recall: 0.59693\n",
      "global step 2130, epoch: 2, batch: 603, loss: 1.28448, auc: 0.55115, f1 score: 0.09070, speed: 0.04 step/s\n",
      "global step 2140, epoch: 2, batch: 613, loss: 2.23318, auc: 0.53966, f1 score: 0.09035, speed: 0.25 step/s\n",
      "global step 2150, epoch: 2, batch: 623, loss: 1.57127, auc: 0.53641, f1 score: 0.09026, speed: 0.23 step/s\n",
      "global step 2160, epoch: 2, batch: 633, loss: 1.94463, auc: 0.53629, f1 score: 0.08891, speed: 0.22 step/s\n",
      "eval loss: 1.82778, auc: 0.55343, f1 score: 0.10690, precison: 0.05968, recall: 0.51177\n",
      "global step 2170, epoch: 2, batch: 643, loss: 1.61430, auc: 0.49658, f1 score: 0.08241, speed: 0.04 step/s\n",
      "global step 2180, epoch: 2, batch: 653, loss: 2.62607, auc: 0.49688, f1 score: 0.08440, speed: 0.24 step/s\n",
      "global step 2190, epoch: 2, batch: 663, loss: 2.02416, auc: 0.50400, f1 score: 0.08737, speed: 0.23 step/s\n",
      "global step 2200, epoch: 2, batch: 673, loss: 1.36052, auc: 0.50832, f1 score: 0.08730, speed: 0.22 step/s\n",
      "eval loss: 1.44288, auc: 0.46486, f1 score: 0.08995, precison: 0.04789, recall: 0.73914\n",
      "global step 2210, epoch: 2, batch: 683, loss: 0.98321, auc: 0.50211, f1 score: 0.08921, speed: 0.04 step/s\n",
      "global step 2220, epoch: 2, batch: 693, loss: 1.75680, auc: 0.52197, f1 score: 0.09154, speed: 0.24 step/s\n",
      "global step 2230, epoch: 2, batch: 703, loss: 2.96937, auc: 0.51842, f1 score: 0.08896, speed: 0.23 step/s\n",
      "global step 2240, epoch: 2, batch: 713, loss: 2.10569, auc: 0.52593, f1 score: 0.08913, speed: 0.22 step/s\n",
      "eval loss: 1.70182, auc: 0.50441, f1 score: 0.09376, precison: 0.05108, recall: 0.56944\n",
      "global step 2250, epoch: 2, batch: 723, loss: 2.09526, auc: 0.51813, f1 score: 0.09346, speed: 0.04 step/s\n",
      "global step 2260, epoch: 2, batch: 733, loss: 1.51257, auc: 0.53962, f1 score: 0.09084, speed: 0.23 step/s\n",
      "global step 2270, epoch: 2, batch: 743, loss: 1.24463, auc: 0.52774, f1 score: 0.08713, speed: 0.22 step/s\n",
      "global step 2280, epoch: 2, batch: 753, loss: 1.84655, auc: 0.52486, f1 score: 0.08809, speed: 0.22 step/s\n",
      "eval loss: 1.73740, auc: 0.56019, f1 score: 0.10563, precison: 0.05721, recall: 0.68684\n",
      "global step 2290, epoch: 2, batch: 763, loss: 3.23296, auc: 0.51719, f1 score: 0.08750, speed: 0.04 step/s\n",
      "global step 2300, epoch: 2, batch: 773, loss: 1.21225, auc: 0.51022, f1 score: 0.08454, speed: 0.24 step/s\n",
      "global step 2310, epoch: 2, batch: 783, loss: 2.50320, auc: 0.51621, f1 score: 0.08417, speed: 0.23 step/s\n",
      "global step 2320, epoch: 2, batch: 793, loss: 1.39835, auc: 0.50761, f1 score: 0.08228, speed: 0.22 step/s\n",
      "eval loss: 1.02585, auc: 0.48036, f1 score: 0.08835, precison: 0.04814, recall: 0.53658\n",
      "global step 2330, epoch: 2, batch: 803, loss: 1.82474, auc: 0.51251, f1 score: 0.08781, speed: 0.04 step/s\n",
      "global step 2340, epoch: 2, batch: 813, loss: 1.61609, auc: 0.51945, f1 score: 0.08626, speed: 0.23 step/s\n",
      "global step 2350, epoch: 2, batch: 823, loss: 1.68581, auc: 0.52485, f1 score: 0.08533, speed: 0.23 step/s\n",
      "global step 2360, epoch: 2, batch: 833, loss: 2.05458, auc: 0.52468, f1 score: 0.08541, speed: 0.22 step/s\n",
      "eval loss: 2.03010, auc: 0.46875, f1 score: 0.08762, precison: 0.04774, recall: 0.53215\n",
      "global step 2370, epoch: 2, batch: 843, loss: 1.70043, auc: 0.53352, f1 score: 0.09306, speed: 0.04 step/s\n",
      "global step 2380, epoch: 2, batch: 853, loss: 2.56992, auc: 0.51802, f1 score: 0.09047, speed: 0.23 step/s\n",
      "global step 2390, epoch: 2, batch: 863, loss: 2.41379, auc: 0.50926, f1 score: 0.08624, speed: 0.22 step/s\n",
      "global step 2400, epoch: 2, batch: 873, loss: 2.07837, auc: 0.51064, f1 score: 0.08548, speed: 0.21 step/s\n",
      "eval loss: 1.99371, auc: 0.49368, f1 score: 0.09454, precison: 0.05033, recall: 0.77690\n",
      "global step 2410, epoch: 2, batch: 883, loss: 1.25777, auc: 0.54588, f1 score: 0.09057, speed: 0.04 step/s\n",
      "global step 2420, epoch: 2, batch: 893, loss: 1.89835, auc: 0.54540, f1 score: 0.09315, speed: 0.23 step/s\n",
      "global step 2430, epoch: 2, batch: 903, loss: 1.49292, auc: 0.53616, f1 score: 0.08994, speed: 0.23 step/s\n",
      "global step 2440, epoch: 2, batch: 913, loss: 1.35900, auc: 0.53519, f1 score: 0.08865, speed: 0.23 step/s\n",
      "eval loss: 1.31891, auc: 0.51428, f1 score: 0.10987, precison: 0.06206, recall: 0.47891\n",
      "global step 2450, epoch: 2, batch: 923, loss: 1.18292, auc: 0.52480, f1 score: 0.09366, speed: 0.04 step/s\n",
      "global step 2460, epoch: 2, batch: 933, loss: 2.05297, auc: 0.52114, f1 score: 0.08599, speed: 0.23 step/s\n",
      "global step 2470, epoch: 2, batch: 943, loss: 1.45763, auc: 0.52180, f1 score: 0.08495, speed: 0.23 step/s\n",
      "global step 2480, epoch: 2, batch: 953, loss: 1.87400, auc: 0.52113, f1 score: 0.08426, speed: 0.23 step/s\n",
      "eval loss: 1.72561, auc: 0.50875, f1 score: 0.09652, precison: 0.05139, recall: 0.79317\n",
      "global step 2490, epoch: 2, batch: 963, loss: 1.29582, auc: 0.50868, f1 score: 0.08491, speed: 0.04 step/s\n",
      "global step 2500, epoch: 2, batch: 973, loss: 1.58488, auc: 0.51924, f1 score: 0.08386, speed: 0.24 step/s\n",
      "global step 2510, epoch: 2, batch: 983, loss: 1.51217, auc: 0.52196, f1 score: 0.08222, speed: 0.25 step/s\n",
      "global step 2520, epoch: 2, batch: 993, loss: 3.44227, auc: 0.51745, f1 score: 0.08211, speed: 0.23 step/s\n",
      "eval loss: 3.96538, auc: 0.51187, f1 score: 0.10313, precison: 0.05908, recall: 0.40528\n",
      "global step 2530, epoch: 2, batch: 1003, loss: 1.85356, auc: 0.51543, f1 score: 0.08750, speed: 0.04 step/s\n",
      "global step 2540, epoch: 2, batch: 1013, loss: 2.14314, auc: 0.51156, f1 score: 0.08457, speed: 0.23 step/s\n",
      "global step 2550, epoch: 2, batch: 1023, loss: 1.15896, auc: 0.51069, f1 score: 0.08256, speed: 0.22 step/s\n",
      "global step 2560, epoch: 2, batch: 1033, loss: 2.03476, auc: 0.51683, f1 score: 0.08864, speed: 0.22 step/s\n",
      "eval loss: 1.92345, auc: 0.54274, f1 score: 0.11506, precison: 0.06498, recall: 0.50150\n",
      "global step 2570, epoch: 2, batch: 1043, loss: 1.70584, auc: 0.53038, f1 score: 0.09175, speed: 0.04 step/s\n",
      "global step 2580, epoch: 2, batch: 1053, loss: 1.70947, auc: 0.51985, f1 score: 0.08410, speed: 0.22 step/s\n",
      "global step 2590, epoch: 2, batch: 1063, loss: 1.33738, auc: 0.52395, f1 score: 0.08421, speed: 0.22 step/s\n",
      "global step 2600, epoch: 2, batch: 1073, loss: 0.79944, auc: 0.52151, f1 score: 0.08188, speed: 0.23 step/s\n",
      "eval loss: 0.53073, auc: 0.63071, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 2610, epoch: 2, batch: 1083, loss: 2.00901, auc: 0.51102, f1 score: 0.09062, speed: 0.04 step/s\n",
      "global step 2620, epoch: 2, batch: 1093, loss: 1.23840, auc: 0.51106, f1 score: 0.09063, speed: 0.24 step/s\n",
      "global step 2630, epoch: 2, batch: 1103, loss: 1.12330, auc: 0.50610, f1 score: 0.08804, speed: 0.22 step/s\n",
      "global step 2640, epoch: 2, batch: 1113, loss: 1.70987, auc: 0.51453, f1 score: 0.08632, speed: 0.22 step/s\n",
      "eval loss: 1.44833, auc: 0.55224, f1 score: 0.22231, precison: 0.17597, recall: 0.30179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 2650, epoch: 2, batch: 1123, loss: 1.19134, auc: 0.52524, f1 score: 0.08446, speed: 0.04 step/s\n",
      "global step 2660, epoch: 2, batch: 1133, loss: 1.70851, auc: 0.52113, f1 score: 0.08373, speed: 0.24 step/s\n",
      "global step 2670, epoch: 2, batch: 1143, loss: 2.78616, auc: 0.51916, f1 score: 0.08286, speed: 0.23 step/s\n",
      "global step 2680, epoch: 2, batch: 1153, loss: 1.71282, auc: 0.52103, f1 score: 0.08274, speed: 0.23 step/s\n",
      "eval loss: 1.62872, auc: 0.63935, f1 score: 0.24524, precison: 0.19412, recall: 0.33291\n",
      "global step 2690, epoch: 2, batch: 1163, loss: 1.10228, auc: 0.53747, f1 score: 0.11033, speed: 0.04 step/s\n",
      "global step 2700, epoch: 2, batch: 1173, loss: 1.60682, auc: 0.54570, f1 score: 0.09668, speed: 0.22 step/s\n",
      "global step 2710, epoch: 2, batch: 1183, loss: 2.12933, auc: 0.52549, f1 score: 0.09024, speed: 0.23 step/s\n",
      "global step 2720, epoch: 2, batch: 1193, loss: 2.42481, auc: 0.51527, f1 score: 0.09195, speed: 0.23 step/s\n",
      "eval loss: 2.03013, auc: 0.43712, f1 score: 0.09311, precison: 0.05742, recall: 0.24617\n",
      "global step 2730, epoch: 2, batch: 1203, loss: 1.03760, auc: 0.51572, f1 score: 0.10653, speed: 0.04 step/s\n",
      "global step 2740, epoch: 2, batch: 1213, loss: 1.33593, auc: 0.52076, f1 score: 0.09853, speed: 0.22 step/s\n",
      "global step 2750, epoch: 2, batch: 1223, loss: 1.74889, auc: 0.52145, f1 score: 0.09802, speed: 0.22 step/s\n",
      "global step 2760, epoch: 2, batch: 1233, loss: 1.57787, auc: 0.52434, f1 score: 0.09619, speed: 0.23 step/s\n",
      "eval loss: 1.43858, auc: 0.53147, f1 score: 0.10239, precison: 0.05579, recall: 0.62190\n",
      "global step 2770, epoch: 2, batch: 1243, loss: 2.19386, auc: 0.51358, f1 score: 0.08897, speed: 0.04 step/s\n",
      "global step 2780, epoch: 2, batch: 1253, loss: 2.10776, auc: 0.52971, f1 score: 0.09127, speed: 0.23 step/s\n",
      "global step 2790, epoch: 2, batch: 1263, loss: 3.40089, auc: 0.51845, f1 score: 0.08285, speed: 0.22 step/s\n",
      "global step 2800, epoch: 2, batch: 1273, loss: 2.23592, auc: 0.52135, f1 score: 0.08393, speed: 0.22 step/s\n",
      "eval loss: 1.87807, auc: 0.62020, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 2810, epoch: 2, batch: 1283, loss: 2.04735, auc: 0.51919, f1 score: 0.08730, speed: 0.04 step/s\n",
      "global step 2820, epoch: 2, batch: 1293, loss: 1.11051, auc: 0.51681, f1 score: 0.08578, speed: 0.23 step/s\n",
      "global step 2830, epoch: 2, batch: 1303, loss: 2.09083, auc: 0.52381, f1 score: 0.08617, speed: 0.22 step/s\n",
      "global step 2840, epoch: 2, batch: 1313, loss: 1.75173, auc: 0.52054, f1 score: 0.08396, speed: 0.22 step/s\n",
      "eval loss: 1.46333, auc: 0.51731, f1 score: 0.09867, precison: 0.05573, recall: 0.43008\n",
      "global step 2850, epoch: 2, batch: 1323, loss: 2.12856, auc: 0.51975, f1 score: 0.09121, speed: 0.04 step/s\n",
      "global step 2860, epoch: 2, batch: 1333, loss: 1.58730, auc: 0.51261, f1 score: 0.08853, speed: 0.23 step/s\n",
      "global step 2870, epoch: 2, batch: 1343, loss: 1.25757, auc: 0.51640, f1 score: 0.09005, speed: 0.22 step/s\n",
      "global step 2880, epoch: 2, batch: 1353, loss: 1.77220, auc: 0.50701, f1 score: 0.08614, speed: 0.23 step/s\n",
      "eval loss: 1.45882, auc: 0.34798, f1 score: 0.08022, precison: 0.04184, recall: 0.96872\n",
      "global step 2890, epoch: 2, batch: 1363, loss: 1.92745, auc: 0.53954, f1 score: 0.10759, speed: 0.04 step/s\n",
      "global step 2900, epoch: 2, batch: 1373, loss: 2.38840, auc: 0.51835, f1 score: 0.08752, speed: 0.22 step/s\n",
      "global step 2910, epoch: 2, batch: 1383, loss: 1.18214, auc: 0.51570, f1 score: 0.08747, speed: 0.23 step/s\n",
      "global step 2920, epoch: 2, batch: 1393, loss: 2.13475, auc: 0.51876, f1 score: 0.08693, speed: 0.23 step/s\n",
      "eval loss: 1.86118, auc: 0.58485, f1 score: 0.12790, precison: 0.07886, recall: 0.33813\n",
      "global step 2930, epoch: 2, batch: 1403, loss: 1.45129, auc: 0.54819, f1 score: 0.09392, speed: 0.04 step/s\n",
      "global step 2940, epoch: 2, batch: 1413, loss: 2.03873, auc: 0.53200, f1 score: 0.08684, speed: 0.24 step/s\n",
      "global step 2950, epoch: 2, batch: 1423, loss: 2.00238, auc: 0.52781, f1 score: 0.08496, speed: 0.22 step/s\n",
      "global step 2960, epoch: 2, batch: 1433, loss: 1.40225, auc: 0.52800, f1 score: 0.08381, speed: 0.21 step/s\n",
      "eval loss: 1.31018, auc: 0.61622, f1 score: 0.15186, precison: 0.09364, recall: 0.40149\n",
      "global step 2970, epoch: 2, batch: 1443, loss: 1.03410, auc: 0.51484, f1 score: 0.08503, speed: 0.04 step/s\n",
      "global step 2980, epoch: 2, batch: 1453, loss: 1.28440, auc: 0.52539, f1 score: 0.09289, speed: 0.23 step/s\n",
      "global step 2990, epoch: 2, batch: 1463, loss: 1.64009, auc: 0.51348, f1 score: 0.09297, speed: 0.23 step/s\n",
      "global step 3000, epoch: 2, batch: 1473, loss: 1.65161, auc: 0.51985, f1 score: 0.08549, speed: 0.24 step/s\n",
      "eval loss: 1.28388, auc: 0.65386, f1 score: 0.18740, precison: 0.12102, recall: 0.41507\n",
      "global step 3010, epoch: 2, batch: 1483, loss: 2.33387, auc: 0.50737, f1 score: 0.08699, speed: 0.04 step/s\n",
      "global step 3020, epoch: 2, batch: 1493, loss: 1.95961, auc: 0.51276, f1 score: 0.08611, speed: 0.24 step/s\n",
      "global step 3030, epoch: 2, batch: 1503, loss: 1.56497, auc: 0.51524, f1 score: 0.08285, speed: 0.22 step/s\n",
      "global step 3040, epoch: 2, batch: 1513, loss: 1.47845, auc: 0.51173, f1 score: 0.08224, speed: 0.23 step/s\n",
      "eval loss: 1.51684, auc: 0.57432, f1 score: 0.15829, precison: 0.10222, recall: 0.35061\n",
      "global step 3050, epoch: 2, batch: 1523, loss: 1.02255, auc: 0.51267, f1 score: 0.08794, speed: 0.04 step/s\n",
      "global step 3060, epoch: 3, batch: 6, loss: 3.45025, auc: 0.52421, f1 score: 0.08635, speed: 0.26 step/s\n",
      "global step 3070, epoch: 3, batch: 16, loss: 2.13481, auc: 0.52242, f1 score: 0.08413, speed: 0.24 step/s\n",
      "global step 3080, epoch: 3, batch: 26, loss: 1.89250, auc: 0.52432, f1 score: 0.08474, speed: 0.24 step/s\n",
      "eval loss: 1.27098, auc: 0.45787, f1 score: 0.09017, precison: 0.04737, recall: 0.93427\n",
      "global step 3090, epoch: 3, batch: 36, loss: 0.87159, auc: 0.47852, f1 score: 0.11053, speed: 0.04 step/s\n",
      "global step 3100, epoch: 3, batch: 46, loss: 3.64756, auc: 0.49257, f1 score: 0.08270, speed: 0.24 step/s\n",
      "global step 3110, epoch: 3, batch: 56, loss: 1.07757, auc: 0.51251, f1 score: 0.08413, speed: 0.24 step/s\n",
      "global step 3120, epoch: 3, batch: 66, loss: 2.01664, auc: 0.51398, f1 score: 0.08296, speed: 0.23 step/s\n",
      "eval loss: 1.39521, auc: 0.60167, f1 score: 0.21766, precison: 0.17229, recall: 0.29547\n",
      "global step 3130, epoch: 3, batch: 76, loss: 0.98616, auc: 0.52957, f1 score: 0.08614, speed: 0.04 step/s\n",
      "global step 3140, epoch: 3, batch: 86, loss: 1.91850, auc: 0.51212, f1 score: 0.08257, speed: 0.24 step/s\n",
      "global step 3150, epoch: 3, batch: 96, loss: 1.11612, auc: 0.51053, f1 score: 0.08299, speed: 0.23 step/s\n",
      "global step 3160, epoch: 3, batch: 106, loss: 1.47007, auc: 0.51838, f1 score: 0.08457, speed: 0.23 step/s\n",
      "eval loss: 1.29692, auc: 0.55139, f1 score: 0.11804, precison: 0.06762, recall: 0.46390\n",
      "global step 3170, epoch: 3, batch: 116, loss: 1.46683, auc: 0.50740, f1 score: 0.09078, speed: 0.04 step/s\n",
      "global step 3180, epoch: 3, batch: 126, loss: 1.46496, auc: 0.51814, f1 score: 0.10389, speed: 0.24 step/s\n",
      "global step 3190, epoch: 3, batch: 136, loss: 1.80503, auc: 0.50387, f1 score: 0.08780, speed: 0.22 step/s\n",
      "global step 3200, epoch: 3, batch: 146, loss: 1.76643, auc: 0.50970, f1 score: 0.08846, speed: 0.23 step/s\n",
      "eval loss: 1.61728, auc: 0.45111, f1 score: 0.08476, precison: 0.04512, recall: 0.69648\n",
      "global step 3210, epoch: 3, batch: 156, loss: 1.26064, auc: 0.52316, f1 score: 0.08520, speed: 0.04 step/s\n",
      "global step 3220, epoch: 3, batch: 166, loss: 1.52205, auc: 0.51876, f1 score: 0.08395, speed: 0.23 step/s\n",
      "global step 3230, epoch: 3, batch: 176, loss: 36.94271, auc: 0.51459, f1 score: 0.08277, speed: 0.24 step/s\n",
      "global step 3240, epoch: 3, batch: 186, loss: 12.64223, auc: 0.51185, f1 score: 0.08235, speed: 0.22 step/s\n",
      "eval loss: 15.72972, auc: 0.55919, f1 score: 0.11897, precison: 0.06815, recall: 0.46753\n",
      "global step 3250, epoch: 3, batch: 196, loss: 12.37205, auc: 0.49369, f1 score: 0.08433, speed: 0.04 step/s\n",
      "global step 3260, epoch: 3, batch: 206, loss: 7.69645, auc: 0.47695, f1 score: 0.08360, speed: 0.23 step/s\n",
      "global step 3270, epoch: 3, batch: 216, loss: 4.95855, auc: 0.48361, f1 score: 0.08235, speed: 0.24 step/s\n",
      "global step 3280, epoch: 3, batch: 226, loss: 3.36134, auc: 0.48770, f1 score: 0.08043, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 2.80680, auc: 0.40755, f1 score: 0.08454, precison: 0.04451, recall: 0.83963\n",
      "global step 3290, epoch: 3, batch: 236, loss: 3.54658, auc: 0.50789, f1 score: 0.08074, speed: 0.04 step/s\n",
      "global step 3300, epoch: 3, batch: 246, loss: 3.49094, auc: 0.52078, f1 score: 0.08503, speed: 0.23 step/s\n",
      "global step 3310, epoch: 3, batch: 256, loss: 5.06637, auc: 0.51686, f1 score: 0.08404, speed: 0.23 step/s\n",
      "global step 3320, epoch: 3, batch: 266, loss: 2.38631, auc: 0.51222, f1 score: 0.08421, speed: 0.22 step/s\n",
      "eval loss: 2.05143, auc: 0.61151, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 3330, epoch: 3, batch: 276, loss: 3.59837, auc: 0.48816, f1 score: 0.08136, speed: 0.04 step/s\n",
      "global step 3340, epoch: 3, batch: 286, loss: 2.90734, auc: 0.50742, f1 score: 0.08503, speed: 0.24 step/s\n",
      "global step 3350, epoch: 3, batch: 296, loss: 2.61356, auc: 0.51136, f1 score: 0.08426, speed: 0.22 step/s\n",
      "global step 3360, epoch: 3, batch: 306, loss: 3.66315, auc: 0.51281, f1 score: 0.08460, speed: 0.21 step/s\n",
      "eval loss: 1.45099, auc: 0.56443, f1 score: 0.14792, precison: 0.09121, recall: 0.39106\n",
      "global step 3370, epoch: 3, batch: 316, loss: 1.76431, auc: 0.49813, f1 score: 0.08273, speed: 0.04 step/s\n",
      "global step 3380, epoch: 3, batch: 326, loss: 1.84035, auc: 0.50721, f1 score: 0.08327, speed: 0.23 step/s\n",
      "global step 3390, epoch: 3, batch: 336, loss: 1.91592, auc: 0.51118, f1 score: 0.08315, speed: 0.23 step/s\n",
      "global step 3400, epoch: 3, batch: 346, loss: 1.57294, auc: 0.51508, f1 score: 0.08422, speed: 0.23 step/s\n",
      "eval loss: 0.60933, auc: 0.53337, f1 score: 0.10210, precison: 0.05477, recall: 0.75146\n",
      "global step 3410, epoch: 3, batch: 356, loss: 1.37069, auc: 0.51866, f1 score: 0.08525, speed: 0.04 step/s\n",
      "global step 3420, epoch: 3, batch: 366, loss: 1.42079, auc: 0.51515, f1 score: 0.08328, speed: 0.24 step/s\n",
      "global step 3430, epoch: 3, batch: 376, loss: 2.51594, auc: 0.51290, f1 score: 0.08458, speed: 0.23 step/s\n",
      "global step 3440, epoch: 3, batch: 386, loss: 2.56120, auc: 0.51473, f1 score: 0.08488, speed: 0.21 step/s\n",
      "eval loss: 2.23947, auc: 0.45120, f1 score: 0.08504, precison: 0.04443, recall: 0.99052\n",
      "global step 3450, epoch: 3, batch: 396, loss: 2.60135, auc: 0.51121, f1 score: 0.10034, speed: 0.04 step/s\n",
      "global step 3460, epoch: 3, batch: 406, loss: 1.88026, auc: 0.52035, f1 score: 0.08708, speed: 0.24 step/s\n",
      "global step 3470, epoch: 3, batch: 416, loss: 1.46472, auc: 0.51909, f1 score: 0.09278, speed: 0.23 step/s\n",
      "global step 3480, epoch: 3, batch: 426, loss: 1.61536, auc: 0.52074, f1 score: 0.08837, speed: 0.22 step/s\n",
      "eval loss: 1.87244, auc: 0.41508, f1 score: 0.08987, precison: 0.04721, recall: 0.93111\n",
      "global step 3490, epoch: 3, batch: 436, loss: 0.98665, auc: 0.52934, f1 score: 0.08742, speed: 0.04 step/s\n",
      "global step 3500, epoch: 3, batch: 446, loss: 1.23057, auc: 0.52033, f1 score: 0.08936, speed: 0.24 step/s\n",
      "global step 3510, epoch: 3, batch: 456, loss: 1.31322, auc: 0.52421, f1 score: 0.08408, speed: 0.23 step/s\n",
      "global step 3520, epoch: 3, batch: 466, loss: 1.32071, auc: 0.52359, f1 score: 0.08892, speed: 0.21 step/s\n",
      "eval loss: 1.09360, auc: 0.66075, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 3530, epoch: 3, batch: 476, loss: 1.36273, auc: 0.57080, f1 score: 0.10580, speed: 0.04 step/s\n",
      "global step 3540, epoch: 3, batch: 486, loss: 0.81221, auc: 0.55415, f1 score: 0.09408, speed: 0.23 step/s\n",
      "global step 3550, epoch: 3, batch: 496, loss: 0.78376, auc: 0.55350, f1 score: 0.09724, speed: 0.24 step/s\n",
      "global step 3560, epoch: 3, batch: 506, loss: 0.65305, auc: 0.55103, f1 score: 0.09390, speed: 0.23 step/s\n",
      "eval loss: 0.58401, auc: 0.54023, f1 score: 0.11437, precison: 0.06830, recall: 0.35140\n",
      "global step 3570, epoch: 3, batch: 516, loss: 1.03768, auc: 0.51758, f1 score: 0.11581, speed: 0.04 step/s\n",
      "global step 3580, epoch: 3, batch: 526, loss: 0.46569, auc: 0.52317, f1 score: 0.11218, speed: 0.23 step/s\n",
      "global step 3590, epoch: 3, batch: 536, loss: 0.74323, auc: 0.51819, f1 score: 0.10142, speed: 0.24 step/s\n",
      "global step 3600, epoch: 3, batch: 546, loss: 0.57982, auc: 0.52239, f1 score: 0.09117, speed: 0.23 step/s\n",
      "eval loss: 0.64631, auc: 0.54198, f1 score: 0.10958, precison: 0.06012, recall: 0.61858\n",
      "global step 3610, epoch: 3, batch: 556, loss: 1.42462, auc: 0.50969, f1 score: 0.10497, speed: 0.04 step/s\n",
      "global step 3620, epoch: 3, batch: 566, loss: 0.93264, auc: 0.50387, f1 score: 0.09978, speed: 0.25 step/s\n",
      "global step 3630, epoch: 3, batch: 576, loss: 0.61067, auc: 0.51441, f1 score: 0.11302, speed: 0.22 step/s\n",
      "global step 3640, epoch: 3, batch: 586, loss: 0.54443, auc: 0.51416, f1 score: 0.11001, speed: 0.24 step/s\n",
      "eval loss: 0.59783, auc: 0.57348, f1 score: 0.20928, precison: 0.16565, recall: 0.28409\n",
      "global step 3650, epoch: 3, batch: 596, loss: 0.83641, auc: 0.53733, f1 score: 0.11208, speed: 0.04 step/s\n",
      "global step 3660, epoch: 3, batch: 606, loss: 0.76588, auc: 0.53775, f1 score: 0.10989, speed: 0.23 step/s\n",
      "global step 3670, epoch: 3, batch: 616, loss: 0.60345, auc: 0.52675, f1 score: 0.10038, speed: 0.24 step/s\n",
      "global step 3680, epoch: 3, batch: 626, loss: 0.78393, auc: 0.52627, f1 score: 0.09296, speed: 0.23 step/s\n",
      "eval loss: 0.60203, auc: 0.60661, f1 score: 0.22557, precison: 0.17855, recall: 0.30621\n",
      "global step 3690, epoch: 3, batch: 636, loss: 0.65132, auc: 0.53953, f1 score: 0.09463, speed: 0.04 step/s\n",
      "global step 3700, epoch: 3, batch: 646, loss: 1.30664, auc: 0.52296, f1 score: 0.09310, speed: 0.24 step/s\n",
      "global step 3710, epoch: 3, batch: 656, loss: 0.91923, auc: 0.52888, f1 score: 0.09524, speed: 0.24 step/s\n",
      "global step 3720, epoch: 3, batch: 666, loss: 0.69927, auc: 0.53442, f1 score: 0.09822, speed: 0.24 step/s\n",
      "eval loss: 0.76146, auc: 0.56571, f1 score: 0.11792, precison: 0.06756, recall: 0.46342\n",
      "global step 3730, epoch: 3, batch: 676, loss: 0.71404, auc: 0.54121, f1 score: 0.08997, speed: 0.04 step/s\n",
      "global step 3740, epoch: 3, batch: 686, loss: 0.54150, auc: 0.53654, f1 score: 0.08698, speed: 0.25 step/s\n",
      "global step 3750, epoch: 3, batch: 696, loss: 1.23880, auc: 0.52848, f1 score: 0.08651, speed: 0.24 step/s\n",
      "global step 3760, epoch: 3, batch: 706, loss: 0.67275, auc: 0.53156, f1 score: 0.09301, speed: 0.24 step/s\n",
      "eval loss: 0.35432, auc: 0.38314, f1 score: 0.08091, precison: 0.04220, recall: 0.97709\n",
      "global step 3770, epoch: 3, batch: 716, loss: 1.31224, auc: 0.51398, f1 score: 0.11594, speed: 0.04 step/s\n",
      "global step 3780, epoch: 3, batch: 726, loss: 0.90383, auc: 0.51860, f1 score: 0.11334, speed: 0.24 step/s\n",
      "global step 3790, epoch: 3, batch: 736, loss: 0.75936, auc: 0.51672, f1 score: 0.10185, speed: 0.24 step/s\n",
      "global step 3800, epoch: 3, batch: 746, loss: 0.90732, auc: 0.51544, f1 score: 0.09870, speed: 0.22 step/s\n",
      "eval loss: 0.99599, auc: 0.38351, f1 score: 0.08083, precison: 0.04216, recall: 0.97614\n",
      "global step 3810, epoch: 3, batch: 756, loss: 1.04588, auc: 0.50596, f1 score: 0.09674, speed: 0.04 step/s\n",
      "global step 3820, epoch: 3, batch: 766, loss: 0.56676, auc: 0.50341, f1 score: 0.09203, speed: 0.24 step/s\n",
      "global step 3830, epoch: 3, batch: 776, loss: 0.92902, auc: 0.50696, f1 score: 0.09496, speed: 0.22 step/s\n",
      "global step 3840, epoch: 3, batch: 786, loss: 0.73428, auc: 0.50790, f1 score: 0.09625, speed: 0.23 step/s\n",
      "eval loss: 0.71754, auc: 0.61881, f1 score: 0.24885, precison: 0.19698, recall: 0.33781\n",
      "global step 3850, epoch: 3, batch: 796, loss: 1.03824, auc: 0.52173, f1 score: 0.08636, speed: 0.04 step/s\n",
      "global step 3860, epoch: 3, batch: 806, loss: 1.90061, auc: 0.52048, f1 score: 0.08448, speed: 0.25 step/s\n",
      "global step 3870, epoch: 3, batch: 816, loss: 1.43084, auc: 0.52635, f1 score: 0.08475, speed: 0.23 step/s\n",
      "global step 3880, epoch: 3, batch: 826, loss: 1.34304, auc: 0.52590, f1 score: 0.08342, speed: 0.22 step/s\n",
      "eval loss: 0.73351, auc: 0.60764, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 3890, epoch: 3, batch: 836, loss: 0.98239, auc: 0.52335, f1 score: 0.09158, speed: 0.04 step/s\n",
      "global step 3900, epoch: 3, batch: 846, loss: 1.02419, auc: 0.52365, f1 score: 0.08853, speed: 0.25 step/s\n",
      "global step 3910, epoch: 3, batch: 856, loss: 0.77674, auc: 0.53656, f1 score: 0.08993, speed: 0.24 step/s\n",
      "global step 3920, epoch: 3, batch: 866, loss: 1.16821, auc: 0.53552, f1 score: 0.08813, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 1.03034, auc: 0.40461, f1 score: 0.08486, precison: 0.04441, recall: 0.95197\n",
      "global step 3930, epoch: 3, batch: 876, loss: 1.41608, auc: 0.53589, f1 score: 0.09394, speed: 0.04 step/s\n",
      "global step 3940, epoch: 3, batch: 886, loss: 1.63146, auc: 0.52347, f1 score: 0.08722, speed: 0.23 step/s\n",
      "global step 3950, epoch: 3, batch: 896, loss: 1.65955, auc: 0.52016, f1 score: 0.08767, speed: 0.23 step/s\n",
      "global step 3960, epoch: 3, batch: 906, loss: 0.96283, auc: 0.52294, f1 score: 0.09498, speed: 0.24 step/s\n",
      "eval loss: 0.80610, auc: 0.44414, f1 score: 0.08603, precison: 0.04511, recall: 0.92827\n",
      "global step 3970, epoch: 3, batch: 916, loss: 1.14845, auc: 0.54123, f1 score: 0.11380, speed: 0.04 step/s\n",
      "global step 3980, epoch: 3, batch: 926, loss: 1.02579, auc: 0.53040, f1 score: 0.09757, speed: 0.25 step/s\n",
      "global step 3990, epoch: 3, batch: 936, loss: 1.28595, auc: 0.53888, f1 score: 0.09412, speed: 0.23 step/s\n",
      "global step 4000, epoch: 3, batch: 946, loss: 1.58919, auc: 0.53299, f1 score: 0.08829, speed: 0.22 step/s\n",
      "eval loss: 1.49667, auc: 0.44253, f1 score: 0.08732, precison: 0.04620, recall: 0.79238\n",
      "global step 4010, epoch: 3, batch: 956, loss: 2.68579, auc: 0.52680, f1 score: 0.08748, speed: 0.04 step/s\n",
      "global step 4020, epoch: 3, batch: 966, loss: 1.14217, auc: 0.52218, f1 score: 0.08407, speed: 0.24 step/s\n",
      "global step 4030, epoch: 3, batch: 976, loss: 1.04504, auc: 0.52557, f1 score: 0.08501, speed: 0.25 step/s\n",
      "global step 4040, epoch: 3, batch: 986, loss: 2.22961, auc: 0.51715, f1 score: 0.08337, speed: 0.23 step/s\n",
      "eval loss: 2.15932, auc: 0.48269, f1 score: 0.08631, precison: 0.04525, recall: 0.93127\n",
      "global step 4050, epoch: 3, batch: 996, loss: 1.16120, auc: 0.49845, f1 score: 0.08148, speed: 0.04 step/s\n",
      "global step 4060, epoch: 3, batch: 1006, loss: 1.45090, auc: 0.51055, f1 score: 0.08265, speed: 0.23 step/s\n",
      "global step 4070, epoch: 3, batch: 1016, loss: 1.70710, auc: 0.51972, f1 score: 0.08468, speed: 0.23 step/s\n",
      "global step 4080, epoch: 3, batch: 1026, loss: 2.10813, auc: 0.51815, f1 score: 0.08316, speed: 0.24 step/s\n",
      "eval loss: 2.12818, auc: 0.51583, f1 score: 0.10297, precison: 0.05610, recall: 0.62538\n",
      "global step 4090, epoch: 3, batch: 1036, loss: 1.48900, auc: 0.51474, f1 score: 0.08409, speed: 0.04 step/s\n",
      "global step 4100, epoch: 3, batch: 1046, loss: 2.75047, auc: 0.53299, f1 score: 0.08648, speed: 0.24 step/s\n",
      "global step 4110, epoch: 3, batch: 1056, loss: 2.73053, auc: 0.52869, f1 score: 0.08481, speed: 0.23 step/s\n",
      "global step 4120, epoch: 3, batch: 1066, loss: 0.86068, auc: 0.52776, f1 score: 0.08421, speed: 0.23 step/s\n",
      "eval loss: 0.89445, auc: 0.43168, f1 score: 0.08697, precison: 0.04560, recall: 0.93838\n",
      "global step 4130, epoch: 3, batch: 1076, loss: 2.12882, auc: 0.51295, f1 score: 0.09343, speed: 0.04 step/s\n",
      "global step 4140, epoch: 3, batch: 1086, loss: 1.48876, auc: 0.51327, f1 score: 0.10434, speed: 0.23 step/s\n",
      "global step 4150, epoch: 3, batch: 1096, loss: 1.85663, auc: 0.52605, f1 score: 0.10263, speed: 0.23 step/s\n",
      "global step 4160, epoch: 3, batch: 1106, loss: 1.63655, auc: 0.52996, f1 score: 0.10404, speed: 0.23 step/s\n",
      "eval loss: 1.46507, auc: 0.42167, f1 score: 0.08119, precison: 0.04249, recall: 0.91089\n",
      "global step 4170, epoch: 3, batch: 1116, loss: 1.92370, auc: 0.51294, f1 score: 0.08482, speed: 0.04 step/s\n",
      "global step 4180, epoch: 3, batch: 1126, loss: 2.11359, auc: 0.53709, f1 score: 0.09150, speed: 0.23 step/s\n",
      "global step 4190, epoch: 3, batch: 1136, loss: 1.37100, auc: 0.53447, f1 score: 0.08796, speed: 0.23 step/s\n",
      "global step 4200, epoch: 3, batch: 1146, loss: 1.77308, auc: 0.53406, f1 score: 0.08794, speed: 0.23 step/s\n",
      "eval loss: 1.38150, auc: 0.56364, f1 score: 0.10899, precison: 0.06027, recall: 0.56849\n",
      "global step 4210, epoch: 3, batch: 1156, loss: 1.98378, auc: 0.50558, f1 score: 0.08468, speed: 0.04 step/s\n",
      "global step 4220, epoch: 3, batch: 1166, loss: 2.09510, auc: 0.50434, f1 score: 0.08116, speed: 0.24 step/s\n",
      "global step 4230, epoch: 3, batch: 1176, loss: 3.32291, auc: 0.50993, f1 score: 0.08243, speed: 0.24 step/s\n",
      "global step 4240, epoch: 3, batch: 1186, loss: 1.50093, auc: 0.51796, f1 score: 0.08314, speed: 0.23 step/s\n",
      "eval loss: 1.54358, auc: 0.58929, f1 score: 0.11637, precison: 0.06303, recall: 0.75668\n",
      "global step 4250, epoch: 3, batch: 1196, loss: 2.21629, auc: 0.51719, f1 score: 0.13258, speed: 0.04 step/s\n",
      "global step 4260, epoch: 3, batch: 1206, loss: 1.76808, auc: 0.51245, f1 score: 0.11594, speed: 0.22 step/s\n",
      "global step 4270, epoch: 3, batch: 1216, loss: 1.40360, auc: 0.51737, f1 score: 0.11199, speed: 0.23 step/s\n",
      "global step 4280, epoch: 3, batch: 1226, loss: 1.22655, auc: 0.51848, f1 score: 0.09242, speed: 0.22 step/s\n",
      "eval loss: 0.89771, auc: 0.65211, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 4290, epoch: 3, batch: 1236, loss: 1.58757, auc: 0.52517, f1 score: 0.10066, speed: 0.04 step/s\n",
      "global step 4300, epoch: 3, batch: 1246, loss: 1.46072, auc: 0.52695, f1 score: 0.09611, speed: 0.24 step/s\n",
      "global step 4310, epoch: 3, batch: 1256, loss: 1.45082, auc: 0.52029, f1 score: 0.10032, speed: 0.22 step/s\n",
      "global step 4320, epoch: 3, batch: 1266, loss: 0.81709, auc: 0.51948, f1 score: 0.10092, speed: 0.23 step/s\n",
      "eval loss: 0.90044, auc: 0.57798, f1 score: 0.21998, precison: 0.17413, recall: 0.29863\n",
      "global step 4330, epoch: 3, batch: 1276, loss: 0.98951, auc: 0.51934, f1 score: 0.10662, speed: 0.04 step/s\n",
      "global step 4340, epoch: 3, batch: 1286, loss: 1.79072, auc: 0.52737, f1 score: 0.10270, speed: 0.24 step/s\n",
      "global step 4350, epoch: 3, batch: 1296, loss: 1.78091, auc: 0.52700, f1 score: 0.10325, speed: 0.23 step/s\n",
      "global step 4360, epoch: 3, batch: 1306, loss: 1.83856, auc: 0.51708, f1 score: 0.09773, speed: 0.22 step/s\n",
      "eval loss: 2.07885, auc: 0.64527, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 4370, epoch: 3, batch: 1316, loss: 1.64146, auc: 0.53175, f1 score: 0.08693, speed: 0.04 step/s\n",
      "global step 4380, epoch: 3, batch: 1326, loss: 1.72489, auc: 0.53299, f1 score: 0.08793, speed: 0.24 step/s\n",
      "global step 4390, epoch: 3, batch: 1336, loss: 1.12981, auc: 0.53008, f1 score: 0.08608, speed: 0.24 step/s\n",
      "global step 4400, epoch: 3, batch: 1346, loss: 1.37015, auc: 0.53164, f1 score: 0.08815, speed: 0.22 step/s\n",
      "eval loss: 1.26697, auc: 0.32910, f1 score: 0.07997, precison: 0.04165, recall: 1.00000\n",
      "global step 4410, epoch: 3, batch: 1356, loss: 2.92432, auc: 0.53189, f1 score: 0.09448, speed: 0.04 step/s\n",
      "global step 4420, epoch: 3, batch: 1366, loss: 2.16912, auc: 0.51751, f1 score: 0.08557, speed: 0.23 step/s\n",
      "global step 4430, epoch: 3, batch: 1376, loss: 1.58475, auc: 0.51295, f1 score: 0.08400, speed: 0.24 step/s\n",
      "global step 4440, epoch: 3, batch: 1386, loss: 2.40979, auc: 0.50939, f1 score: 0.08392, speed: 0.23 step/s\n",
      "eval loss: 2.48461, auc: 0.50800, f1 score: 0.09388, precison: 0.04968, recall: 0.85195\n",
      "global step 4450, epoch: 3, batch: 1396, loss: 1.62356, auc: 0.52122, f1 score: 0.08876, speed: 0.04 step/s\n",
      "global step 4460, epoch: 3, batch: 1406, loss: 1.64131, auc: 0.52458, f1 score: 0.08683, speed: 0.23 step/s\n",
      "global step 4470, epoch: 3, batch: 1416, loss: 2.31065, auc: 0.52528, f1 score: 0.08768, speed: 0.23 step/s\n",
      "global step 4480, epoch: 3, batch: 1426, loss: 1.71017, auc: 0.52377, f1 score: 0.08671, speed: 0.22 step/s\n",
      "eval loss: 1.83324, auc: 0.56480, f1 score: 0.12870, precison: 0.07373, recall: 0.50577\n",
      "global step 4490, epoch: 3, batch: 1436, loss: 1.19388, auc: 0.54965, f1 score: 0.09369, speed: 0.04 step/s\n",
      "global step 4500, epoch: 3, batch: 1446, loss: 1.41274, auc: 0.53424, f1 score: 0.08913, speed: 0.23 step/s\n",
      "global step 4510, epoch: 3, batch: 1456, loss: 1.62906, auc: 0.53014, f1 score: 0.08814, speed: 0.23 step/s\n",
      "global step 4520, epoch: 3, batch: 1466, loss: 1.34293, auc: 0.53000, f1 score: 0.08778, speed: 0.22 step/s\n",
      "eval loss: 2.00273, auc: 0.60472, f1 score: 0.19549, precison: 0.13574, recall: 0.34919\n",
      "global step 4530, epoch: 3, batch: 1476, loss: 2.71667, auc: 0.52234, f1 score: 0.08824, speed: 0.04 step/s\n",
      "global step 4540, epoch: 3, batch: 1486, loss: 1.28666, auc: 0.53741, f1 score: 0.09159, speed: 0.23 step/s\n",
      "global step 4550, epoch: 3, batch: 1496, loss: 1.61559, auc: 0.51772, f1 score: 0.08650, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 4560, epoch: 3, batch: 1506, loss: 1.16999, auc: 0.51840, f1 score: 0.08765, speed: 0.22 step/s\n",
      "eval loss: 1.19964, auc: 0.52657, f1 score: 0.17231, precison: 0.11965, recall: 0.30779\n",
      "global step 4570, epoch: 3, batch: 1516, loss: 1.86271, auc: 0.55032, f1 score: 0.10816, speed: 0.04 step/s\n",
      "global step 4580, epoch: 3, batch: 1526, loss: 1.92682, auc: 0.54441, f1 score: 0.09834, speed: 0.23 step/s\n"
     ]
    }
   ],
   "source": [
    "epochs = 3 # training times\n",
    "ckpt_dir = \"ernie_ckpt\" # Folder for saving model parameters during training\n",
    "\n",
    "global_step = 0  # Number of iterations\n",
    "tic_train = time.time()\n",
    "best_f1_score = 0\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # Calculate model output, loss function value, classification probability value, accuracy, f1 score.\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        metric.update(probs, labels)\n",
    "        auc, f1_score, _, _ = metric.accumulate()\n",
    "\n",
    "        # Print the loss function value, accuracy, f1 score, and computation speed for each 10 iterations.\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, auc, f1_score,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # Reverse gradient passback with updated parameters.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # Every 40 iterations, evaluate the current trained model, save the current best model parameters and word list of the word splitter, etc.\n",
    "        if global_step % 40 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            eval_f1_score = evaluate(model, criterion, metric, test_data_loader, label_vocab, if_return_results=False)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdef08",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ad41c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0 performance on GoEmotions micro-emotion 28 classification test set： eval loss: 0.35317, auc: 0.67353, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n"
     ]
    }
   ],
   "source": [
    "# Load the optimal parameters of the trained model.\n",
    "model.set_dict(paddle.load('ernie_ckpt/model_state.pdparams'))\n",
    "\n",
    "# Load the parameters of the previously trained model.\n",
    "# model.set_dict(paddle.load('/home/aistudio/work/model_state.pdparams'))\n",
    "\n",
    "# Model Validation.\n",
    "print(\"ERNIE 3.0 performance on GoEmotions micro-emotion 28 classification test set：\", end= \" \")\n",
    "results = evaluate(model, criterion, metric, test_data_loader, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133337e7",
   "metadata": {},
   "source": [
    "# Use the model to make predictions about the sentiment contained in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3aa3380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loading and processing functions.\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "def convert_example(example, tokenizer, max_seq_length=64, is_test=False):\n",
    "    qtconcat = example[\"text\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# Define the model prediction function.\n",
    "def predict(model, data, tokenizer, label_vocab, batch_size=1, max_seq=64):\n",
    "    examples = []\n",
    "    # Process input data (list format) into a format acceptable to the model.\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=max_seq,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.sigmoid(logits)\n",
    "        probs = probs.tolist()\n",
    "        # The results were processed by selecting the sentiment categories with probability greater than 0.5.\n",
    "        for prob in probs:\n",
    "            result = []\n",
    "            for c, pred in enumerate(prob):\n",
    "                if pred > 0.5:\n",
    "                    result.append(label_vocab[c])\n",
    "            results.append(','.join(result))\n",
    "    return results  # Return prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3a0bf9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: You do a great job! \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Lets have fun! \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: You shut your mouth \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: You are so annoyed \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: You are allowed to do this \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Are you feeling well? \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: This problem is so hard and I cannot solve this problem \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Why would I do that? \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I want this gift so much \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am very disappointed by everything you have done to me \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: You are not admitted to the college. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Thats absolutely disgusting. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Thats so embarrassing. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am so excited \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am so scared of skydiving \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Thank you. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: My grandpa passed away \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Happy Birthday \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I love you so much \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am so nervous. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: It is just so so. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Successful people only focus on giving their best effort. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am so proud of you. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: Thank you for letting me realizing this rule. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: You are doing better than you think you are \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am guilty. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am so sad. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n",
      "Text: I am so surprised that you made it. \t Labels: admiration,amusement,approval,confusion,disapproval,disgust,excitement,fear,joy,nervousness,optimism,neutral\n"
     ]
    }
   ],
   "source": [
    "# Define the text data to be subjected to micro-sentiment analysis.\n",
    "data = [\n",
    "    # 0 admiration\n",
    "    {\"text\": 'You do a great job!'},\n",
    "    # 1 amusement\n",
    "    {\"text\": 'Lets have fun!'},\n",
    "    # 2 anger\n",
    "    {\"text\":\"You shut your mouth\"},\n",
    "    # 3 annoyance\n",
    "    {\"text\": 'You are so annoyed'},\n",
    "    # 4 approval\n",
    "    {\"text\": 'You are allowed to do this'},\n",
    "    # 5 caring & # 7 Curiosity\n",
    "    {\"text\": 'Are you feeling well?'},\n",
    "    # 6 confusion\n",
    "    {\"text\": 'This problem is so hard and I cannot solve this problem'},\n",
    "    # 7 curiosity\n",
    "    {\"text\":'Why would I do that?'},\n",
    "    # 8 desire\n",
    "    {\"text\": 'I want this gift so much'},\n",
    "    # 9 disappointment\n",
    "    {\"text\": 'I am very disappointed by everything you have done to me'},\n",
    "    # 10 disapproval\n",
    "    {\"text\": 'You are not admitted to the college.'},\n",
    "    # 11 disgust\n",
    "    {\"text\": 'Thats absolutely disgusting.'},\n",
    "    # 12 embarrassment\n",
    "    {\"text\": 'Thats so embarrassing.'},\n",
    "    # 13 excitement\n",
    "    {\"text\": 'I am so excited'},\n",
    "    # 14 fear\n",
    "    {\"text\": 'I am so scared of skydiving'},\n",
    "    # 15 gratitude\n",
    "    {\"text\":\"Thank you.\"},\n",
    "    # 16 grief\n",
    "    {\"text\": 'My grandpa passed away'},\n",
    "    # 17 joy\n",
    "    {\"text\": 'Happy Birthday'},\n",
    "    # 18 love\n",
    "    {\"text\": 'I love you so much'},\n",
    "    # 19 nervousness\n",
    "    {\"text\": 'I am so nervous.'},\n",
    "    # 20 neutral\n",
    "    {\"text\": 'It is just so so.'},\n",
    "    # 21 optimism\n",
    "    {\"text\": 'Successful people only focus on giving their best effort.'},\n",
    "    # 22 pride\n",
    "    {\"text\": 'I am so proud of you.'},\n",
    "    # 23 realization\n",
    "    {\"text\": 'Thank you for letting me realizing this rule.'},\n",
    "    # 24 relief \n",
    "    {\"text\": 'You are doing better than you think you are'},\n",
    "    # 25 remorse\n",
    "    {\"text\": 'I am guilty.'},\n",
    "    # 26 sadness\n",
    "    {\"text\": 'I am so sad.'},\n",
    "    # 27 surprise\n",
    "    {\"text\": 'I am so surprised that you made it.'},\n",
    "]\n",
    "\n",
    "# Model Predictions.\n",
    "labels =  predict(model, data, tokenizer, label_vocab, batch_size=1)\n",
    "\n",
    "# Output prediction results\n",
    "for idx, text in enumerate(data):\n",
    "    print('Text: {} \\t Labels: {}'.format(text['text'], labels[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
