{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637c7bf8",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f59e76",
   "metadata": {},
   "source": [
    "## Import the pandas library and perform data reading for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a6dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv', sep='\\t', header=None)\n",
    "test = pd.read_csv('test.csv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9137ac",
   "metadata": {},
   "source": [
    "## Adding column names to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90815dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns = [\"text\",'labels']\n",
    "test.columns = [\"text\",'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dbef43",
   "metadata": {},
   "source": [
    "## The first 10 texts of the training set are read in the format \"text, labels\". The labels may contain multiple sentiment categories, each sentiment is separated by ','."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f350e950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My favourite food is anything I didn't have to...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now if he does off himself, everyone will thin...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHY THE FUCK IS BAYLESS ISOING</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To make her feel threatened</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dirty Southern Wankers</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Yes I heard abt the f bombs! That has to be wh...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>We need more boards and to create a bit more s...</td>\n",
       "      <td>8,20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Damn youtube and outrage drama is super lucrat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It might be linked to the trust factor of your...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0  My favourite food is anything I didn't have to...     27\n",
       "1  Now if he does off himself, everyone will thin...     27\n",
       "2                     WHY THE FUCK IS BAYLESS ISOING      2\n",
       "3                        To make her feel threatened     14\n",
       "4                             Dirty Southern Wankers      3\n",
       "5  OmG pEyToN iSn'T gOoD eNoUgH tO hElP uS iN tHe...     26\n",
       "6  Yes I heard abt the f bombs! That has to be wh...     15\n",
       "7  We need more boards and to create a bit more s...   8,20\n",
       "8  Damn youtube and outrage drama is super lucrat...      0\n",
       "9  It might be linked to the trust factor of your...     27"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76da08ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I’m really sorry about your situation :( Altho...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It's wonderful because it's awful. At not with.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kings fan here, good luck to you guys! Will be...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I didn't know that, thank you for teaching me ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>They got bored from haunting earth for thousan...</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thank you for asking questions and recognizing...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You’re welcome</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100%! Congrats on your job too!</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I’m sorry to hear that friend :(. It’s for the...</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Girlfriend weak as well, that jump was pathetic.</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text labels\n",
       "0  I’m really sorry about your situation :( Altho...     25\n",
       "1    It's wonderful because it's awful. At not with.      0\n",
       "2  Kings fan here, good luck to you guys! Will be...     13\n",
       "3  I didn't know that, thank you for teaching me ...     15\n",
       "4  They got bored from haunting earth for thousan...     27\n",
       "5  Thank you for asking questions and recognizing...     15\n",
       "6                                     You’re welcome     15\n",
       "7                    100%! Congrats on your job too!     15\n",
       "8  I’m sorry to hear that friend :(. It’s for the...     24\n",
       "9   Girlfriend weak as well, that jump was pathetic.     25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c68bb0",
   "metadata": {},
   "source": [
    "# Import related libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "842978b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wubowei/miniforge3/envs/nlp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/wubowei/miniforge3/envs/nlp/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import paddle\n",
    "import paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8810e",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cf320c",
   "metadata": {},
   "source": [
    "## For the 28 micro-sentiment multi-label classification scenario, where a sentence may correspond to multiple sentiment category labels, the sentiment labels of the dataset need to be transformed using One-Hot coding first, with \"0\" indicating absence and \"1\" indicating presence for each sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e354f",
   "metadata": {},
   "source": [
    "### Create sentiment label mapping relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3dfdb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = {\n",
    "    0: \"admiration\",\n",
    "    1: \"amusement\",\n",
    "    2: \"anger\",\n",
    "    3: \"annoyance\",\n",
    "    4: \"approval\",\n",
    "    5: \"caring\",\n",
    "    6: \"confusion\",\n",
    "    7: \"curiosity\",\n",
    "    8: \"desire\",\n",
    "    9: \"disappointment\",\n",
    "    10: \"disapproval\",\n",
    "    11: \"disgust\",\n",
    "    12: \"embarrassment\",\n",
    "    13: \"excitement\",\n",
    "    14: \"fear\",\n",
    "    15: \"gratitude\",\n",
    "    16: \"grief\",\n",
    "    17: \"joy\",\n",
    "    18: \"love\",\n",
    "    19: \"nervousness\",\n",
    "    20: \"optimism\",\n",
    "    21: \"pride\",\n",
    "    22: \"realization\",\n",
    "    23: \"relief\",\n",
    "    24: \"remorse\",\n",
    "    25: \"sadness\",\n",
    "    26: \"surprise\",\n",
    "    27: \"neutral\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920ee3ff",
   "metadata": {},
   "source": [
    "### Customize the dataset, read the data file, create the dataset and define the data type as MapDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf5f40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "# Clear invalid characters\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"\\r\", \"\").replace(\"\\n\", \"\")\n",
    "    text = re.sub(r\"\\\\n\\n\", \".\", text)\n",
    "    return text\n",
    "\n",
    "# Define the read data set function\n",
    "def read_custom_data(filepath, is_one_hot=True):\n",
    "    f = open(filepath)\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        data = line.strip().split('\\t')\n",
    "        # One-hot processing for 28 types of micro sentiment tags\n",
    "        if is_one_hot:\n",
    "            labels = [float(1) if str(i) in data[1].split(',') else float(0) for i in range(28)]  # 28 types\n",
    "        else:\n",
    "            labels = [int(d) for d in data[1].split(',')]\n",
    "        yield {\"text\": clean_text(data[0]), \"labels\": labels}\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e190e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dataset() to Create dataset.\n",
    "# lazy=False，The dataset is returned as a MapDataset type.\n",
    "# Pre-processing of training and validation sets.\n",
    "train_ds = load_dataset(read_custom_data, filepath='train.csv', lazy=False) \n",
    "test_ds = load_dataset(read_custom_data, filepath='test.csv', lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675b782",
   "metadata": {},
   "source": [
    "### Print dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc0c6514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datatype: <class 'paddlenlp.datasets.dataset.MapDataset'>\n",
      "training dataset example: {'text': \"My favourite food is anything I didn't have to cook myself.\", 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]}\n",
      "testing dataset example: {'text': 'I’m really sorry about your situation :( Although I love the names Sapphira, Cirilla, and Scarlett!', 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]}\n"
     ]
    }
   ],
   "source": [
    "print(\"datatype:\", type(train_ds))\n",
    "print(\"training dataset example:\", train_ds[0])\n",
    "print(\"testing dataset example:\", test_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b4596",
   "metadata": {},
   "source": [
    "## Load Chinese ERNIE 3.0 pre-training model and word splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0edba76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:04:47,191] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.modeling.ErnieForSequenceClassification'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:04:47,196] [    INFO]\u001b[0m - Model config ErnieConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"enable_recompute\": false,\n",
      "  \"fuse\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 2048,\n",
      "  \"model_type\": \"ernie\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"paddlenlp_version\": null,\n",
      "  \"pool_act\": \"tanh\",\n",
      "  \"task_id\": 0,\n",
      "  \"task_type_vocab_size\": 16,\n",
      "  \"type_vocab_size\": 4,\n",
      "  \"use_task_id\": true,\n",
      "  \"vocab_size\": 40000\n",
      "}\n",
      "\u001b[0m\n",
      "\u001b[33m[2023-04-04 15:04:50,298] [ WARNING]\u001b[0m - Some weights of the model checkpoint at ernie-3.0-medium-zh were not used when initializing ErnieForSequenceClassification: ['ernie.encoder.layers.6.self_attn.out_proj.weight', 'ernie.encoder.layers.6.self_attn.q_proj.bias', 'ernie.encoder.layers.6.self_attn.out_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.bias', 'ernie.encoder.layers.6.self_attn.v_proj.weight', 'ernie.encoder.layers.6.norm1.bias', 'ernie.encoder.layers.6.linear2.weight', 'ernie.encoder.layers.6.self_attn.k_proj.weight', 'ernie.encoder.layers.6.norm2.bias', 'ernie.encoder.layers.6.linear2.bias', 'ernie.encoder.layers.6.linear1.weight', 'ernie.encoder.layers.6.norm1.weight', 'ernie.encoder.layers.6.self_attn.k_proj.bias', 'ernie.encoder.layers.6.norm2.weight', 'ernie.encoder.layers.6.self_attn.q_proj.weight', 'ernie.encoder.layers.6.linear1.bias']\n",
      "- This IS expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[33m[2023-04-04 15:04:50,298] [ WARNING]\u001b[0m - Some weights of ErnieForSequenceClassification were not initialized from the model checkpoint at ernie-3.0-medium-zh and are newly initialized: ['classifier.bias', 'ernie.pooler.dense.bias', 'ernie.pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:04:50,299] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'ernie-3.0-medium-zh'.\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:04:50,300] [    INFO]\u001b[0m - Already cached /Users/wubowei/.paddlenlp/models/ernie-3.0-medium-zh/ernie_3.0_medium_zh_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:04:50,312] [    INFO]\u001b[0m - tokenizer config file saved in /Users/wubowei/.paddlenlp/models/ernie-3.0-medium-zh/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:04:50,312] [    INFO]\u001b[0m - Special tokens file saved in /Users/wubowei/.paddlenlp/models/ernie-3.0-medium-zh/special_tokens_map.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"ernie-3.0-medium-zh\"   # ERNIE3.0 model\n",
    "num_classes = 28  # 28 classification mission\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_classes=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320374a4",
   "metadata": {},
   "source": [
    "## Process the raw data into a model-acceptable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "464fa91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "\n",
    "from paddle.io import DataLoader, BatchSampler\n",
    "from paddlenlp.data import DataCollatorWithPadding\n",
    "\n",
    "# Data pre-processing function to convert text into integer sequences using a word splitter.\n",
    "def preprocess_function(examples, tokenizer, max_seq_length):\n",
    "    result = tokenizer(text=examples[\"text\"], max_seq_len=max_seq_length)\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "trans_func = functools.partial(preprocess_function, tokenizer=tokenizer, max_seq_length=64)\n",
    "train_ds = train_ds.map(trans_func)\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "# function is constructed to extend the different length sequences to the maximum length of the data in the batch, and then stack the data.\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "# Define the BatchSampler, select the batch size and whether to randomly jumble the DataLoader.\n",
    "train_batch_sampler = BatchSampler(train_ds, batch_size=32, shuffle=True)\n",
    "test_batch_sampler = BatchSampler(test_ds, batch_size=16, shuffle=False)\n",
    "train_data_loader = DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(dataset=test_ds, batch_sampler=test_batch_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919dd05d",
   "metadata": {},
   "source": [
    "## Define model validation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7025e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "from paddle.metric import Metric\n",
    "\n",
    "# Customize MultiLabelReport evaluation metrics.\n",
    "class MultiLabelReport(Metric):\n",
    "    \"\"\"\n",
    "    AUC and F1 Score for multi-label text classification task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name='MultiLabelReport', average='micro'):\n",
    "        super(MultiLabelReport, self).__init__()\n",
    "        self.average = average\n",
    "        self._name = name\n",
    "        self.reset()\n",
    "\n",
    "    def f1_score(self, y_prob):\n",
    "        '''\n",
    "        Returns the f1 score by searching the best threshhold\n",
    "        '''\n",
    "        best_score = 0\n",
    "        for threshold in [i * 0.01 for i in range(100)]:\n",
    "            self.y_pred = y_prob > threshold\n",
    "            score = sklearn.metrics.f1_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                precison = precision_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "                recall = recall_score(y_pred=self.y_pred, y_true=self.y_true, average=self.average)\n",
    "        return best_score, precison, recall\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets all of the metric state.\n",
    "        \"\"\"\n",
    "        self.y_prob = None\n",
    "        self.y_true = None\n",
    "\n",
    "    def update(self, probs, labels):\n",
    "        if self.y_prob is not None:\n",
    "            self.y_prob = np.append(self.y_prob, probs.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_prob = probs.numpy()\n",
    "        if self.y_true is not None:\n",
    "            self.y_true = np.append(self.y_true, labels.numpy(), axis=0)\n",
    "        else:\n",
    "            self.y_true = labels.numpy()\n",
    "\n",
    "    def accumulate(self):\n",
    "        auc = roc_auc_score(\n",
    "            y_score=self.y_prob, y_true=self.y_true, average=self.average)\n",
    "        f1_score, precison, recall = self.f1_score(y_prob=self.y_prob)\n",
    "        return auc, f1_score, precison, recall\n",
    "\n",
    "    def name(self):\n",
    "        \"\"\"\n",
    "        Returns metric name\n",
    "        \"\"\"\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968143c",
   "metadata": {},
   "source": [
    "# Building the training model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43683d",
   "metadata": {},
   "source": [
    "## Select an optimization strategy and run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76696dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# AdamW optimizer, cross-entropy loss function, custom MultiLabelReport evaluation metrics.\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=4e-4, parameters=model.parameters(), weight_decay=0.01)\n",
    "criterion = paddle.nn.BCEWithLogitsLoss()\n",
    "metric = MultiLabelReport()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0ef4a4",
   "metadata": {},
   "source": [
    "## Model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5feb0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddle\n",
    "import numpy as np\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "# Build the validation set evaluate function.\n",
    "@paddle.no_grad()\n",
    "def evaluate(model, criterion, metric, data_loader, label_vocab, if_return_results=True):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    results = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        losses.append(loss.numpy())\n",
    "        metric.update(probs, labels)\n",
    "        if if_return_results:\n",
    "            probs = probs.tolist()\n",
    "            for prob in probs:\n",
    "                result = []\n",
    "                for c, pred in enumerate(prob):\n",
    "                    if pred > 0.5:\n",
    "                        result.append(label_vocab[c])\n",
    "                results.append(','.join(result))\n",
    "\n",
    "    auc, f1_score, precison, recall = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, auc: %.5f, f1 score: %.5f, precison: %.5f, recall: %.5f\" %\n",
    "          (np.mean(losses), auc, f1_score, precison, recall))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    if if_return_results:\n",
    "        return results\n",
    "    else:\n",
    "        return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f65d194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 10, epoch: 1, batch: 10, loss: 0.34893, auc: 0.62177, f1 score: 0.13097, speed: 0.27 step/s\n",
      "global step 20, epoch: 1, batch: 20, loss: 0.26597, auc: 0.61383, f1 score: 0.13291, speed: 0.23 step/s\n",
      "global step 30, epoch: 1, batch: 30, loss: 0.20397, auc: 0.62279, f1 score: 0.13165, speed: 0.22 step/s\n",
      "global step 40, epoch: 1, batch: 40, loss: 0.19057, auc: 0.62550, f1 score: 0.13263, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:11:35,554] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:11:35,698] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:11:35,698] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.18321, auc: 0.73252, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 50, epoch: 1, batch: 50, loss: 0.18767, auc: 0.71758, f1 score: 0.27299, speed: 0.04 step/s\n",
      "global step 60, epoch: 1, batch: 60, loss: 0.16483, auc: 0.70821, f1 score: 0.28531, speed: 0.23 step/s\n",
      "global step 70, epoch: 1, batch: 70, loss: 0.15621, auc: 0.70034, f1 score: 0.27697, speed: 0.24 step/s\n",
      "global step 80, epoch: 1, batch: 80, loss: 0.16471, auc: 0.70787, f1 score: 0.28054, speed: 0.23 step/s\n",
      "eval loss: 0.15575, auc: 0.76443, f1 score: 0.30401, precison: 0.32928, recall: 0.28235\n",
      "global step 90, epoch: 1, batch: 90, loss: 0.15024, auc: 0.75344, f1 score: 0.32558, speed: 0.04 step/s\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.16127, auc: 0.75289, f1 score: 0.31159, speed: 0.25 step/s\n",
      "global step 110, epoch: 1, batch: 110, loss: 0.15715, auc: 0.75754, f1 score: 0.30391, speed: 0.24 step/s\n",
      "global step 120, epoch: 1, batch: 120, loss: 0.14657, auc: 0.75115, f1 score: 0.29792, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:24:36,230] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:24:36,406] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:24:36,407] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.15043, auc: 0.77121, f1 score: 0.30476, precison: 0.33148, recall: 0.28204\n",
      "global step 130, epoch: 1, batch: 130, loss: 0.15814, auc: 0.74970, f1 score: 0.28239, speed: 0.04 step/s\n",
      "global step 140, epoch: 1, batch: 140, loss: 0.15222, auc: 0.75630, f1 score: 0.28123, speed: 0.23 step/s\n",
      "global step 150, epoch: 1, batch: 150, loss: 0.15298, auc: 0.75554, f1 score: 0.27983, speed: 0.22 step/s\n",
      "global step 160, epoch: 1, batch: 160, loss: 0.15051, auc: 0.75523, f1 score: 0.27594, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:31:09,131] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:31:09,277] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:31:09,278] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.14904, auc: 0.77479, f1 score: 0.30629, precison: 0.33490, recall: 0.28219\n",
      "global step 170, epoch: 1, batch: 170, loss: 0.16317, auc: 0.74311, f1 score: 0.26615, speed: 0.04 step/s\n",
      "global step 180, epoch: 1, batch: 180, loss: 0.16992, auc: 0.74825, f1 score: 0.28149, speed: 0.24 step/s\n",
      "global step 190, epoch: 1, batch: 190, loss: 0.15096, auc: 0.75328, f1 score: 0.29141, speed: 0.25 step/s\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.16414, auc: 0.75439, f1 score: 0.30007, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:37:35,793] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:37:35,947] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:37:35,948] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.14644, auc: 0.78581, f1 score: 0.32059, precison: 0.40104, recall: 0.26702\n",
      "global step 210, epoch: 1, batch: 210, loss: 0.15721, auc: 0.76982, f1 score: 0.30952, speed: 0.04 step/s\n",
      "global step 220, epoch: 1, batch: 220, loss: 0.12539, auc: 0.77846, f1 score: 0.30979, speed: 0.24 step/s\n",
      "global step 230, epoch: 1, batch: 230, loss: 0.14757, auc: 0.78122, f1 score: 0.31752, speed: 0.24 step/s\n",
      "global step 240, epoch: 1, batch: 240, loss: 0.12131, auc: 0.78918, f1 score: 0.32933, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:44:02,384] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:44:02,536] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:44:02,536] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.14097, auc: 0.80364, f1 score: 0.34856, precison: 0.32944, recall: 0.37004\n",
      "global step 250, epoch: 1, batch: 250, loss: 0.16172, auc: 0.80653, f1 score: 0.37984, speed: 0.04 step/s\n",
      "global step 260, epoch: 1, batch: 260, loss: 0.15310, auc: 0.80373, f1 score: 0.37388, speed: 0.24 step/s\n",
      "global step 270, epoch: 1, batch: 270, loss: 0.14103, auc: 0.80753, f1 score: 0.37392, speed: 0.24 step/s\n",
      "global step 280, epoch: 1, batch: 280, loss: 0.12492, auc: 0.81032, f1 score: 0.38649, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:50:30,854] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:50:31,015] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:50:31,015] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.13569, auc: 0.81217, f1 score: 0.39852, precison: 0.48132, recall: 0.34002\n",
      "global step 290, epoch: 1, batch: 290, loss: 0.14816, auc: 0.82647, f1 score: 0.40780, speed: 0.04 step/s\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.13959, auc: 0.81132, f1 score: 0.38462, speed: 0.22 step/s\n",
      "global step 310, epoch: 1, batch: 310, loss: 0.12747, auc: 0.81132, f1 score: 0.38296, speed: 0.25 step/s\n",
      "global step 320, epoch: 1, batch: 320, loss: 0.12885, auc: 0.81469, f1 score: 0.37783, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 15:57:02,906] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:57:03,063] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 15:57:03,064] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.13199, auc: 0.83045, f1 score: 0.41049, precison: 0.45371, recall: 0.37478\n",
      "global step 330, epoch: 1, batch: 330, loss: 0.11933, auc: 0.81458, f1 score: 0.37462, speed: 0.04 step/s\n",
      "global step 340, epoch: 1, batch: 340, loss: 0.11628, auc: 0.82320, f1 score: 0.40478, speed: 0.24 step/s\n",
      "global step 350, epoch: 1, batch: 350, loss: 0.13378, auc: 0.82140, f1 score: 0.40257, speed: 0.23 step/s\n",
      "global step 360, epoch: 1, batch: 360, loss: 0.13204, auc: 0.82184, f1 score: 0.40467, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:03:31,298] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:03:31,451] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:03:31,451] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.12955, auc: 0.83892, f1 score: 0.43673, precison: 0.49858, recall: 0.38853\n",
      "global step 370, epoch: 1, batch: 370, loss: 0.11646, auc: 0.85968, f1 score: 0.45521, speed: 0.04 step/s\n",
      "global step 380, epoch: 1, batch: 380, loss: 0.13693, auc: 0.84215, f1 score: 0.45083, speed: 0.25 step/s\n",
      "global step 390, epoch: 1, batch: 390, loss: 0.10614, auc: 0.85170, f1 score: 0.45545, speed: 0.23 step/s\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.11115, auc: 0.85440, f1 score: 0.45885, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:10:00,331] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:10:00,490] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:10:00,491] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.12457, auc: 0.84564, f1 score: 0.47140, precison: 0.49118, recall: 0.45315\n",
      "global step 410, epoch: 1, batch: 410, loss: 0.13978, auc: 0.81312, f1 score: 0.43402, speed: 0.04 step/s\n",
      "global step 420, epoch: 1, batch: 420, loss: 0.11618, auc: 0.83651, f1 score: 0.45521, speed: 0.24 step/s\n",
      "global step 430, epoch: 1, batch: 430, loss: 0.10717, auc: 0.84874, f1 score: 0.47562, speed: 0.23 step/s\n",
      "global step 440, epoch: 1, batch: 440, loss: 0.12177, auc: 0.84707, f1 score: 0.46404, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:16:31,890] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:16:32,044] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:16:32,044] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.12334, auc: 0.86351, f1 score: 0.47149, precison: 0.49903, recall: 0.44683\n",
      "global step 450, epoch: 1, batch: 450, loss: 0.10280, auc: 0.85846, f1 score: 0.49347, speed: 0.04 step/s\n",
      "global step 460, epoch: 1, batch: 460, loss: 0.11194, auc: 0.86858, f1 score: 0.48962, speed: 0.25 step/s\n",
      "global step 470, epoch: 1, batch: 470, loss: 0.14453, auc: 0.86261, f1 score: 0.47516, speed: 0.23 step/s\n",
      "global step 480, epoch: 1, batch: 480, loss: 0.13176, auc: 0.86265, f1 score: 0.46916, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:23:04,846] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:23:05,014] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:23:05,015] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.11945, auc: 0.87335, f1 score: 0.48865, precison: 0.53368, recall: 0.45062\n",
      "global step 490, epoch: 1, batch: 490, loss: 0.13874, auc: 0.86630, f1 score: 0.47820, speed: 0.04 step/s\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.12427, auc: 0.87764, f1 score: 0.48404, speed: 0.24 step/s\n",
      "global step 510, epoch: 1, batch: 510, loss: 0.11405, auc: 0.87521, f1 score: 0.49320, speed: 0.23 step/s\n",
      "global step 520, epoch: 1, batch: 520, loss: 0.10115, auc: 0.87538, f1 score: 0.49864, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:29:33,639] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:29:33,806] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:29:33,807] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.11653, auc: 0.87469, f1 score: 0.49937, precison: 0.53421, recall: 0.46879\n",
      "global step 530, epoch: 1, batch: 530, loss: 0.13086, auc: 0.86039, f1 score: 0.46429, speed: 0.04 step/s\n",
      "global step 540, epoch: 1, batch: 540, loss: 0.11535, auc: 0.86566, f1 score: 0.45883, speed: 0.23 step/s\n",
      "global step 550, epoch: 1, batch: 550, loss: 0.11971, auc: 0.87186, f1 score: 0.47783, speed: 0.24 step/s\n",
      "global step 560, epoch: 1, batch: 560, loss: 0.10962, auc: 0.87286, f1 score: 0.47873, speed: 0.23 step/s\n",
      "eval loss: 0.11507, auc: 0.88976, f1 score: 0.49815, precison: 0.50662, recall: 0.48997\n",
      "global step 570, epoch: 1, batch: 570, loss: 0.10419, auc: 0.87835, f1 score: 0.47632, speed: 0.04 step/s\n",
      "global step 580, epoch: 1, batch: 580, loss: 0.10976, auc: 0.88253, f1 score: 0.48631, speed: 0.24 step/s\n",
      "global step 590, epoch: 1, batch: 590, loss: 0.11871, auc: 0.88089, f1 score: 0.48556, speed: 0.23 step/s\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.13045, auc: 0.88157, f1 score: 0.49481, speed: 0.22 step/s\n",
      "eval loss: 0.11410, auc: 0.88295, f1 score: 0.49743, precison: 0.52424, recall: 0.47322\n",
      "global step 610, epoch: 1, batch: 610, loss: 0.10437, auc: 0.88276, f1 score: 0.49524, speed: 0.04 step/s\n",
      "global step 620, epoch: 1, batch: 620, loss: 0.13238, auc: 0.88095, f1 score: 0.49205, speed: 0.23 step/s\n",
      "global step 630, epoch: 1, batch: 630, loss: 0.10630, auc: 0.88287, f1 score: 0.50617, speed: 0.22 step/s\n",
      "global step 640, epoch: 1, batch: 640, loss: 0.11087, auc: 0.88643, f1 score: 0.51446, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:49:08,312] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:49:08,471] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:49:08,472] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.11257, auc: 0.88970, f1 score: 0.50302, precison: 0.50000, recall: 0.50608\n",
      "global step 650, epoch: 1, batch: 650, loss: 0.10098, auc: 0.89391, f1 score: 0.52874, speed: 0.04 step/s\n",
      "global step 660, epoch: 1, batch: 660, loss: 0.08246, auc: 0.89636, f1 score: 0.53118, speed: 0.23 step/s\n",
      "global step 670, epoch: 1, batch: 670, loss: 0.08726, auc: 0.90022, f1 score: 0.53244, speed: 0.23 step/s\n",
      "global step 680, epoch: 1, batch: 680, loss: 0.08934, auc: 0.89808, f1 score: 0.53405, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 16:55:39,280] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:55:39,439] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 16:55:39,440] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.11056, auc: 0.90414, f1 score: 0.50975, precison: 0.53175, recall: 0.48949\n",
      "global step 690, epoch: 1, batch: 690, loss: 0.11322, auc: 0.89801, f1 score: 0.52334, speed: 0.04 step/s\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.12200, auc: 0.89073, f1 score: 0.50336, speed: 0.24 step/s\n",
      "global step 710, epoch: 1, batch: 710, loss: 0.10780, auc: 0.89208, f1 score: 0.50564, speed: 0.24 step/s\n",
      "global step 720, epoch: 1, batch: 720, loss: 0.10726, auc: 0.89578, f1 score: 0.51436, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:02:08,604] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:02:08,763] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:02:08,764] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10907, auc: 0.90098, f1 score: 0.51901, precison: 0.53526, recall: 0.50371\n",
      "global step 730, epoch: 1, batch: 730, loss: 0.11134, auc: 0.90871, f1 score: 0.48168, speed: 0.04 step/s\n",
      "global step 740, epoch: 1, batch: 740, loss: 0.09757, auc: 0.91011, f1 score: 0.52299, speed: 0.25 step/s\n",
      "global step 750, epoch: 1, batch: 750, loss: 0.11292, auc: 0.90278, f1 score: 0.52405, speed: 0.23 step/s\n",
      "global step 760, epoch: 1, batch: 760, loss: 0.11699, auc: 0.90111, f1 score: 0.51647, speed: 0.24 step/s\n",
      "eval loss: 0.11075, auc: 0.89969, f1 score: 0.50351, precison: 0.52733, recall: 0.48175\n",
      "global step 770, epoch: 1, batch: 770, loss: 0.14386, auc: 0.88428, f1 score: 0.49852, speed: 0.04 step/s\n",
      "global step 780, epoch: 1, batch: 780, loss: 0.09767, auc: 0.89319, f1 score: 0.50112, speed: 0.24 step/s\n",
      "global step 790, epoch: 1, batch: 790, loss: 0.12792, auc: 0.89054, f1 score: 0.49142, speed: 0.25 step/s\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.08179, auc: 0.89767, f1 score: 0.49902, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:15:07,762] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:15:07,924] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:15:07,924] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10642, auc: 0.90818, f1 score: 0.52447, precison: 0.55214, recall: 0.49945\n",
      "global step 810, epoch: 1, batch: 810, loss: 0.10850, auc: 0.89761, f1 score: 0.50378, speed: 0.04 step/s\n",
      "global step 820, epoch: 1, batch: 820, loss: 0.10636, auc: 0.90470, f1 score: 0.52539, speed: 0.24 step/s\n",
      "global step 830, epoch: 1, batch: 830, loss: 0.12500, auc: 0.90096, f1 score: 0.50944, speed: 0.23 step/s\n",
      "global step 840, epoch: 1, batch: 840, loss: 0.10492, auc: 0.89930, f1 score: 0.50557, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:21:38,656] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:21:38,821] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:21:38,822] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10534, auc: 0.91536, f1 score: 0.53031, precison: 0.52910, recall: 0.53152\n",
      "global step 850, epoch: 1, batch: 850, loss: 0.10578, auc: 0.91421, f1 score: 0.54753, speed: 0.04 step/s\n",
      "global step 860, epoch: 1, batch: 860, loss: 0.10936, auc: 0.90798, f1 score: 0.52120, speed: 0.24 step/s\n",
      "global step 870, epoch: 1, batch: 870, loss: 0.12142, auc: 0.91130, f1 score: 0.52886, speed: 0.24 step/s\n",
      "global step 880, epoch: 1, batch: 880, loss: 0.09608, auc: 0.91188, f1 score: 0.52639, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:28:06,339] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:28:06,501] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:28:06,502] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10297, auc: 0.91793, f1 score: 0.54230, precison: 0.55489, recall: 0.53026\n",
      "global step 890, epoch: 1, batch: 890, loss: 0.10002, auc: 0.91253, f1 score: 0.49409, speed: 0.04 step/s\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.10591, auc: 0.90943, f1 score: 0.50000, speed: 0.24 step/s\n",
      "global step 910, epoch: 1, batch: 910, loss: 0.11906, auc: 0.90937, f1 score: 0.51779, speed: 0.23 step/s\n",
      "global step 920, epoch: 1, batch: 920, loss: 0.10614, auc: 0.91277, f1 score: 0.53054, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:34:38,226] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:34:38,376] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:34:38,377] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10383, auc: 0.91184, f1 score: 0.54295, precison: 0.56121, recall: 0.52583\n",
      "global step 930, epoch: 1, batch: 930, loss: 0.12002, auc: 0.91151, f1 score: 0.54545, speed: 0.04 step/s\n",
      "global step 940, epoch: 1, batch: 940, loss: 0.10258, auc: 0.91718, f1 score: 0.54255, speed: 0.25 step/s\n",
      "global step 950, epoch: 1, batch: 950, loss: 0.11946, auc: 0.91768, f1 score: 0.53843, speed: 0.23 step/s\n",
      "global step 960, epoch: 1, batch: 960, loss: 0.09919, auc: 0.91922, f1 score: 0.53069, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:41:09,081] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:41:09,242] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:41:09,242] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10196, auc: 0.91861, f1 score: 0.54370, precison: 0.54293, recall: 0.54448\n",
      "global step 970, epoch: 1, batch: 970, loss: 0.11436, auc: 0.90726, f1 score: 0.52941, speed: 0.04 step/s\n",
      "global step 980, epoch: 1, batch: 980, loss: 0.08672, auc: 0.90841, f1 score: 0.52310, speed: 0.23 step/s\n",
      "global step 990, epoch: 1, batch: 990, loss: 0.12930, auc: 0.90647, f1 score: 0.51781, speed: 0.22 step/s\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.11649, auc: 0.90696, f1 score: 0.51777, speed: 0.23 step/s\n",
      "eval loss: 0.10174, auc: 0.92281, f1 score: 0.54277, precison: 0.56671, recall: 0.52078\n",
      "global step 1010, epoch: 1, batch: 1010, loss: 0.10900, auc: 0.90562, f1 score: 0.52022, speed: 0.04 step/s\n",
      "global step 1020, epoch: 1, batch: 1020, loss: 0.09665, auc: 0.91417, f1 score: 0.54881, speed: 0.24 step/s\n",
      "global step 1030, epoch: 1, batch: 1030, loss: 0.09838, auc: 0.91003, f1 score: 0.54587, speed: 0.24 step/s\n",
      "global step 1040, epoch: 1, batch: 1040, loss: 0.11529, auc: 0.91036, f1 score: 0.53595, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 17:54:16,073] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:54:16,231] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 17:54:16,232] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10018, auc: 0.92302, f1 score: 0.55478, precison: 0.57389, recall: 0.53689\n",
      "global step 1050, epoch: 1, batch: 1050, loss: 0.09835, auc: 0.91144, f1 score: 0.55448, speed: 0.04 step/s\n",
      "global step 1060, epoch: 1, batch: 1060, loss: 0.10060, auc: 0.91605, f1 score: 0.53361, speed: 0.25 step/s\n",
      "global step 1070, epoch: 1, batch: 1070, loss: 0.08108, auc: 0.91496, f1 score: 0.53103, speed: 0.23 step/s\n",
      "global step 1080, epoch: 1, batch: 1080, loss: 0.09407, auc: 0.91559, f1 score: 0.54020, speed: 0.23 step/s\n",
      "eval loss: 0.10018, auc: 0.92452, f1 score: 0.54501, precison: 0.54570, recall: 0.54432\n",
      "global step 1090, epoch: 1, batch: 1090, loss: 0.09010, auc: 0.92344, f1 score: 0.54041, speed: 0.04 step/s\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.10860, auc: 0.91945, f1 score: 0.54482, speed: 0.24 step/s\n",
      "global step 1110, epoch: 1, batch: 1110, loss: 0.10158, auc: 0.91987, f1 score: 0.54295, speed: 0.23 step/s\n",
      "global step 1120, epoch: 1, batch: 1120, loss: 0.12138, auc: 0.91953, f1 score: 0.53951, speed: 0.24 step/s\n",
      "eval loss: 0.09885, auc: 0.92745, f1 score: 0.55104, precison: 0.55860, recall: 0.54369\n",
      "global step 1130, epoch: 1, batch: 1130, loss: 0.08870, auc: 0.92489, f1 score: 0.57143, speed: 0.04 step/s\n",
      "global step 1140, epoch: 1, batch: 1140, loss: 0.10038, auc: 0.91994, f1 score: 0.55750, speed: 0.25 step/s\n",
      "global step 1150, epoch: 1, batch: 1150, loss: 0.13604, auc: 0.91795, f1 score: 0.54993, speed: 0.23 step/s\n",
      "global step 1160, epoch: 1, batch: 1160, loss: 0.11262, auc: 0.92117, f1 score: 0.55381, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 18:13:47,206] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:13:47,371] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:13:47,372] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09906, auc: 0.92593, f1 score: 0.55642, precison: 0.57434, recall: 0.53958\n",
      "global step 1170, epoch: 1, batch: 1170, loss: 0.09416, auc: 0.92902, f1 score: 0.55652, speed: 0.04 step/s\n",
      "global step 1180, epoch: 1, batch: 1180, loss: 0.11614, auc: 0.92222, f1 score: 0.56060, speed: 0.25 step/s\n",
      "global step 1190, epoch: 1, batch: 1190, loss: 0.11201, auc: 0.92493, f1 score: 0.56337, speed: 0.23 step/s\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.07074, auc: 0.92599, f1 score: 0.56059, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 18:20:16,181] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:20:16,353] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:20:16,353] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09836, auc: 0.92971, f1 score: 0.55842, precison: 0.53973, recall: 0.57845\n",
      "global step 1210, epoch: 1, batch: 1210, loss: 0.09059, auc: 0.90664, f1 score: 0.53078, speed: 0.04 step/s\n",
      "global step 1220, epoch: 1, batch: 1220, loss: 0.09624, auc: 0.91958, f1 score: 0.53508, speed: 0.23 step/s\n",
      "global step 1230, epoch: 1, batch: 1230, loss: 0.11662, auc: 0.92043, f1 score: 0.55610, speed: 0.24 step/s\n",
      "global step 1240, epoch: 1, batch: 1240, loss: 0.09462, auc: 0.92031, f1 score: 0.55766, speed: 0.22 step/s\n",
      "eval loss: 0.09837, auc: 0.92659, f1 score: 0.55566, precison: 0.53622, recall: 0.57655\n",
      "global step 1250, epoch: 1, batch: 1250, loss: 0.09360, auc: 0.91177, f1 score: 0.53224, speed: 0.04 step/s\n",
      "global step 1260, epoch: 1, batch: 1260, loss: 0.11657, auc: 0.91375, f1 score: 0.52344, speed: 0.24 step/s\n",
      "global step 1270, epoch: 1, batch: 1270, loss: 0.12369, auc: 0.91330, f1 score: 0.51960, speed: 0.23 step/s\n",
      "global step 1280, epoch: 1, batch: 1280, loss: 0.10978, auc: 0.91713, f1 score: 0.52030, speed: 0.23 step/s\n",
      "eval loss: 0.10049, auc: 0.92854, f1 score: 0.53479, precison: 0.52196, recall: 0.54827\n",
      "global step 1290, epoch: 1, batch: 1290, loss: 0.09449, auc: 0.92489, f1 score: 0.57067, speed: 0.04 step/s\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.08637, auc: 0.92392, f1 score: 0.55448, speed: 0.24 step/s\n",
      "global step 1310, epoch: 1, batch: 1310, loss: 0.09210, auc: 0.92418, f1 score: 0.56217, speed: 0.25 step/s\n",
      "global step 1320, epoch: 1, batch: 1320, loss: 0.08622, auc: 0.92607, f1 score: 0.55874, speed: 0.24 step/s\n",
      "eval loss: 0.09826, auc: 0.93005, f1 score: 0.55630, precison: 0.55723, recall: 0.55538\n",
      "global step 1330, epoch: 1, batch: 1330, loss: 0.10175, auc: 0.91576, f1 score: 0.53576, speed: 0.04 step/s\n",
      "global step 1340, epoch: 1, batch: 1340, loss: 0.08586, auc: 0.92562, f1 score: 0.55660, speed: 0.24 step/s\n",
      "global step 1350, epoch: 1, batch: 1350, loss: 0.09984, auc: 0.92121, f1 score: 0.55496, speed: 0.23 step/s\n",
      "global step 1360, epoch: 1, batch: 1360, loss: 0.09127, auc: 0.92002, f1 score: 0.55430, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 18:46:17,858] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:46:18,021] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:46:18,022] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09672, auc: 0.93192, f1 score: 0.56032, precison: 0.56518, recall: 0.55554\n",
      "global step 1370, epoch: 1, batch: 1370, loss: 0.09550, auc: 0.92581, f1 score: 0.59331, speed: 0.04 step/s\n",
      "global step 1380, epoch: 1, batch: 1380, loss: 0.10000, auc: 0.93310, f1 score: 0.58527, speed: 0.23 step/s\n",
      "global step 1390, epoch: 1, batch: 1390, loss: 0.10565, auc: 0.93386, f1 score: 0.58781, speed: 0.24 step/s\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.08764, auc: 0.93211, f1 score: 0.58407, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 18:52:48,089] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:52:48,238] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 18:52:48,239] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09584, auc: 0.93369, f1 score: 0.56927, precison: 0.57310, recall: 0.56549\n",
      "global step 1410, epoch: 1, batch: 1410, loss: 0.11209, auc: 0.92554, f1 score: 0.55826, speed: 0.04 step/s\n",
      "global step 1420, epoch: 1, batch: 1420, loss: 0.06975, auc: 0.93576, f1 score: 0.56423, speed: 0.24 step/s\n",
      "global step 1430, epoch: 1, batch: 1430, loss: 0.09186, auc: 0.93080, f1 score: 0.56039, speed: 0.24 step/s\n",
      "global step 1440, epoch: 1, batch: 1440, loss: 0.08628, auc: 0.92934, f1 score: 0.55758, speed: 0.22 step/s\n",
      "eval loss: 0.09582, auc: 0.93459, f1 score: 0.56165, precison: 0.53604, recall: 0.58982\n",
      "global step 1450, epoch: 1, batch: 1450, loss: 0.10471, auc: 0.93143, f1 score: 0.55808, speed: 0.04 step/s\n",
      "global step 1460, epoch: 1, batch: 1460, loss: 0.10282, auc: 0.93480, f1 score: 0.57294, speed: 0.24 step/s\n",
      "global step 1470, epoch: 1, batch: 1470, loss: 0.09047, auc: 0.93134, f1 score: 0.55709, speed: 0.22 step/s\n",
      "global step 1480, epoch: 1, batch: 1480, loss: 0.10196, auc: 0.93129, f1 score: 0.55448, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 19:05:52,431] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 19:05:52,600] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 19:05:52,600] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09487, auc: 0.93514, f1 score: 0.57327, precison: 0.56428, recall: 0.58256\n",
      "global step 1490, epoch: 1, batch: 1490, loss: 0.09788, auc: 0.92580, f1 score: 0.53867, speed: 0.04 step/s\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.08520, auc: 0.92738, f1 score: 0.56439, speed: 0.23 step/s\n",
      "global step 1510, epoch: 1, batch: 1510, loss: 0.10852, auc: 0.92266, f1 score: 0.55449, speed: 0.23 step/s\n",
      "global step 1520, epoch: 1, batch: 1520, loss: 0.09786, auc: 0.92590, f1 score: 0.55274, speed: 0.22 step/s\n",
      "eval loss: 0.09526, auc: 0.93645, f1 score: 0.56428, precison: 0.57296, recall: 0.55585\n",
      "global step 1530, epoch: 2, batch: 3, loss: 0.08225, auc: 0.92206, f1 score: 0.55879, speed: 0.04 step/s\n",
      "global step 1540, epoch: 2, batch: 13, loss: 0.11759, auc: 0.92462, f1 score: 0.53998, speed: 0.24 step/s\n",
      "global step 1550, epoch: 2, batch: 23, loss: 0.08853, auc: 0.92844, f1 score: 0.55399, speed: 0.24 step/s\n",
      "global step 1560, epoch: 2, batch: 33, loss: 0.10651, auc: 0.92653, f1 score: 0.55365, speed: 0.22 step/s\n",
      "eval loss: 0.09450, auc: 0.93692, f1 score: 0.56503, precison: 0.58943, recall: 0.54258\n",
      "global step 1570, epoch: 2, batch: 43, loss: 0.09040, auc: 0.94132, f1 score: 0.57223, speed: 0.04 step/s\n",
      "global step 1580, epoch: 2, batch: 53, loss: 0.08742, auc: 0.94251, f1 score: 0.58038, speed: 0.25 step/s\n",
      "global step 1590, epoch: 2, batch: 63, loss: 0.10905, auc: 0.93960, f1 score: 0.58728, speed: 0.24 step/s\n",
      "global step 1600, epoch: 2, batch: 73, loss: 0.08085, auc: 0.93998, f1 score: 0.59009, speed: 0.22 step/s\n",
      "eval loss: 0.09587, auc: 0.93605, f1 score: 0.56243, precison: 0.55939, recall: 0.56549\n",
      "global step 1610, epoch: 2, batch: 83, loss: 0.07739, auc: 0.94937, f1 score: 0.60331, speed: 0.04 step/s\n",
      "global step 1620, epoch: 2, batch: 93, loss: 0.07110, auc: 0.94590, f1 score: 0.60790, speed: 0.25 step/s\n",
      "global step 1630, epoch: 2, batch: 103, loss: 0.07448, auc: 0.94172, f1 score: 0.59365, speed: 0.23 step/s\n",
      "global step 1640, epoch: 2, batch: 113, loss: 0.08519, auc: 0.94056, f1 score: 0.59590, speed: 0.23 step/s\n",
      "eval loss: 0.09367, auc: 0.93715, f1 score: 0.56954, precison: 0.57527, recall: 0.56391\n",
      "global step 1650, epoch: 2, batch: 123, loss: 0.10517, auc: 0.91941, f1 score: 0.53941, speed: 0.04 step/s\n",
      "global step 1660, epoch: 2, batch: 133, loss: 0.10499, auc: 0.92624, f1 score: 0.54204, speed: 0.24 step/s\n",
      "global step 1670, epoch: 2, batch: 143, loss: 0.10177, auc: 0.93091, f1 score: 0.55917, speed: 0.24 step/s\n",
      "global step 1680, epoch: 2, batch: 153, loss: 0.10365, auc: 0.93177, f1 score: 0.56242, speed: 0.24 step/s\n",
      "eval loss: 0.09468, auc: 0.93732, f1 score: 0.56561, precison: 0.54837, recall: 0.58398\n",
      "global step 1690, epoch: 2, batch: 163, loss: 0.11785, auc: 0.93722, f1 score: 0.54759, speed: 0.04 step/s\n",
      "global step 1700, epoch: 2, batch: 173, loss: 0.09862, auc: 0.93571, f1 score: 0.56133, speed: 0.24 step/s\n",
      "global step 1710, epoch: 2, batch: 183, loss: 0.08733, auc: 0.93091, f1 score: 0.56427, speed: 0.24 step/s\n",
      "global step 1720, epoch: 2, batch: 193, loss: 0.09900, auc: 0.93256, f1 score: 0.56896, speed: 0.23 step/s\n",
      "eval loss: 0.09453, auc: 0.93587, f1 score: 0.56325, precison: 0.54490, recall: 0.58287\n",
      "global step 1730, epoch: 2, batch: 203, loss: 0.10844, auc: 0.93074, f1 score: 0.54787, speed: 0.04 step/s\n",
      "global step 1740, epoch: 2, batch: 213, loss: 0.09789, auc: 0.93266, f1 score: 0.54905, speed: 0.25 step/s\n",
      "global step 1750, epoch: 2, batch: 223, loss: 0.10524, auc: 0.93216, f1 score: 0.54287, speed: 0.23 step/s\n",
      "global step 1760, epoch: 2, batch: 233, loss: 0.09135, auc: 0.93195, f1 score: 0.54384, speed: 0.24 step/s\n",
      "eval loss: 0.09586, auc: 0.93744, f1 score: 0.55675, precison: 0.56300, recall: 0.55064\n",
      "global step 1770, epoch: 2, batch: 243, loss: 0.10653, auc: 0.94667, f1 score: 0.62468, speed: 0.04 step/s\n",
      "global step 1780, epoch: 2, batch: 253, loss: 0.09395, auc: 0.94778, f1 score: 0.61391, speed: 0.23 step/s\n",
      "global step 1790, epoch: 2, batch: 263, loss: 0.10961, auc: 0.94544, f1 score: 0.60258, speed: 0.23 step/s\n",
      "global step 1800, epoch: 2, batch: 273, loss: 0.12861, auc: 0.94248, f1 score: 0.59314, speed: 0.24 step/s\n",
      "eval loss: 0.09386, auc: 0.93757, f1 score: 0.56824, precison: 0.55376, recall: 0.58350\n",
      "global step 1810, epoch: 2, batch: 283, loss: 0.09116, auc: 0.93888, f1 score: 0.58824, speed: 0.04 step/s\n",
      "global step 1820, epoch: 2, batch: 293, loss: 0.07723, auc: 0.94622, f1 score: 0.59723, speed: 0.24 step/s\n",
      "global step 1830, epoch: 2, batch: 303, loss: 0.09144, auc: 0.94265, f1 score: 0.59165, speed: 0.22 step/s\n",
      "global step 1840, epoch: 2, batch: 313, loss: 0.11952, auc: 0.94244, f1 score: 0.58504, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 20:04:28,211] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:04:28,374] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:04:28,374] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09348, auc: 0.93667, f1 score: 0.57330, precison: 0.57818, recall: 0.56849\n",
      "global step 1850, epoch: 2, batch: 323, loss: 0.11068, auc: 0.92569, f1 score: 0.54965, speed: 0.04 step/s\n",
      "global step 1860, epoch: 2, batch: 333, loss: 0.08944, auc: 0.93201, f1 score: 0.57409, speed: 0.24 step/s\n",
      "global step 1870, epoch: 2, batch: 343, loss: 0.10048, auc: 0.93012, f1 score: 0.54977, speed: 0.24 step/s\n",
      "global step 1880, epoch: 2, batch: 353, loss: 0.06737, auc: 0.93353, f1 score: 0.55604, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 20:10:54,467] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:10:54,632] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:10:54,633] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09236, auc: 0.93873, f1 score: 0.57503, precison: 0.56356, recall: 0.58698\n",
      "global step 1890, epoch: 2, batch: 363, loss: 0.08107, auc: 0.94476, f1 score: 0.58776, speed: 0.04 step/s\n",
      "global step 1900, epoch: 2, batch: 373, loss: 0.11527, auc: 0.93946, f1 score: 0.58744, speed: 0.24 step/s\n",
      "global step 1910, epoch: 2, batch: 383, loss: 0.09785, auc: 0.93709, f1 score: 0.58738, speed: 0.23 step/s\n",
      "global step 1920, epoch: 2, batch: 393, loss: 0.10269, auc: 0.93597, f1 score: 0.57392, speed: 0.23 step/s\n",
      "eval loss: 0.09608, auc: 0.93745, f1 score: 0.55052, precison: 0.49739, recall: 0.61637\n",
      "global step 1930, epoch: 2, batch: 403, loss: 0.07470, auc: 0.94999, f1 score: 0.57143, speed: 0.04 step/s\n",
      "global step 1940, epoch: 2, batch: 413, loss: 0.08995, auc: 0.94400, f1 score: 0.57958, speed: 0.24 step/s\n",
      "global step 1950, epoch: 2, batch: 423, loss: 0.08751, auc: 0.94341, f1 score: 0.58107, speed: 0.23 step/s\n",
      "global step 1960, epoch: 2, batch: 433, loss: 0.12347, auc: 0.94133, f1 score: 0.58917, speed: 0.22 step/s\n",
      "eval loss: 0.09417, auc: 0.93498, f1 score: 0.57438, precison: 0.58154, recall: 0.56739\n",
      "global step 1970, epoch: 2, batch: 443, loss: 0.10497, auc: 0.92912, f1 score: 0.53875, speed: 0.04 step/s\n",
      "global step 1980, epoch: 2, batch: 453, loss: 0.12049, auc: 0.93392, f1 score: 0.57088, speed: 0.24 step/s\n",
      "global step 1990, epoch: 2, batch: 463, loss: 0.09525, auc: 0.93673, f1 score: 0.58100, speed: 0.24 step/s\n",
      "global step 2000, epoch: 2, batch: 473, loss: 0.10022, auc: 0.93492, f1 score: 0.57949, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 20:30:27,684] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:30:27,845] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:30:27,846] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09277, auc: 0.93909, f1 score: 0.57674, precison: 0.55445, recall: 0.60088\n",
      "global step 2010, epoch: 2, batch: 483, loss: 0.10559, auc: 0.94520, f1 score: 0.59401, speed: 0.04 step/s\n",
      "global step 2020, epoch: 2, batch: 493, loss: 0.08598, auc: 0.94466, f1 score: 0.58815, speed: 0.23 step/s\n",
      "global step 2030, epoch: 2, batch: 503, loss: 0.10613, auc: 0.94198, f1 score: 0.57976, speed: 0.23 step/s\n",
      "global step 2040, epoch: 2, batch: 513, loss: 0.09508, auc: 0.94206, f1 score: 0.57181, speed: 0.23 step/s\n",
      "eval loss: 0.09344, auc: 0.93708, f1 score: 0.56758, precison: 0.55039, recall: 0.58587\n",
      "global step 2050, epoch: 2, batch: 523, loss: 0.09642, auc: 0.94127, f1 score: 0.54404, speed: 0.04 step/s\n",
      "global step 2060, epoch: 2, batch: 533, loss: 0.11093, auc: 0.94211, f1 score: 0.56124, speed: 0.24 step/s\n",
      "global step 2070, epoch: 2, batch: 543, loss: 0.09119, auc: 0.94063, f1 score: 0.55950, speed: 0.24 step/s\n",
      "global step 2080, epoch: 2, batch: 553, loss: 0.10261, auc: 0.94072, f1 score: 0.55847, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 20:43:34,861] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:43:35,013] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:43:35,013] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09273, auc: 0.93988, f1 score: 0.57933, precison: 0.58956, recall: 0.56944\n",
      "global step 2090, epoch: 2, batch: 563, loss: 0.08559, auc: 0.93567, f1 score: 0.55891, speed: 0.04 step/s\n",
      "global step 2100, epoch: 2, batch: 573, loss: 0.06827, auc: 0.93824, f1 score: 0.58550, speed: 0.23 step/s\n",
      "global step 2110, epoch: 2, batch: 583, loss: 0.08114, auc: 0.93581, f1 score: 0.58630, speed: 0.23 step/s\n",
      "global step 2120, epoch: 2, batch: 593, loss: 0.08615, auc: 0.93429, f1 score: 0.58924, speed: 0.23 step/s\n",
      "eval loss: 0.09357, auc: 0.93744, f1 score: 0.57035, precison: 0.56936, recall: 0.57134\n",
      "global step 2130, epoch: 2, batch: 603, loss: 0.09632, auc: 0.94369, f1 score: 0.60921, speed: 0.04 step/s\n",
      "global step 2140, epoch: 2, batch: 613, loss: 0.10760, auc: 0.93870, f1 score: 0.57576, speed: 0.24 step/s\n",
      "global step 2150, epoch: 2, batch: 623, loss: 0.08362, auc: 0.94093, f1 score: 0.58260, speed: 0.25 step/s\n",
      "global step 2160, epoch: 2, batch: 633, loss: 0.07029, auc: 0.94239, f1 score: 0.58853, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 20:56:40,251] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:56:40,402] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 20:56:40,403] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09167, auc: 0.94008, f1 score: 0.58260, precison: 0.58013, recall: 0.58508\n",
      "global step 2170, epoch: 2, batch: 643, loss: 0.08987, auc: 0.93803, f1 score: 0.60302, speed: 0.04 step/s\n",
      "global step 2180, epoch: 2, batch: 653, loss: 0.10421, auc: 0.93953, f1 score: 0.59144, speed: 0.23 step/s\n",
      "global step 2190, epoch: 2, batch: 663, loss: 0.11852, auc: 0.93794, f1 score: 0.57928, speed: 0.24 step/s\n",
      "global step 2200, epoch: 2, batch: 673, loss: 0.08768, auc: 0.93995, f1 score: 0.58513, speed: 0.24 step/s\n",
      "eval loss: 0.09158, auc: 0.94108, f1 score: 0.58211, precison: 0.56489, recall: 0.60041\n",
      "global step 2210, epoch: 2, batch: 683, loss: 0.09839, auc: 0.93993, f1 score: 0.57831, speed: 0.04 step/s\n",
      "global step 2220, epoch: 2, batch: 693, loss: 0.09579, auc: 0.94554, f1 score: 0.58974, speed: 0.22 step/s\n",
      "global step 2230, epoch: 2, batch: 703, loss: 0.09897, auc: 0.94510, f1 score: 0.57918, speed: 0.23 step/s\n",
      "global step 2240, epoch: 2, batch: 713, loss: 0.09293, auc: 0.94507, f1 score: 0.58862, speed: 0.24 step/s\n",
      "eval loss: 0.09248, auc: 0.94039, f1 score: 0.57217, precison: 0.55072, recall: 0.59535\n",
      "global step 2250, epoch: 2, batch: 723, loss: 0.08674, auc: 0.93650, f1 score: 0.58079, speed: 0.04 step/s\n",
      "global step 2260, epoch: 2, batch: 733, loss: 0.08326, auc: 0.93441, f1 score: 0.56635, speed: 0.23 step/s\n",
      "global step 2270, epoch: 2, batch: 743, loss: 0.11404, auc: 0.93506, f1 score: 0.57325, speed: 0.22 step/s\n",
      "global step 2280, epoch: 2, batch: 753, loss: 0.08690, auc: 0.93927, f1 score: 0.57688, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 21:16:15,358] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 21:16:15,515] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 21:16:15,515] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09080, auc: 0.94119, f1 score: 0.59025, precison: 0.58862, recall: 0.59188\n",
      "global step 2290, epoch: 2, batch: 763, loss: 0.10927, auc: 0.94583, f1 score: 0.55756, speed: 0.04 step/s\n",
      "global step 2300, epoch: 2, batch: 773, loss: 0.09730, auc: 0.94852, f1 score: 0.58483, speed: 0.24 step/s\n",
      "global step 2310, epoch: 2, batch: 783, loss: 0.09198, auc: 0.94560, f1 score: 0.57761, speed: 0.23 step/s\n",
      "global step 2320, epoch: 2, batch: 793, loss: 0.09695, auc: 0.94036, f1 score: 0.57383, speed: 0.22 step/s\n",
      "eval loss: 0.09064, auc: 0.94198, f1 score: 0.58470, precison: 0.57501, recall: 0.59472\n",
      "global step 2330, epoch: 2, batch: 803, loss: 0.07293, auc: 0.93870, f1 score: 0.61934, speed: 0.04 step/s\n",
      "global step 2340, epoch: 2, batch: 813, loss: 0.11329, auc: 0.93407, f1 score: 0.57759, speed: 0.23 step/s\n",
      "global step 2350, epoch: 2, batch: 823, loss: 0.09601, auc: 0.93814, f1 score: 0.59406, speed: 0.23 step/s\n",
      "global step 2360, epoch: 2, batch: 833, loss: 0.08669, auc: 0.93886, f1 score: 0.59040, speed: 0.22 step/s\n",
      "eval loss: 0.09053, auc: 0.94348, f1 score: 0.58136, precison: 0.56154, recall: 0.60262\n",
      "global step 2370, epoch: 2, batch: 843, loss: 0.08552, auc: 0.94901, f1 score: 0.59866, speed: 0.04 step/s\n",
      "global step 2380, epoch: 2, batch: 853, loss: 0.09041, auc: 0.94911, f1 score: 0.59826, speed: 0.23 step/s\n",
      "global step 2390, epoch: 2, batch: 863, loss: 0.09904, auc: 0.94411, f1 score: 0.59892, speed: 0.21 step/s\n",
      "global step 2400, epoch: 2, batch: 873, loss: 0.09405, auc: 0.94089, f1 score: 0.59171, speed: 0.22 step/s\n",
      "eval loss: 0.09176, auc: 0.94089, f1 score: 0.57980, precison: 0.56908, recall: 0.59093\n",
      "global step 2410, epoch: 2, batch: 883, loss: 0.08234, auc: 0.94390, f1 score: 0.58278, speed: 0.04 step/s\n",
      "global step 2420, epoch: 2, batch: 893, loss: 0.07311, auc: 0.94818, f1 score: 0.60114, speed: 0.24 step/s\n",
      "global step 2430, epoch: 2, batch: 903, loss: 0.08523, auc: 0.94471, f1 score: 0.59398, speed: 0.24 step/s\n",
      "global step 2440, epoch: 2, batch: 913, loss: 0.11501, auc: 0.94285, f1 score: 0.59539, speed: 0.22 step/s\n",
      "eval loss: 0.09137, auc: 0.94126, f1 score: 0.58476, precison: 0.59336, recall: 0.57639\n",
      "global step 2450, epoch: 2, batch: 923, loss: 0.10886, auc: 0.94375, f1 score: 0.58999, speed: 0.04 step/s\n",
      "global step 2460, epoch: 2, batch: 933, loss: 0.09871, auc: 0.94350, f1 score: 0.59394, speed: 0.23 step/s\n",
      "global step 2470, epoch: 2, batch: 943, loss: 0.10361, auc: 0.94353, f1 score: 0.58982, speed: 0.22 step/s\n",
      "global step 2480, epoch: 2, batch: 953, loss: 0.08558, auc: 0.94220, f1 score: 0.58581, speed: 0.23 step/s\n",
      "eval loss: 0.09131, auc: 0.94253, f1 score: 0.57562, precison: 0.55427, recall: 0.59867\n",
      "global step 2490, epoch: 2, batch: 963, loss: 0.11583, auc: 0.94803, f1 score: 0.58289, speed: 0.04 step/s\n",
      "global step 2500, epoch: 2, batch: 973, loss: 0.09974, auc: 0.93481, f1 score: 0.55706, speed: 0.24 step/s\n",
      "global step 2510, epoch: 2, batch: 983, loss: 0.07110, auc: 0.93751, f1 score: 0.55427, speed: 0.24 step/s\n",
      "global step 2520, epoch: 2, batch: 993, loss: 0.10266, auc: 0.93985, f1 score: 0.56501, speed: 0.23 step/s\n",
      "eval loss: 0.09036, auc: 0.94304, f1 score: 0.58491, precison: 0.57840, recall: 0.59156\n",
      "global step 2530, epoch: 2, batch: 1003, loss: 0.08328, auc: 0.93544, f1 score: 0.58321, speed: 0.04 step/s\n",
      "global step 2540, epoch: 2, batch: 1013, loss: 0.09360, auc: 0.93665, f1 score: 0.57124, speed: 0.24 step/s\n",
      "global step 2550, epoch: 2, batch: 1023, loss: 0.08255, auc: 0.93916, f1 score: 0.57243, speed: 0.23 step/s\n",
      "global step 2560, epoch: 2, batch: 1033, loss: 0.14283, auc: 0.94142, f1 score: 0.57891, speed: 0.23 step/s\n",
      "eval loss: 0.09047, auc: 0.94158, f1 score: 0.58858, precison: 0.58765, recall: 0.58951\n",
      "global step 2570, epoch: 2, batch: 1043, loss: 0.09266, auc: 0.94230, f1 score: 0.57636, speed: 0.04 step/s\n",
      "global step 2580, epoch: 2, batch: 1053, loss: 0.08427, auc: 0.94331, f1 score: 0.57293, speed: 0.24 step/s\n",
      "global step 2590, epoch: 2, batch: 1063, loss: 0.08727, auc: 0.94716, f1 score: 0.59295, speed: 0.23 step/s\n",
      "global step 2600, epoch: 2, batch: 1073, loss: 0.08885, auc: 0.94568, f1 score: 0.59379, speed: 0.22 step/s\n",
      "eval loss: 0.09116, auc: 0.94364, f1 score: 0.58310, precison: 0.58333, recall: 0.58287\n",
      "global step 2610, epoch: 2, batch: 1083, loss: 0.09434, auc: 0.94127, f1 score: 0.57320, speed: 0.04 step/s\n",
      "global step 2620, epoch: 2, batch: 1093, loss: 0.10291, auc: 0.94440, f1 score: 0.56095, speed: 0.23 step/s\n",
      "global step 2630, epoch: 2, batch: 1103, loss: 0.10680, auc: 0.94340, f1 score: 0.55399, speed: 0.24 step/s\n",
      "global step 2640, epoch: 2, batch: 1113, loss: 0.09173, auc: 0.94321, f1 score: 0.55740, speed: 0.22 step/s\n",
      "eval loss: 0.09042, auc: 0.94315, f1 score: 0.58329, precison: 0.55564, recall: 0.61384\n",
      "global step 2650, epoch: 2, batch: 1123, loss: 0.08421, auc: 0.94580, f1 score: 0.60710, speed: 0.04 step/s\n",
      "global step 2660, epoch: 2, batch: 1133, loss: 0.09917, auc: 0.94931, f1 score: 0.59210, speed: 0.24 step/s\n",
      "global step 2670, epoch: 2, batch: 1143, loss: 0.10219, auc: 0.94921, f1 score: 0.59523, speed: 0.23 step/s\n",
      "global step 2680, epoch: 2, batch: 1153, loss: 0.08760, auc: 0.94600, f1 score: 0.59181, speed: 0.23 step/s\n",
      "eval loss: 0.09012, auc: 0.94468, f1 score: 0.58154, precison: 0.57290, recall: 0.59046\n",
      "global step 2690, epoch: 2, batch: 1163, loss: 0.10498, auc: 0.93173, f1 score: 0.56416, speed: 0.04 step/s\n",
      "global step 2700, epoch: 2, batch: 1173, loss: 0.10969, auc: 0.93763, f1 score: 0.56524, speed: 0.25 step/s\n",
      "global step 2710, epoch: 2, batch: 1183, loss: 0.08345, auc: 0.93672, f1 score: 0.56191, speed: 0.23 step/s\n",
      "global step 2720, epoch: 2, batch: 1193, loss: 0.07147, auc: 0.93952, f1 score: 0.56420, speed: 0.24 step/s\n",
      "eval loss: 0.08971, auc: 0.94311, f1 score: 0.58676, precison: 0.57604, recall: 0.59788\n",
      "global step 2730, epoch: 2, batch: 1203, loss: 0.08856, auc: 0.95170, f1 score: 0.59029, speed: 0.04 step/s\n",
      "global step 2740, epoch: 2, batch: 1213, loss: 0.09044, auc: 0.94960, f1 score: 0.59334, speed: 0.23 step/s\n",
      "global step 2750, epoch: 2, batch: 1223, loss: 0.08858, auc: 0.94777, f1 score: 0.59021, speed: 0.23 step/s\n",
      "global step 2760, epoch: 2, batch: 1233, loss: 0.07913, auc: 0.94871, f1 score: 0.58698, speed: 0.23 step/s\n",
      "eval loss: 0.09017, auc: 0.94557, f1 score: 0.58101, precison: 0.57622, recall: 0.58587\n",
      "global step 2770, epoch: 2, batch: 1243, loss: 0.09595, auc: 0.94082, f1 score: 0.59357, speed: 0.04 step/s\n",
      "global step 2780, epoch: 2, batch: 1253, loss: 0.09572, auc: 0.94713, f1 score: 0.59934, speed: 0.24 step/s\n",
      "global step 2790, epoch: 2, batch: 1263, loss: 0.10200, auc: 0.94846, f1 score: 0.59754, speed: 0.23 step/s\n",
      "global step 2800, epoch: 2, batch: 1273, loss: 0.09994, auc: 0.94586, f1 score: 0.58671, speed: 0.23 step/s\n",
      "eval loss: 0.08920, auc: 0.94681, f1 score: 0.58555, precison: 0.57012, recall: 0.60183\n",
      "global step 2810, epoch: 2, batch: 1283, loss: 0.10667, auc: 0.93857, f1 score: 0.55510, speed: 0.04 step/s\n",
      "global step 2820, epoch: 2, batch: 1293, loss: 0.08667, auc: 0.93773, f1 score: 0.56220, speed: 0.24 step/s\n",
      "global step 2830, epoch: 2, batch: 1303, loss: 0.08984, auc: 0.94003, f1 score: 0.56587, speed: 0.23 step/s\n",
      "global step 2840, epoch: 2, batch: 1313, loss: 0.07244, auc: 0.94114, f1 score: 0.56956, speed: 0.22 step/s\n",
      "eval loss: 0.08935, auc: 0.94623, f1 score: 0.58400, precison: 0.55718, recall: 0.61353\n",
      "global step 2850, epoch: 2, batch: 1323, loss: 0.07198, auc: 0.95831, f1 score: 0.62647, speed: 0.04 step/s\n",
      "global step 2860, epoch: 2, batch: 1333, loss: 0.09365, auc: 0.94919, f1 score: 0.60738, speed: 0.23 step/s\n",
      "global step 2870, epoch: 2, batch: 1343, loss: 0.08148, auc: 0.94727, f1 score: 0.59464, speed: 0.23 step/s\n",
      "global step 2880, epoch: 2, batch: 1353, loss: 0.07754, auc: 0.94566, f1 score: 0.58927, speed: 0.25 step/s\n",
      "eval loss: 0.08940, auc: 0.94667, f1 score: 0.58419, precison: 0.57446, recall: 0.59425\n",
      "global step 2890, epoch: 2, batch: 1363, loss: 0.05715, auc: 0.95290, f1 score: 0.62216, speed: 0.04 step/s\n",
      "global step 2900, epoch: 2, batch: 1373, loss: 0.10587, auc: 0.95239, f1 score: 0.61495, speed: 0.25 step/s\n",
      "global step 2910, epoch: 2, batch: 1383, loss: 0.07988, auc: 0.95244, f1 score: 0.60640, speed: 0.23 step/s\n",
      "global step 2920, epoch: 2, batch: 1393, loss: 0.10026, auc: 0.95043, f1 score: 0.59665, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08882, auc: 0.94691, f1 score: 0.58450, precison: 0.59102, recall: 0.57813\n",
      "global step 2930, epoch: 2, batch: 1403, loss: 0.09948, auc: 0.93662, f1 score: 0.57725, speed: 0.04 step/s\n",
      "global step 2940, epoch: 2, batch: 1413, loss: 0.10384, auc: 0.93641, f1 score: 0.55716, speed: 0.23 step/s\n",
      "global step 2950, epoch: 2, batch: 1423, loss: 0.08346, auc: 0.94424, f1 score: 0.57568, speed: 0.25 step/s\n",
      "global step 2960, epoch: 2, batch: 1433, loss: 0.09970, auc: 0.94175, f1 score: 0.57944, speed: 0.23 step/s\n",
      "eval loss: 0.08912, auc: 0.94688, f1 score: 0.58729, precison: 0.58618, recall: 0.58840\n",
      "global step 2970, epoch: 2, batch: 1443, loss: 0.09579, auc: 0.95138, f1 score: 0.60459, speed: 0.04 step/s\n",
      "global step 2980, epoch: 2, batch: 1453, loss: 0.10630, auc: 0.95005, f1 score: 0.59732, speed: 0.23 step/s\n",
      "global step 2990, epoch: 2, batch: 1463, loss: 0.09219, auc: 0.95253, f1 score: 0.59927, speed: 0.23 step/s\n",
      "global step 3000, epoch: 2, batch: 1473, loss: 0.08674, auc: 0.95047, f1 score: 0.59318, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-04 23:14:13,194] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 23:14:13,374] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-04 23:14:13,375] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08785, auc: 0.94786, f1 score: 0.59491, precison: 0.58136, recall: 0.60910\n",
      "global step 3010, epoch: 2, batch: 1483, loss: 0.08859, auc: 0.95143, f1 score: 0.60883, speed: 0.04 step/s\n",
      "global step 3020, epoch: 2, batch: 1493, loss: 0.09514, auc: 0.94805, f1 score: 0.60013, speed: 0.24 step/s\n",
      "global step 3030, epoch: 2, batch: 1503, loss: 0.11719, auc: 0.94864, f1 score: 0.60118, speed: 0.23 step/s\n",
      "global step 3040, epoch: 2, batch: 1513, loss: 0.09967, auc: 0.94695, f1 score: 0.58924, speed: 0.23 step/s\n",
      "eval loss: 0.08894, auc: 0.94703, f1 score: 0.58115, precison: 0.57788, recall: 0.58445\n",
      "global step 3050, epoch: 2, batch: 1523, loss: 0.07444, auc: 0.94729, f1 score: 0.64099, speed: 0.04 step/s\n",
      "global step 3060, epoch: 3, batch: 6, loss: 0.07501, auc: 0.95173, f1 score: 0.63391, speed: 0.25 step/s\n",
      "global step 3070, epoch: 3, batch: 16, loss: 0.09392, auc: 0.95049, f1 score: 0.62450, speed: 0.23 step/s\n",
      "global step 3080, epoch: 3, batch: 26, loss: 0.08246, auc: 0.95322, f1 score: 0.62482, speed: 0.23 step/s\n",
      "eval loss: 0.08996, auc: 0.94496, f1 score: 0.57999, precison: 0.59508, recall: 0.56565\n",
      "global step 3090, epoch: 3, batch: 36, loss: 0.07902, auc: 0.95831, f1 score: 0.63889, speed: 0.04 step/s\n",
      "global step 3100, epoch: 3, batch: 46, loss: 0.08278, auc: 0.95610, f1 score: 0.62361, speed: 0.25 step/s\n",
      "global step 3110, epoch: 3, batch: 56, loss: 0.08311, auc: 0.95709, f1 score: 0.62187, speed: 0.24 step/s\n",
      "global step 3120, epoch: 3, batch: 66, loss: 0.09299, auc: 0.95788, f1 score: 0.62273, speed: 0.23 step/s\n",
      "eval loss: 0.08931, auc: 0.94664, f1 score: 0.58120, precison: 0.57722, recall: 0.58524\n",
      "global step 3130, epoch: 3, batch: 76, loss: 0.09242, auc: 0.95459, f1 score: 0.60263, speed: 0.04 step/s\n",
      "global step 3140, epoch: 3, batch: 86, loss: 0.08888, auc: 0.95253, f1 score: 0.60951, speed: 0.23 step/s\n",
      "global step 3150, epoch: 3, batch: 96, loss: 0.10550, auc: 0.95014, f1 score: 0.61600, speed: 0.22 step/s\n",
      "global step 3160, epoch: 3, batch: 106, loss: 0.10614, auc: 0.95067, f1 score: 0.61554, speed: 0.23 step/s\n",
      "eval loss: 0.08950, auc: 0.94648, f1 score: 0.58350, precison: 0.56879, recall: 0.59899\n",
      "global step 3170, epoch: 3, batch: 116, loss: 0.09239, auc: 0.95509, f1 score: 0.64837, speed: 0.04 step/s\n",
      "global step 3180, epoch: 3, batch: 126, loss: 0.08174, auc: 0.95664, f1 score: 0.64925, speed: 0.25 step/s\n",
      "global step 3190, epoch: 3, batch: 136, loss: 0.08094, auc: 0.95567, f1 score: 0.63361, speed: 0.23 step/s\n",
      "global step 3200, epoch: 3, batch: 146, loss: 0.07224, auc: 0.95480, f1 score: 0.62651, speed: 0.22 step/s\n",
      "eval loss: 0.08980, auc: 0.94496, f1 score: 0.58471, precison: 0.56811, recall: 0.60231\n",
      "global step 3210, epoch: 3, batch: 156, loss: 0.09702, auc: 0.95258, f1 score: 0.62500, speed: 0.04 step/s\n",
      "global step 3220, epoch: 3, batch: 166, loss: 0.08146, auc: 0.95303, f1 score: 0.61235, speed: 0.24 step/s\n",
      "global step 3230, epoch: 3, batch: 176, loss: 0.07721, auc: 0.95541, f1 score: 0.61767, speed: 0.23 step/s\n",
      "global step 3240, epoch: 3, batch: 186, loss: 0.07893, auc: 0.95487, f1 score: 0.61635, speed: 0.22 step/s\n",
      "eval loss: 0.08954, auc: 0.94598, f1 score: 0.57699, precison: 0.57092, recall: 0.58319\n",
      "global step 3250, epoch: 3, batch: 196, loss: 0.07448, auc: 0.96045, f1 score: 0.64030, speed: 0.04 step/s\n",
      "global step 3260, epoch: 3, batch: 206, loss: 0.10522, auc: 0.95702, f1 score: 0.62433, speed: 0.24 step/s\n",
      "global step 3270, epoch: 3, batch: 216, loss: 0.08900, auc: 0.95623, f1 score: 0.62470, speed: 0.24 step/s\n",
      "global step 3280, epoch: 3, batch: 226, loss: 0.09164, auc: 0.95450, f1 score: 0.61450, speed: 0.23 step/s\n",
      "eval loss: 0.08902, auc: 0.94708, f1 score: 0.58756, precison: 0.56585, recall: 0.61100\n",
      "global step 3290, epoch: 3, batch: 236, loss: 0.07171, auc: 0.96506, f1 score: 0.66482, speed: 0.04 step/s\n",
      "global step 3300, epoch: 3, batch: 246, loss: 0.09891, auc: 0.95793, f1 score: 0.62625, speed: 0.24 step/s\n",
      "global step 3310, epoch: 3, batch: 256, loss: 0.08640, auc: 0.95615, f1 score: 0.62593, speed: 0.24 step/s\n",
      "global step 3320, epoch: 3, batch: 266, loss: 0.07999, auc: 0.95657, f1 score: 0.61888, speed: 0.23 step/s\n",
      "eval loss: 0.08845, auc: 0.94638, f1 score: 0.59061, precison: 0.59061, recall: 0.59061\n",
      "global step 3330, epoch: 3, batch: 276, loss: 0.07783, auc: 0.95435, f1 score: 0.59751, speed: 0.04 step/s\n",
      "global step 3340, epoch: 3, batch: 286, loss: 0.10783, auc: 0.94771, f1 score: 0.60029, speed: 0.24 step/s\n",
      "global step 3350, epoch: 3, batch: 296, loss: 0.10488, auc: 0.95014, f1 score: 0.60124, speed: 0.23 step/s\n",
      "global step 3360, epoch: 3, batch: 306, loss: 0.08307, auc: 0.95012, f1 score: 0.59751, speed: 0.23 step/s\n",
      "eval loss: 0.08818, auc: 0.94854, f1 score: 0.58882, precison: 0.58183, recall: 0.59599\n",
      "global step 3370, epoch: 3, batch: 316, loss: 0.08999, auc: 0.95385, f1 score: 0.62616, speed: 0.04 step/s\n",
      "global step 3380, epoch: 3, batch: 326, loss: 0.07197, auc: 0.95278, f1 score: 0.59580, speed: 0.24 step/s\n",
      "global step 3390, epoch: 3, batch: 336, loss: 0.07662, auc: 0.95343, f1 score: 0.59873, speed: 0.23 step/s\n",
      "global step 3400, epoch: 3, batch: 346, loss: 0.09143, auc: 0.95244, f1 score: 0.59604, speed: 0.23 step/s\n",
      "eval loss: 0.08883, auc: 0.94781, f1 score: 0.58138, precison: 0.55503, recall: 0.61036\n",
      "global step 3410, epoch: 3, batch: 356, loss: 0.06743, auc: 0.95864, f1 score: 0.67824, speed: 0.04 step/s\n",
      "global step 3420, epoch: 3, batch: 366, loss: 0.08215, auc: 0.95822, f1 score: 0.65418, speed: 0.24 step/s\n",
      "global step 3430, epoch: 3, batch: 376, loss: 0.07273, auc: 0.95654, f1 score: 0.62384, speed: 0.23 step/s\n",
      "global step 3440, epoch: 3, batch: 386, loss: 0.09982, auc: 0.95432, f1 score: 0.60859, speed: 0.22 step/s\n",
      "eval loss: 0.08855, auc: 0.94756, f1 score: 0.58769, precison: 0.58713, recall: 0.58824\n",
      "global step 3450, epoch: 3, batch: 396, loss: 0.10432, auc: 0.95727, f1 score: 0.61125, speed: 0.04 step/s\n",
      "global step 3460, epoch: 3, batch: 406, loss: 0.10103, auc: 0.95710, f1 score: 0.62055, speed: 0.24 step/s\n",
      "global step 3470, epoch: 3, batch: 416, loss: 0.06356, auc: 0.95998, f1 score: 0.62728, speed: 0.23 step/s\n",
      "global step 3480, epoch: 3, batch: 426, loss: 0.08753, auc: 0.95933, f1 score: 0.62366, speed: 0.23 step/s\n",
      "eval loss: 0.08979, auc: 0.94587, f1 score: 0.58262, precison: 0.56600, recall: 0.60025\n",
      "global step 3490, epoch: 3, batch: 436, loss: 0.07973, auc: 0.95164, f1 score: 0.61635, speed: 0.04 step/s\n",
      "global step 3500, epoch: 3, batch: 446, loss: 0.07717, auc: 0.95494, f1 score: 0.60985, speed: 0.24 step/s\n",
      "global step 3510, epoch: 3, batch: 456, loss: 0.07571, auc: 0.95270, f1 score: 0.60343, speed: 0.24 step/s\n",
      "global step 3520, epoch: 3, batch: 466, loss: 0.06828, auc: 0.95599, f1 score: 0.61699, speed: 0.24 step/s\n",
      "eval loss: 0.08883, auc: 0.94703, f1 score: 0.59166, precison: 0.56872, recall: 0.61653\n",
      "global step 3530, epoch: 3, batch: 476, loss: 0.09023, auc: 0.95664, f1 score: 0.62024, speed: 0.04 step/s\n",
      "global step 3540, epoch: 3, batch: 486, loss: 0.07803, auc: 0.95051, f1 score: 0.61097, speed: 0.25 step/s\n",
      "global step 3550, epoch: 3, batch: 496, loss: 0.09254, auc: 0.95541, f1 score: 0.62569, speed: 0.23 step/s\n",
      "global step 3560, epoch: 3, batch: 506, loss: 0.07846, auc: 0.95542, f1 score: 0.62741, speed: 0.23 step/s\n",
      "eval loss: 0.08848, auc: 0.94756, f1 score: 0.58388, precison: 0.59907, recall: 0.56944\n",
      "global step 3570, epoch: 3, batch: 516, loss: 0.07448, auc: 0.95599, f1 score: 0.65019, speed: 0.04 step/s\n",
      "global step 3580, epoch: 3, batch: 526, loss: 0.06646, auc: 0.95403, f1 score: 0.63366, speed: 0.24 step/s\n",
      "global step 3590, epoch: 3, batch: 536, loss: 0.09231, auc: 0.95374, f1 score: 0.62125, speed: 0.24 step/s\n",
      "global step 3600, epoch: 3, batch: 546, loss: 0.07940, auc: 0.95391, f1 score: 0.61618, speed: 0.23 step/s\n",
      "eval loss: 0.08841, auc: 0.94699, f1 score: 0.59359, precison: 0.58587, recall: 0.60152\n",
      "global step 3610, epoch: 3, batch: 556, loss: 0.07987, auc: 0.95893, f1 score: 0.60150, speed: 0.04 step/s\n",
      "global step 3620, epoch: 3, batch: 566, loss: 0.08965, auc: 0.95986, f1 score: 0.62069, speed: 0.24 step/s\n",
      "global step 3630, epoch: 3, batch: 576, loss: 0.06915, auc: 0.95953, f1 score: 0.61989, speed: 0.22 step/s\n",
      "global step 3640, epoch: 3, batch: 586, loss: 0.06144, auc: 0.95864, f1 score: 0.62074, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08860, auc: 0.94653, f1 score: 0.59314, precison: 0.60058, recall: 0.58587\n",
      "global step 3650, epoch: 3, batch: 596, loss: 0.07710, auc: 0.96262, f1 score: 0.62348, speed: 0.04 step/s\n",
      "global step 3660, epoch: 3, batch: 606, loss: 0.07162, auc: 0.95702, f1 score: 0.60997, speed: 0.25 step/s\n",
      "global step 3670, epoch: 3, batch: 616, loss: 0.09855, auc: 0.95498, f1 score: 0.60471, speed: 0.23 step/s\n",
      "global step 3680, epoch: 3, batch: 626, loss: 0.10696, auc: 0.95645, f1 score: 0.60230, speed: 0.22 step/s\n",
      "eval loss: 0.08937, auc: 0.94637, f1 score: 0.58090, precison: 0.56360, recall: 0.59930\n",
      "global step 3690, epoch: 3, batch: 636, loss: 0.06463, auc: 0.95180, f1 score: 0.60438, speed: 0.04 step/s\n",
      "global step 3700, epoch: 3, batch: 646, loss: 0.07768, auc: 0.95497, f1 score: 0.59976, speed: 0.23 step/s\n",
      "global step 3710, epoch: 3, batch: 656, loss: 0.08972, auc: 0.95611, f1 score: 0.60736, speed: 0.23 step/s\n",
      "global step 3720, epoch: 3, batch: 666, loss: 0.08761, auc: 0.95520, f1 score: 0.60105, speed: 0.22 step/s\n",
      "eval loss: 0.08801, auc: 0.94860, f1 score: 0.58677, precison: 0.57173, recall: 0.60262\n",
      "global step 3730, epoch: 3, batch: 676, loss: 0.09181, auc: 0.95125, f1 score: 0.61429, speed: 0.04 step/s\n",
      "global step 3740, epoch: 3, batch: 686, loss: 0.09927, auc: 0.95564, f1 score: 0.60714, speed: 0.23 step/s\n",
      "global step 3750, epoch: 3, batch: 696, loss: 0.08126, auc: 0.95608, f1 score: 0.62500, speed: 0.23 step/s\n",
      "global step 3760, epoch: 3, batch: 706, loss: 0.07597, auc: 0.95376, f1 score: 0.61876, speed: 0.23 step/s\n",
      "eval loss: 0.08889, auc: 0.94768, f1 score: 0.58438, precison: 0.57162, recall: 0.59772\n",
      "global step 3770, epoch: 3, batch: 716, loss: 0.08361, auc: 0.95639, f1 score: 0.63257, speed: 0.04 step/s\n",
      "global step 3780, epoch: 3, batch: 726, loss: 0.08283, auc: 0.95395, f1 score: 0.62189, speed: 0.25 step/s\n",
      "global step 3790, epoch: 3, batch: 736, loss: 0.08727, auc: 0.95552, f1 score: 0.63040, speed: 0.22 step/s\n",
      "global step 3800, epoch: 3, batch: 746, loss: 0.07346, auc: 0.95514, f1 score: 0.62686, speed: 0.24 step/s\n",
      "eval loss: 0.08873, auc: 0.94804, f1 score: 0.58478, precison: 0.58814, recall: 0.58145\n",
      "global step 3810, epoch: 3, batch: 756, loss: 0.10802, auc: 0.95839, f1 score: 0.62794, speed: 0.04 step/s\n",
      "global step 3820, epoch: 3, batch: 766, loss: 0.08122, auc: 0.95609, f1 score: 0.63019, speed: 0.24 step/s\n",
      "global step 3830, epoch: 3, batch: 776, loss: 0.07210, auc: 0.95534, f1 score: 0.62391, speed: 0.23 step/s\n",
      "global step 3840, epoch: 3, batch: 786, loss: 0.07331, auc: 0.95533, f1 score: 0.62071, speed: 0.23 step/s\n",
      "eval loss: 0.08710, auc: 0.95010, f1 score: 0.59020, precison: 0.58033, recall: 0.60041\n",
      "global step 3850, epoch: 3, batch: 796, loss: 0.07016, auc: 0.95410, f1 score: 0.60458, speed: 0.04 step/s\n",
      "global step 3860, epoch: 3, batch: 806, loss: 0.07467, auc: 0.95491, f1 score: 0.62160, speed: 0.24 step/s\n",
      "global step 3870, epoch: 3, batch: 816, loss: 0.10043, auc: 0.95380, f1 score: 0.62612, speed: 0.23 step/s\n",
      "global step 3880, epoch: 3, batch: 826, loss: 0.07221, auc: 0.95498, f1 score: 0.62716, speed: 0.23 step/s\n",
      "eval loss: 0.08891, auc: 0.94752, f1 score: 0.58150, precison: 0.56784, recall: 0.59583\n",
      "global step 3890, epoch: 3, batch: 836, loss: 0.09257, auc: 0.94941, f1 score: 0.57766, speed: 0.04 step/s\n",
      "global step 3900, epoch: 3, batch: 846, loss: 0.09136, auc: 0.95078, f1 score: 0.58751, speed: 0.24 step/s\n",
      "global step 3910, epoch: 3, batch: 856, loss: 0.06664, auc: 0.95195, f1 score: 0.60082, speed: 0.23 step/s\n",
      "global step 3920, epoch: 3, batch: 866, loss: 0.11782, auc: 0.95154, f1 score: 0.59691, speed: 0.24 step/s\n",
      "eval loss: 0.08784, auc: 0.94955, f1 score: 0.58044, precison: 0.57121, recall: 0.58998\n",
      "global step 3930, epoch: 3, batch: 876, loss: 0.09495, auc: 0.96213, f1 score: 0.65286, speed: 0.04 step/s\n",
      "global step 3940, epoch: 3, batch: 886, loss: 0.08142, auc: 0.95815, f1 score: 0.62590, speed: 0.24 step/s\n",
      "global step 3950, epoch: 3, batch: 896, loss: 0.07994, auc: 0.95849, f1 score: 0.62165, speed: 0.24 step/s\n",
      "global step 3960, epoch: 3, batch: 906, loss: 0.08563, auc: 0.95759, f1 score: 0.61554, speed: 0.23 step/s\n",
      "eval loss: 0.08802, auc: 0.94786, f1 score: 0.58446, precison: 0.55789, recall: 0.61368\n",
      "global step 3970, epoch: 3, batch: 916, loss: 0.09182, auc: 0.95876, f1 score: 0.60723, speed: 0.04 step/s\n",
      "global step 3980, epoch: 3, batch: 926, loss: 0.09380, auc: 0.96040, f1 score: 0.60496, speed: 0.24 step/s\n",
      "global step 3990, epoch: 3, batch: 936, loss: 0.09093, auc: 0.95610, f1 score: 0.59665, speed: 0.23 step/s\n",
      "global step 4000, epoch: 3, batch: 946, loss: 0.07221, auc: 0.95578, f1 score: 0.59787, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-05 01:57:09,602] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 01:57:09,770] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 01:57:09,770] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08743, auc: 0.94852, f1 score: 0.59564, precison: 0.58160, recall: 0.61036\n",
      "global step 4010, epoch: 3, batch: 956, loss: 0.09521, auc: 0.94968, f1 score: 0.62565, speed: 0.04 step/s\n",
      "global step 4020, epoch: 3, batch: 966, loss: 0.07049, auc: 0.95477, f1 score: 0.64193, speed: 0.24 step/s\n",
      "global step 4030, epoch: 3, batch: 976, loss: 0.07113, auc: 0.95722, f1 score: 0.62831, speed: 0.24 step/s\n",
      "global step 4040, epoch: 3, batch: 986, loss: 0.06731, auc: 0.95801, f1 score: 0.62288, speed: 0.23 step/s\n",
      "eval loss: 0.08739, auc: 0.94924, f1 score: 0.59003, precison: 0.55633, recall: 0.62806\n",
      "global step 4050, epoch: 3, batch: 996, loss: 0.08126, auc: 0.95724, f1 score: 0.63388, speed: 0.04 step/s\n",
      "global step 4060, epoch: 3, batch: 1006, loss: 0.06949, auc: 0.95313, f1 score: 0.60159, speed: 0.23 step/s\n",
      "global step 4070, epoch: 3, batch: 1016, loss: 0.10425, auc: 0.95144, f1 score: 0.59839, speed: 0.23 step/s\n",
      "global step 4080, epoch: 3, batch: 1026, loss: 0.08063, auc: 0.95178, f1 score: 0.60041, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-05 02:10:13,366] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 02:10:13,516] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 02:10:13,516] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08702, auc: 0.94952, f1 score: 0.59569, precison: 0.57120, recall: 0.62237\n",
      "global step 4090, epoch: 3, batch: 1036, loss: 0.07372, auc: 0.96190, f1 score: 0.62624, speed: 0.04 step/s\n",
      "global step 4100, epoch: 3, batch: 1046, loss: 0.07382, auc: 0.96324, f1 score: 0.62760, speed: 0.25 step/s\n",
      "global step 4110, epoch: 3, batch: 1056, loss: 0.09415, auc: 0.96000, f1 score: 0.61940, speed: 0.23 step/s\n",
      "global step 4120, epoch: 3, batch: 1066, loss: 0.09244, auc: 0.96124, f1 score: 0.62212, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-05 02:16:43,675] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 02:16:43,818] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 02:16:43,818] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08674, auc: 0.94932, f1 score: 0.59793, precison: 0.59346, recall: 0.60246\n",
      "global step 4130, epoch: 3, batch: 1076, loss: 0.07545, auc: 0.94988, f1 score: 0.61219, speed: 0.04 step/s\n",
      "global step 4140, epoch: 3, batch: 1086, loss: 0.08622, auc: 0.95353, f1 score: 0.62060, speed: 0.23 step/s\n",
      "global step 4150, epoch: 3, batch: 1096, loss: 0.08257, auc: 0.95415, f1 score: 0.62625, speed: 0.23 step/s\n",
      "global step 4160, epoch: 3, batch: 1106, loss: 0.06586, auc: 0.95588, f1 score: 0.61538, speed: 0.22 step/s\n",
      "eval loss: 0.08651, auc: 0.95089, f1 score: 0.59419, precison: 0.59509, recall: 0.59330\n",
      "global step 4170, epoch: 3, batch: 1116, loss: 0.06725, auc: 0.95358, f1 score: 0.60942, speed: 0.04 step/s\n",
      "global step 4180, epoch: 3, batch: 1126, loss: 0.09133, auc: 0.95267, f1 score: 0.60042, speed: 0.24 step/s\n",
      "global step 4190, epoch: 3, batch: 1136, loss: 0.06465, auc: 0.95096, f1 score: 0.59635, speed: 0.24 step/s\n",
      "global step 4200, epoch: 3, batch: 1146, loss: 0.08342, auc: 0.95233, f1 score: 0.60664, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-05 02:29:48,153] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 02:29:48,302] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 02:29:48,302] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08698, auc: 0.95060, f1 score: 0.59810, precison: 0.60086, recall: 0.59535\n",
      "global step 4210, epoch: 3, batch: 1156, loss: 0.09437, auc: 0.94790, f1 score: 0.59701, speed: 0.04 step/s\n",
      "global step 4220, epoch: 3, batch: 1166, loss: 0.07314, auc: 0.95158, f1 score: 0.60614, speed: 0.23 step/s\n",
      "global step 4230, epoch: 3, batch: 1176, loss: 0.06944, auc: 0.95388, f1 score: 0.61714, speed: 0.23 step/s\n",
      "global step 4240, epoch: 3, batch: 1186, loss: 0.08111, auc: 0.95380, f1 score: 0.61352, speed: 0.23 step/s\n",
      "eval loss: 0.08681, auc: 0.95012, f1 score: 0.59519, precison: 0.57638, recall: 0.61526\n",
      "global step 4250, epoch: 3, batch: 1196, loss: 0.07761, auc: 0.96139, f1 score: 0.62420, speed: 0.04 step/s\n",
      "global step 4260, epoch: 3, batch: 1206, loss: 0.06306, auc: 0.96207, f1 score: 0.62888, speed: 0.24 step/s\n",
      "global step 4270, epoch: 3, batch: 1216, loss: 0.07322, auc: 0.96195, f1 score: 0.63021, speed: 0.23 step/s\n",
      "global step 4280, epoch: 3, batch: 1226, loss: 0.08034, auc: 0.96074, f1 score: 0.62538, speed: 0.23 step/s\n",
      "eval loss: 0.08642, auc: 0.95113, f1 score: 0.59804, precison: 0.59004, recall: 0.60626\n",
      "global step 4290, epoch: 3, batch: 1236, loss: 0.06837, auc: 0.95083, f1 score: 0.58824, speed: 0.04 step/s\n",
      "global step 4300, epoch: 3, batch: 1246, loss: 0.09878, auc: 0.95449, f1 score: 0.59529, speed: 0.23 step/s\n",
      "global step 4310, epoch: 3, batch: 1256, loss: 0.06538, auc: 0.95670, f1 score: 0.59956, speed: 0.24 step/s\n",
      "global step 4320, epoch: 3, batch: 1266, loss: 0.07434, auc: 0.95762, f1 score: 0.60707, speed: 0.24 step/s\n",
      "eval loss: 0.08752, auc: 0.94915, f1 score: 0.59050, precison: 0.57655, recall: 0.60515\n",
      "global step 4330, epoch: 3, batch: 1276, loss: 0.06859, auc: 0.95659, f1 score: 0.63989, speed: 0.04 step/s\n",
      "global step 4340, epoch: 3, batch: 1286, loss: 0.07966, auc: 0.95828, f1 score: 0.64262, speed: 0.23 step/s\n",
      "global step 4350, epoch: 3, batch: 1296, loss: 0.09315, auc: 0.95701, f1 score: 0.62895, speed: 0.22 step/s\n",
      "global step 4360, epoch: 3, batch: 1306, loss: 0.08306, auc: 0.95740, f1 score: 0.62081, speed: 0.23 step/s\n",
      "eval loss: 0.08757, auc: 0.94966, f1 score: 0.59290, precison: 0.58984, recall: 0.59599\n",
      "global step 4370, epoch: 3, batch: 1316, loss: 0.08702, auc: 0.95199, f1 score: 0.62275, speed: 0.04 step/s\n",
      "global step 4380, epoch: 3, batch: 1326, loss: 0.09085, auc: 0.95296, f1 score: 0.61962, speed: 0.25 step/s\n",
      "global step 4390, epoch: 3, batch: 1336, loss: 0.07520, auc: 0.95552, f1 score: 0.62747, speed: 0.22 step/s\n",
      "global step 4400, epoch: 3, batch: 1346, loss: 0.07704, auc: 0.95533, f1 score: 0.61861, speed: 0.23 step/s\n",
      "eval loss: 0.08837, auc: 0.94954, f1 score: 0.57626, precison: 0.57116, recall: 0.58145\n",
      "global step 4410, epoch: 3, batch: 1356, loss: 0.08263, auc: 0.95457, f1 score: 0.59801, speed: 0.04 step/s\n",
      "global step 4420, epoch: 3, batch: 1366, loss: 0.08616, auc: 0.94986, f1 score: 0.59987, speed: 0.23 step/s\n",
      "global step 4430, epoch: 3, batch: 1376, loss: 0.09888, auc: 0.94794, f1 score: 0.59958, speed: 0.22 step/s\n",
      "global step 4440, epoch: 3, batch: 1386, loss: 0.07254, auc: 0.94955, f1 score: 0.60474, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-04-05 03:09:01,249] [    INFO]\u001b[0m - Configuration saved in ernie_ckpt/config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 03:09:01,402] [    INFO]\u001b[0m - tokenizer config file saved in ernie_ckpt/tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-04-05 03:09:01,403] [    INFO]\u001b[0m - Special tokens file saved in ernie_ckpt/special_tokens_map.json\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08623, auc: 0.95069, f1 score: 0.60178, precison: 0.58872, recall: 0.61542\n",
      "global step 4450, epoch: 3, batch: 1396, loss: 0.08783, auc: 0.96301, f1 score: 0.65123, speed: 0.04 step/s\n",
      "global step 4460, epoch: 3, batch: 1406, loss: 0.07202, auc: 0.96125, f1 score: 0.64281, speed: 0.24 step/s\n",
      "global step 4470, epoch: 3, batch: 1416, loss: 0.07738, auc: 0.96116, f1 score: 0.63372, speed: 0.23 step/s\n",
      "global step 4480, epoch: 3, batch: 1426, loss: 0.07894, auc: 0.95931, f1 score: 0.63529, speed: 0.23 step/s\n",
      "eval loss: 0.08790, auc: 0.94775, f1 score: 0.59152, precison: 0.57353, recall: 0.61068\n",
      "global step 4490, epoch: 3, batch: 1436, loss: 0.05346, auc: 0.95068, f1 score: 0.63202, speed: 0.04 step/s\n",
      "global step 4500, epoch: 3, batch: 1446, loss: 0.06686, auc: 0.95787, f1 score: 0.65147, speed: 0.24 step/s\n",
      "global step 4510, epoch: 3, batch: 1456, loss: 0.07323, auc: 0.95501, f1 score: 0.63764, speed: 0.23 step/s\n",
      "global step 4520, epoch: 3, batch: 1466, loss: 0.09391, auc: 0.95520, f1 score: 0.62887, speed: 0.23 step/s\n",
      "eval loss: 0.08691, auc: 0.94975, f1 score: 0.59581, precison: 0.60045, recall: 0.59125\n",
      "global step 4530, epoch: 3, batch: 1476, loss: 0.06853, auc: 0.95096, f1 score: 0.60969, speed: 0.04 step/s\n",
      "global step 4540, epoch: 3, batch: 1486, loss: 0.06985, auc: 0.95372, f1 score: 0.60909, speed: 0.24 step/s\n",
      "global step 4550, epoch: 3, batch: 1496, loss: 0.09511, auc: 0.95682, f1 score: 0.60892, speed: 0.24 step/s\n",
      "global step 4560, epoch: 3, batch: 1506, loss: 0.08615, auc: 0.95529, f1 score: 0.60610, speed: 0.22 step/s\n",
      "eval loss: 0.08720, auc: 0.95012, f1 score: 0.59251, precison: 0.55534, recall: 0.63501\n",
      "global step 4570, epoch: 3, batch: 1516, loss: 0.09460, auc: 0.96199, f1 score: 0.62516, speed: 0.04 step/s\n",
      "global step 4580, epoch: 3, batch: 1526, loss: 0.10987, auc: 0.95303, f1 score: 0.61668, speed: 0.23 step/s\n",
      "global step 4590, epoch: 4, batch: 9, loss: 0.08844, auc: 0.95679, f1 score: 0.62303, speed: 0.27 step/s\n",
      "global step 4600, epoch: 4, batch: 19, loss: 0.05363, auc: 0.95935, f1 score: 0.62837, speed: 0.23 step/s\n",
      "eval loss: 0.08651, auc: 0.95078, f1 score: 0.59653, precison: 0.57989, recall: 0.61416\n",
      "global step 4610, epoch: 4, batch: 29, loss: 0.07952, auc: 0.96867, f1 score: 0.64424, speed: 0.04 step/s\n",
      "global step 4620, epoch: 4, batch: 39, loss: 0.08493, auc: 0.96273, f1 score: 0.63805, speed: 0.24 step/s\n",
      "global step 4630, epoch: 4, batch: 49, loss: 0.08615, auc: 0.96377, f1 score: 0.63167, speed: 0.22 step/s\n",
      "global step 4640, epoch: 4, batch: 59, loss: 0.07599, auc: 0.96375, f1 score: 0.63306, speed: 0.24 step/s\n",
      "eval loss: 0.08944, auc: 0.94691, f1 score: 0.58422, precison: 0.58103, recall: 0.58745\n",
      "global step 4650, epoch: 4, batch: 69, loss: 0.08932, auc: 0.96036, f1 score: 0.63380, speed: 0.04 step/s\n",
      "global step 4660, epoch: 4, batch: 79, loss: 0.06642, auc: 0.96055, f1 score: 0.63572, speed: 0.24 step/s\n",
      "global step 4670, epoch: 4, batch: 89, loss: 0.06491, auc: 0.96116, f1 score: 0.64253, speed: 0.23 step/s\n",
      "global step 4680, epoch: 4, batch: 99, loss: 0.07255, auc: 0.96365, f1 score: 0.64807, speed: 0.22 step/s\n",
      "eval loss: 0.08870, auc: 0.94893, f1 score: 0.58689, precison: 0.57851, recall: 0.59551\n",
      "global step 4690, epoch: 4, batch: 109, loss: 0.08612, auc: 0.96400, f1 score: 0.65970, speed: 0.04 step/s\n",
      "global step 4700, epoch: 4, batch: 119, loss: 0.07900, auc: 0.96332, f1 score: 0.65064, speed: 0.23 step/s\n",
      "global step 4710, epoch: 4, batch: 129, loss: 0.06580, auc: 0.96642, f1 score: 0.65287, speed: 0.24 step/s\n",
      "global step 4720, epoch: 4, batch: 139, loss: 0.09559, auc: 0.96594, f1 score: 0.64988, speed: 0.23 step/s\n",
      "eval loss: 0.08922, auc: 0.94856, f1 score: 0.57796, precison: 0.56368, recall: 0.59298\n",
      "global step 4730, epoch: 4, batch: 149, loss: 0.08106, auc: 0.96928, f1 score: 0.67045, speed: 0.04 step/s\n",
      "global step 4740, epoch: 4, batch: 159, loss: 0.06462, auc: 0.96671, f1 score: 0.66185, speed: 0.23 step/s\n",
      "global step 4750, epoch: 4, batch: 169, loss: 0.07142, auc: 0.96634, f1 score: 0.65772, speed: 0.24 step/s\n",
      "global step 4760, epoch: 4, batch: 179, loss: 0.07127, auc: 0.96532, f1 score: 0.65357, speed: 0.23 step/s\n",
      "eval loss: 0.08970, auc: 0.94687, f1 score: 0.57788, precison: 0.57145, recall: 0.58445\n",
      "global step 4770, epoch: 4, batch: 189, loss: 0.08956, auc: 0.95913, f1 score: 0.62197, speed: 0.04 step/s\n",
      "global step 4780, epoch: 4, batch: 199, loss: 0.07357, auc: 0.96426, f1 score: 0.64847, speed: 0.23 step/s\n",
      "global step 4790, epoch: 4, batch: 209, loss: 0.06420, auc: 0.96577, f1 score: 0.66141, speed: 0.23 step/s\n",
      "global step 4800, epoch: 4, batch: 219, loss: 0.07406, auc: 0.96588, f1 score: 0.65527, speed: 0.23 step/s\n",
      "eval loss: 0.08815, auc: 0.94871, f1 score: 0.58997, precison: 0.56829, recall: 0.61337\n",
      "global step 4810, epoch: 4, batch: 229, loss: 0.06681, auc: 0.96394, f1 score: 0.64498, speed: 0.04 step/s\n",
      "global step 4820, epoch: 4, batch: 239, loss: 0.07998, auc: 0.96382, f1 score: 0.65536, speed: 0.23 step/s\n",
      "global step 4830, epoch: 4, batch: 249, loss: 0.08530, auc: 0.96406, f1 score: 0.64877, speed: 0.23 step/s\n",
      "global step 4840, epoch: 4, batch: 259, loss: 0.06804, auc: 0.96487, f1 score: 0.64694, speed: 0.23 step/s\n",
      "eval loss: 0.08884, auc: 0.94813, f1 score: 0.59114, precison: 0.55091, recall: 0.63770\n",
      "global step 4850, epoch: 4, batch: 269, loss: 0.07887, auc: 0.96459, f1 score: 0.65272, speed: 0.04 step/s\n",
      "global step 4860, epoch: 4, batch: 279, loss: 0.07551, auc: 0.96696, f1 score: 0.66862, speed: 0.24 step/s\n",
      "global step 4870, epoch: 4, batch: 289, loss: 0.07039, auc: 0.96632, f1 score: 0.66142, speed: 0.22 step/s\n",
      "global step 4880, epoch: 4, batch: 299, loss: 0.07279, auc: 0.96655, f1 score: 0.65769, speed: 0.24 step/s\n",
      "eval loss: 0.08821, auc: 0.94855, f1 score: 0.59867, precison: 0.57442, recall: 0.62506\n",
      "global step 4890, epoch: 4, batch: 309, loss: 0.05636, auc: 0.96729, f1 score: 0.65949, speed: 0.04 step/s\n",
      "global step 4900, epoch: 4, batch: 319, loss: 0.06369, auc: 0.96393, f1 score: 0.65403, speed: 0.23 step/s\n",
      "global step 4910, epoch: 4, batch: 329, loss: 0.06020, auc: 0.96600, f1 score: 0.66576, speed: 0.23 step/s\n",
      "global step 4920, epoch: 4, batch: 339, loss: 0.07133, auc: 0.96574, f1 score: 0.66508, speed: 0.24 step/s\n",
      "eval loss: 0.08932, auc: 0.94812, f1 score: 0.57911, precison: 0.53495, recall: 0.63122\n",
      "global step 4930, epoch: 4, batch: 349, loss: 0.07666, auc: 0.95466, f1 score: 0.63171, speed: 0.04 step/s\n",
      "global step 4940, epoch: 4, batch: 359, loss: 0.07026, auc: 0.96012, f1 score: 0.63349, speed: 0.24 step/s\n",
      "global step 4950, epoch: 4, batch: 369, loss: 0.04748, auc: 0.96398, f1 score: 0.63906, speed: 0.24 step/s\n",
      "global step 4960, epoch: 4, batch: 379, loss: 0.07852, auc: 0.96208, f1 score: 0.63952, speed: 0.24 step/s\n",
      "eval loss: 0.08896, auc: 0.94831, f1 score: 0.59241, precison: 0.60970, recall: 0.57608\n",
      "global step 4970, epoch: 4, batch: 389, loss: 0.07751, auc: 0.95990, f1 score: 0.62484, speed: 0.04 step/s\n",
      "global step 4980, epoch: 4, batch: 399, loss: 0.06402, auc: 0.96250, f1 score: 0.63451, speed: 0.25 step/s\n",
      "global step 4990, epoch: 4, batch: 409, loss: 0.08336, auc: 0.96149, f1 score: 0.63482, speed: 0.22 step/s\n",
      "global step 5000, epoch: 4, batch: 419, loss: 0.05190, auc: 0.96448, f1 score: 0.65003, speed: 0.23 step/s\n",
      "eval loss: 0.08917, auc: 0.94630, f1 score: 0.59248, precison: 0.59025, recall: 0.59472\n",
      "global step 5010, epoch: 4, batch: 429, loss: 0.06928, auc: 0.97048, f1 score: 0.65614, speed: 0.04 step/s\n",
      "global step 5020, epoch: 4, batch: 439, loss: 0.05280, auc: 0.96889, f1 score: 0.66014, speed: 0.23 step/s\n",
      "global step 5030, epoch: 4, batch: 449, loss: 0.05721, auc: 0.96810, f1 score: 0.65472, speed: 0.23 step/s\n",
      "global step 5040, epoch: 4, batch: 459, loss: 0.06530, auc: 0.96736, f1 score: 0.65257, speed: 0.23 step/s\n",
      "eval loss: 0.09011, auc: 0.94531, f1 score: 0.59337, precison: 0.59109, recall: 0.59567\n",
      "global step 5050, epoch: 4, batch: 469, loss: 0.07358, auc: 0.96502, f1 score: 0.65943, speed: 0.04 step/s\n",
      "global step 5060, epoch: 4, batch: 479, loss: 0.08005, auc: 0.96418, f1 score: 0.66713, speed: 0.25 step/s\n",
      "global step 5070, epoch: 4, batch: 489, loss: 0.07664, auc: 0.96399, f1 score: 0.66294, speed: 0.23 step/s\n",
      "global step 5080, epoch: 4, batch: 499, loss: 0.08290, auc: 0.96355, f1 score: 0.65417, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08994, auc: 0.94734, f1 score: 0.57800, precison: 0.56664, recall: 0.58982\n",
      "global step 5090, epoch: 4, batch: 509, loss: 0.07177, auc: 0.96406, f1 score: 0.66667, speed: 0.04 step/s\n",
      "global step 5100, epoch: 4, batch: 519, loss: 0.08012, auc: 0.96316, f1 score: 0.66532, speed: 0.24 step/s\n",
      "global step 5110, epoch: 4, batch: 529, loss: 0.09269, auc: 0.96516, f1 score: 0.66637, speed: 0.24 step/s\n",
      "global step 5120, epoch: 4, batch: 539, loss: 0.06346, auc: 0.96529, f1 score: 0.65930, speed: 0.24 step/s\n",
      "eval loss: 0.08892, auc: 0.94729, f1 score: 0.58889, precison: 0.56874, recall: 0.61052\n",
      "global step 5130, epoch: 4, batch: 549, loss: 0.09059, auc: 0.96262, f1 score: 0.64435, speed: 0.04 step/s\n",
      "global step 5140, epoch: 4, batch: 559, loss: 0.06577, auc: 0.96303, f1 score: 0.65217, speed: 0.24 step/s\n",
      "global step 5150, epoch: 4, batch: 569, loss: 0.06320, auc: 0.96271, f1 score: 0.64270, speed: 0.24 step/s\n",
      "global step 5160, epoch: 4, batch: 579, loss: 0.08784, auc: 0.96287, f1 score: 0.64189, speed: 0.23 step/s\n",
      "eval loss: 0.08762, auc: 0.94981, f1 score: 0.59509, precison: 0.58955, recall: 0.60073\n",
      "global step 5170, epoch: 4, batch: 589, loss: 0.06937, auc: 0.96238, f1 score: 0.63072, speed: 0.04 step/s\n",
      "global step 5180, epoch: 4, batch: 599, loss: 0.07130, auc: 0.96165, f1 score: 0.64675, speed: 0.24 step/s\n",
      "global step 5190, epoch: 4, batch: 609, loss: 0.10336, auc: 0.96097, f1 score: 0.63092, speed: 0.24 step/s\n",
      "global step 5200, epoch: 4, batch: 619, loss: 0.08155, auc: 0.96300, f1 score: 0.63325, speed: 0.22 step/s\n",
      "eval loss: 0.08727, auc: 0.94904, f1 score: 0.60075, precison: 0.59443, recall: 0.60720\n",
      "global step 5210, epoch: 4, batch: 629, loss: 0.05925, auc: 0.97322, f1 score: 0.67112, speed: 0.04 step/s\n",
      "global step 5220, epoch: 4, batch: 639, loss: 0.09224, auc: 0.96676, f1 score: 0.64300, speed: 0.23 step/s\n",
      "global step 5230, epoch: 4, batch: 649, loss: 0.09165, auc: 0.96439, f1 score: 0.63862, speed: 0.23 step/s\n",
      "global step 5240, epoch: 4, batch: 659, loss: 0.06869, auc: 0.96397, f1 score: 0.63183, speed: 0.23 step/s\n",
      "eval loss: 0.08748, auc: 0.94948, f1 score: 0.59552, precison: 0.55024, recall: 0.64892\n",
      "global step 5250, epoch: 4, batch: 669, loss: 0.09777, auc: 0.96485, f1 score: 0.62518, speed: 0.04 step/s\n",
      "global step 5260, epoch: 4, batch: 679, loss: 0.06769, auc: 0.96623, f1 score: 0.64516, speed: 0.23 step/s\n",
      "global step 5270, epoch: 4, batch: 689, loss: 0.07002, auc: 0.96670, f1 score: 0.65069, speed: 0.23 step/s\n",
      "global step 5280, epoch: 4, batch: 699, loss: 0.09311, auc: 0.96608, f1 score: 0.64724, speed: 0.23 step/s\n",
      "eval loss: 0.08761, auc: 0.94943, f1 score: 0.59097, precison: 0.55851, recall: 0.62743\n",
      "global step 5290, epoch: 4, batch: 709, loss: 0.09089, auc: 0.95723, f1 score: 0.60719, speed: 0.04 step/s\n",
      "global step 5300, epoch: 4, batch: 719, loss: 0.06969, auc: 0.95912, f1 score: 0.62905, speed: 0.24 step/s\n",
      "global step 5310, epoch: 4, batch: 729, loss: 0.06616, auc: 0.95801, f1 score: 0.62565, speed: 0.23 step/s\n",
      "global step 5320, epoch: 4, batch: 739, loss: 0.08103, auc: 0.96011, f1 score: 0.63887, speed: 0.22 step/s\n",
      "eval loss: 0.08780, auc: 0.94889, f1 score: 0.58760, precison: 0.57175, recall: 0.60436\n",
      "global step 5330, epoch: 4, batch: 749, loss: 0.07256, auc: 0.96415, f1 score: 0.66229, speed: 0.04 step/s\n",
      "global step 5340, epoch: 4, batch: 759, loss: 0.05421, auc: 0.96615, f1 score: 0.67843, speed: 0.24 step/s\n",
      "global step 5350, epoch: 4, batch: 769, loss: 0.06165, auc: 0.96584, f1 score: 0.67346, speed: 0.24 step/s\n",
      "global step 5360, epoch: 4, batch: 779, loss: 0.06761, auc: 0.96660, f1 score: 0.66953, speed: 0.23 step/s\n",
      "eval loss: 0.08800, auc: 0.94826, f1 score: 0.59066, precison: 0.57052, recall: 0.61226\n",
      "global step 5370, epoch: 4, batch: 789, loss: 0.08127, auc: 0.96054, f1 score: 0.62679, speed: 0.04 step/s\n",
      "global step 5380, epoch: 4, batch: 799, loss: 0.06476, auc: 0.96674, f1 score: 0.64162, speed: 0.23 step/s\n",
      "global step 5390, epoch: 4, batch: 809, loss: 0.06520, auc: 0.96686, f1 score: 0.64068, speed: 0.23 step/s\n",
      "global step 5400, epoch: 4, batch: 819, loss: 0.07261, auc: 0.96581, f1 score: 0.64512, speed: 0.23 step/s\n",
      "eval loss: 0.08747, auc: 0.94899, f1 score: 0.59560, precison: 0.59664, recall: 0.59456\n",
      "global step 5410, epoch: 4, batch: 829, loss: 0.06918, auc: 0.96283, f1 score: 0.65629, speed: 0.04 step/s\n",
      "global step 5420, epoch: 4, batch: 839, loss: 0.07374, auc: 0.96253, f1 score: 0.65727, speed: 0.24 step/s\n",
      "global step 5430, epoch: 4, batch: 849, loss: 0.07207, auc: 0.96424, f1 score: 0.64856, speed: 0.22 step/s\n",
      "global step 5440, epoch: 4, batch: 859, loss: 0.06388, auc: 0.96558, f1 score: 0.64911, speed: 0.23 step/s\n",
      "eval loss: 0.08743, auc: 0.94994, f1 score: 0.59420, precison: 0.57220, recall: 0.61795\n",
      "global step 5450, epoch: 4, batch: 869, loss: 0.09614, auc: 0.96584, f1 score: 0.63056, speed: 0.04 step/s\n",
      "global step 5460, epoch: 4, batch: 879, loss: 0.08387, auc: 0.96537, f1 score: 0.63437, speed: 0.23 step/s\n",
      "global step 5470, epoch: 4, batch: 889, loss: 0.10219, auc: 0.96561, f1 score: 0.64494, speed: 0.23 step/s\n",
      "global step 5480, epoch: 4, batch: 899, loss: 0.07507, auc: 0.96566, f1 score: 0.64759, speed: 0.23 step/s\n",
      "eval loss: 0.08669, auc: 0.95096, f1 score: 0.59837, precison: 0.61845, recall: 0.57955\n",
      "global step 5490, epoch: 4, batch: 909, loss: 0.06896, auc: 0.96558, f1 score: 0.65640, speed: 0.04 step/s\n",
      "global step 5500, epoch: 4, batch: 919, loss: 0.07449, auc: 0.96589, f1 score: 0.64553, speed: 0.23 step/s\n",
      "global step 5510, epoch: 4, batch: 929, loss: 0.05333, auc: 0.96415, f1 score: 0.63526, speed: 0.23 step/s\n",
      "global step 5520, epoch: 4, batch: 939, loss: 0.06610, auc: 0.96499, f1 score: 0.63386, speed: 0.24 step/s\n",
      "eval loss: 0.08750, auc: 0.94917, f1 score: 0.59143, precison: 0.58756, recall: 0.59535\n",
      "global step 5530, epoch: 4, batch: 949, loss: 0.05896, auc: 0.96613, f1 score: 0.66418, speed: 0.04 step/s\n",
      "global step 5540, epoch: 4, batch: 959, loss: 0.09400, auc: 0.96725, f1 score: 0.66085, speed: 0.23 step/s\n",
      "global step 5550, epoch: 4, batch: 969, loss: 0.09667, auc: 0.96534, f1 score: 0.64262, speed: 0.23 step/s\n",
      "global step 5560, epoch: 4, batch: 979, loss: 0.06834, auc: 0.96434, f1 score: 0.64635, speed: 0.23 step/s\n",
      "eval loss: 0.08929, auc: 0.94763, f1 score: 0.58596, precison: 0.57939, recall: 0.59267\n",
      "global step 5570, epoch: 4, batch: 989, loss: 0.07967, auc: 0.96205, f1 score: 0.66756, speed: 0.04 step/s\n",
      "global step 5580, epoch: 4, batch: 999, loss: 0.07543, auc: 0.96173, f1 score: 0.64730, speed: 0.24 step/s\n",
      "global step 5590, epoch: 4, batch: 1009, loss: 0.06699, auc: 0.96245, f1 score: 0.63943, speed: 0.24 step/s\n",
      "global step 5600, epoch: 4, batch: 1019, loss: 0.05932, auc: 0.96323, f1 score: 0.63530, speed: 0.23 step/s\n",
      "eval loss: 0.08717, auc: 0.95013, f1 score: 0.59559, precison: 0.56176, recall: 0.63375\n",
      "global step 5610, epoch: 4, batch: 1029, loss: 0.06251, auc: 0.96481, f1 score: 0.65934, speed: 0.04 step/s\n",
      "global step 5620, epoch: 4, batch: 1039, loss: 0.08155, auc: 0.96266, f1 score: 0.65242, speed: 0.24 step/s\n",
      "global step 5630, epoch: 4, batch: 1049, loss: 0.07543, auc: 0.96330, f1 score: 0.64593, speed: 0.23 step/s\n",
      "global step 5640, epoch: 4, batch: 1059, loss: 0.08981, auc: 0.96282, f1 score: 0.64199, speed: 0.23 step/s\n",
      "eval loss: 0.08651, auc: 0.95064, f1 score: 0.60068, precison: 0.61159, recall: 0.59014\n",
      "global step 5650, epoch: 4, batch: 1069, loss: 0.07962, auc: 0.96301, f1 score: 0.63201, speed: 0.04 step/s\n",
      "global step 5660, epoch: 4, batch: 1079, loss: 0.07110, auc: 0.96595, f1 score: 0.65545, speed: 0.24 step/s\n",
      "global step 5670, epoch: 4, batch: 1089, loss: 0.07130, auc: 0.96664, f1 score: 0.65463, speed: 0.24 step/s\n",
      "global step 5680, epoch: 4, batch: 1099, loss: 0.07925, auc: 0.96661, f1 score: 0.64970, speed: 0.21 step/s\n",
      "eval loss: 0.08805, auc: 0.94985, f1 score: 0.58544, precison: 0.57351, recall: 0.59788\n",
      "global step 5690, epoch: 4, batch: 1109, loss: 0.09346, auc: 0.96455, f1 score: 0.62360, speed: 0.04 step/s\n",
      "global step 5700, epoch: 4, batch: 1119, loss: 0.10148, auc: 0.95851, f1 score: 0.62153, speed: 0.23 step/s\n",
      "global step 5710, epoch: 4, batch: 1129, loss: 0.08289, auc: 0.96069, f1 score: 0.62747, speed: 0.22 step/s\n",
      "global step 5720, epoch: 4, batch: 1139, loss: 0.05169, auc: 0.96271, f1 score: 0.63465, speed: 0.24 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08759, auc: 0.95017, f1 score: 0.59878, precison: 0.59239, recall: 0.60531\n",
      "global step 5730, epoch: 4, batch: 1149, loss: 0.05976, auc: 0.95792, f1 score: 0.63083, speed: 0.04 step/s\n",
      "global step 5740, epoch: 4, batch: 1159, loss: 0.09502, auc: 0.96200, f1 score: 0.63523, speed: 0.23 step/s\n",
      "global step 5750, epoch: 4, batch: 1169, loss: 0.08237, auc: 0.96602, f1 score: 0.64621, speed: 0.24 step/s\n",
      "global step 5760, epoch: 4, batch: 1179, loss: 0.08521, auc: 0.96497, f1 score: 0.64115, speed: 0.23 step/s\n",
      "eval loss: 0.08743, auc: 0.94957, f1 score: 0.59417, precison: 0.59051, recall: 0.59788\n",
      "global step 5770, epoch: 4, batch: 1189, loss: 0.07468, auc: 0.96857, f1 score: 0.64525, speed: 0.04 step/s\n",
      "global step 5780, epoch: 4, batch: 1199, loss: 0.08469, auc: 0.96743, f1 score: 0.64471, speed: 0.24 step/s\n",
      "global step 5790, epoch: 4, batch: 1209, loss: 0.09009, auc: 0.96671, f1 score: 0.64114, speed: 0.24 step/s\n",
      "global step 5800, epoch: 4, batch: 1219, loss: 0.08165, auc: 0.96566, f1 score: 0.64286, speed: 0.24 step/s\n",
      "eval loss: 0.08906, auc: 0.94760, f1 score: 0.58975, precison: 0.57828, recall: 0.60167\n",
      "global step 5810, epoch: 4, batch: 1229, loss: 0.09042, auc: 0.96172, f1 score: 0.61425, speed: 0.04 step/s\n",
      "global step 5820, epoch: 4, batch: 1239, loss: 0.05436, auc: 0.96303, f1 score: 0.63031, speed: 0.24 step/s\n",
      "global step 5830, epoch: 4, batch: 1249, loss: 0.06527, auc: 0.96587, f1 score: 0.64378, speed: 0.22 step/s\n",
      "global step 5840, epoch: 4, batch: 1259, loss: 0.07993, auc: 0.96494, f1 score: 0.63835, speed: 0.22 step/s\n",
      "eval loss: 0.08825, auc: 0.94887, f1 score: 0.58683, precison: 0.56695, recall: 0.60815\n",
      "global step 5850, epoch: 4, batch: 1269, loss: 0.06413, auc: 0.97218, f1 score: 0.65793, speed: 0.04 step/s\n",
      "global step 5860, epoch: 4, batch: 1279, loss: 0.05451, auc: 0.96969, f1 score: 0.64869, speed: 0.23 step/s\n",
      "global step 5870, epoch: 4, batch: 1289, loss: 0.05653, auc: 0.96722, f1 score: 0.64427, speed: 0.23 step/s\n",
      "global step 5880, epoch: 4, batch: 1299, loss: 0.05332, auc: 0.96548, f1 score: 0.64987, speed: 0.22 step/s\n",
      "eval loss: 0.08802, auc: 0.94905, f1 score: 0.59893, precison: 0.59714, recall: 0.60073\n",
      "global step 5890, epoch: 4, batch: 1309, loss: 0.04891, auc: 0.96512, f1 score: 0.67548, speed: 0.04 step/s\n",
      "global step 5900, epoch: 4, batch: 1319, loss: 0.07327, auc: 0.96132, f1 score: 0.64481, speed: 0.24 step/s\n",
      "global step 5910, epoch: 4, batch: 1329, loss: 0.08240, auc: 0.96360, f1 score: 0.66031, speed: 0.24 step/s\n",
      "global step 5920, epoch: 4, batch: 1339, loss: 0.05699, auc: 0.96459, f1 score: 0.65819, speed: 0.23 step/s\n",
      "eval loss: 0.08910, auc: 0.94888, f1 score: 0.57855, precison: 0.54548, recall: 0.61590\n",
      "global step 5930, epoch: 4, batch: 1349, loss: 0.08523, auc: 0.95851, f1 score: 0.64722, speed: 0.04 step/s\n",
      "global step 5940, epoch: 4, batch: 1359, loss: 0.05967, auc: 0.96140, f1 score: 0.63602, speed: 0.24 step/s\n",
      "global step 5950, epoch: 4, batch: 1369, loss: 0.08820, auc: 0.96185, f1 score: 0.63636, speed: 0.24 step/s\n",
      "global step 5960, epoch: 4, batch: 1379, loss: 0.07544, auc: 0.96121, f1 score: 0.63447, speed: 0.22 step/s\n",
      "eval loss: 0.08902, auc: 0.94827, f1 score: 0.58466, precison: 0.55288, recall: 0.62032\n",
      "global step 5970, epoch: 4, batch: 1389, loss: 0.08448, auc: 0.95140, f1 score: 0.58523, speed: 0.04 step/s\n",
      "global step 5980, epoch: 4, batch: 1399, loss: 0.10392, auc: 0.95555, f1 score: 0.59786, speed: 0.24 step/s\n",
      "global step 5990, epoch: 4, batch: 1409, loss: 0.07470, auc: 0.95905, f1 score: 0.59983, speed: 0.23 step/s\n",
      "global step 6000, epoch: 4, batch: 1419, loss: 0.06553, auc: 0.96018, f1 score: 0.60820, speed: 0.22 step/s\n",
      "eval loss: 0.08785, auc: 0.94984, f1 score: 0.59269, precison: 0.55468, recall: 0.63628\n",
      "global step 6010, epoch: 4, batch: 1429, loss: 0.07842, auc: 0.97023, f1 score: 0.65796, speed: 0.04 step/s\n",
      "global step 6020, epoch: 4, batch: 1439, loss: 0.08825, auc: 0.96738, f1 score: 0.64653, speed: 0.23 step/s\n",
      "global step 6030, epoch: 4, batch: 1449, loss: 0.06928, auc: 0.96512, f1 score: 0.64536, speed: 0.23 step/s\n",
      "global step 6040, epoch: 4, batch: 1459, loss: 0.07479, auc: 0.96257, f1 score: 0.64018, speed: 0.23 step/s\n",
      "eval loss: 0.08794, auc: 0.94983, f1 score: 0.59042, precison: 0.55803, recall: 0.62680\n",
      "global step 6050, epoch: 4, batch: 1469, loss: 0.06532, auc: 0.96851, f1 score: 0.65600, speed: 0.04 step/s\n",
      "global step 6060, epoch: 4, batch: 1479, loss: 0.06673, auc: 0.96421, f1 score: 0.62308, speed: 0.23 step/s\n",
      "global step 6070, epoch: 4, batch: 1489, loss: 0.05695, auc: 0.96288, f1 score: 0.61197, speed: 0.24 step/s\n",
      "global step 6080, epoch: 4, batch: 1499, loss: 0.08192, auc: 0.96390, f1 score: 0.61620, speed: 0.23 step/s\n",
      "eval loss: 0.08741, auc: 0.94995, f1 score: 0.59141, precison: 0.55993, recall: 0.62664\n",
      "global step 6090, epoch: 4, batch: 1509, loss: 0.08181, auc: 0.95782, f1 score: 0.60134, speed: 0.04 step/s\n",
      "global step 6100, epoch: 4, batch: 1519, loss: 0.07523, auc: 0.96187, f1 score: 0.62476, speed: 0.24 step/s\n",
      "global step 6110, epoch: 5, batch: 2, loss: 0.07430, auc: 0.96222, f1 score: 0.62357, speed: 0.26 step/s\n",
      "global step 6120, epoch: 5, batch: 12, loss: 0.05490, auc: 0.96323, f1 score: 0.63185, speed: 0.23 step/s\n",
      "eval loss: 0.08898, auc: 0.94765, f1 score: 0.58493, precison: 0.58305, recall: 0.58682\n",
      "global step 6130, epoch: 5, batch: 22, loss: 0.06436, auc: 0.97802, f1 score: 0.69767, speed: 0.04 step/s\n",
      "global step 6140, epoch: 5, batch: 32, loss: 0.09207, auc: 0.97020, f1 score: 0.66272, speed: 0.24 step/s\n",
      "global step 6150, epoch: 5, batch: 42, loss: 0.05914, auc: 0.97122, f1 score: 0.67634, speed: 0.24 step/s\n",
      "global step 6160, epoch: 5, batch: 52, loss: 0.07313, auc: 0.96996, f1 score: 0.68170, speed: 0.23 step/s\n",
      "eval loss: 0.09003, auc: 0.94736, f1 score: 0.58271, precison: 0.57046, recall: 0.59551\n",
      "global step 6170, epoch: 5, batch: 62, loss: 0.07901, auc: 0.96922, f1 score: 0.66753, speed: 0.04 step/s\n",
      "global step 6180, epoch: 5, batch: 72, loss: 0.03831, auc: 0.96902, f1 score: 0.68071, speed: 0.23 step/s\n",
      "global step 6190, epoch: 5, batch: 82, loss: 0.07969, auc: 0.97092, f1 score: 0.68365, speed: 0.22 step/s\n",
      "global step 6200, epoch: 5, batch: 92, loss: 0.08226, auc: 0.97211, f1 score: 0.69102, speed: 0.22 step/s\n",
      "eval loss: 0.08971, auc: 0.94682, f1 score: 0.58895, precison: 0.57954, recall: 0.59867\n",
      "global step 6210, epoch: 5, batch: 102, loss: 0.05512, auc: 0.97698, f1 score: 0.72423, speed: 0.04 step/s\n",
      "global step 6220, epoch: 5, batch: 112, loss: 0.07733, auc: 0.97484, f1 score: 0.70403, speed: 0.23 step/s\n",
      "global step 6230, epoch: 5, batch: 122, loss: 0.06649, auc: 0.97366, f1 score: 0.69047, speed: 0.24 step/s\n",
      "global step 6240, epoch: 5, batch: 132, loss: 0.04873, auc: 0.97341, f1 score: 0.69696, speed: 0.22 step/s\n",
      "eval loss: 0.09111, auc: 0.94678, f1 score: 0.57436, precison: 0.56535, recall: 0.58366\n",
      "global step 6250, epoch: 5, batch: 142, loss: 0.07152, auc: 0.97052, f1 score: 0.65888, speed: 0.04 step/s\n",
      "global step 6260, epoch: 5, batch: 152, loss: 0.05132, auc: 0.97116, f1 score: 0.66431, speed: 0.24 step/s\n",
      "global step 6270, epoch: 5, batch: 162, loss: 0.06325, auc: 0.97361, f1 score: 0.67200, speed: 0.22 step/s\n",
      "global step 6280, epoch: 5, batch: 172, loss: 0.09323, auc: 0.97286, f1 score: 0.66967, speed: 0.23 step/s\n",
      "eval loss: 0.09085, auc: 0.94541, f1 score: 0.58969, precison: 0.56926, recall: 0.61163\n",
      "global step 6290, epoch: 5, batch: 182, loss: 0.05979, auc: 0.96676, f1 score: 0.66006, speed: 0.04 step/s\n",
      "global step 6300, epoch: 5, batch: 192, loss: 0.08096, auc: 0.96833, f1 score: 0.66146, speed: 0.23 step/s\n",
      "global step 6310, epoch: 5, batch: 202, loss: 0.07446, auc: 0.97159, f1 score: 0.67739, speed: 0.24 step/s\n",
      "global step 6320, epoch: 5, batch: 212, loss: 0.06870, auc: 0.97223, f1 score: 0.68013, speed: 0.23 step/s\n",
      "eval loss: 0.09165, auc: 0.94586, f1 score: 0.57384, precison: 0.54344, recall: 0.60784\n",
      "global step 6330, epoch: 5, batch: 222, loss: 0.06799, auc: 0.96670, f1 score: 0.67882, speed: 0.04 step/s\n",
      "global step 6340, epoch: 5, batch: 232, loss: 0.06664, auc: 0.97221, f1 score: 0.68056, speed: 0.23 step/s\n",
      "global step 6350, epoch: 5, batch: 242, loss: 0.05576, auc: 0.97286, f1 score: 0.69343, speed: 0.23 step/s\n",
      "global step 6360, epoch: 5, batch: 252, loss: 0.06902, auc: 0.97215, f1 score: 0.69831, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.09163, auc: 0.94518, f1 score: 0.58776, precison: 0.58509, recall: 0.59046\n",
      "global step 6370, epoch: 5, batch: 262, loss: 0.06362, auc: 0.97686, f1 score: 0.69681, speed: 0.04 step/s\n",
      "global step 6380, epoch: 5, batch: 272, loss: 0.06667, auc: 0.96901, f1 score: 0.67989, speed: 0.24 step/s\n",
      "global step 6390, epoch: 5, batch: 282, loss: 0.06737, auc: 0.97032, f1 score: 0.68452, speed: 0.22 step/s\n",
      "global step 6400, epoch: 5, batch: 292, loss: 0.06099, auc: 0.97229, f1 score: 0.68573, speed: 0.23 step/s\n",
      "eval loss: 0.09035, auc: 0.94728, f1 score: 0.58398, precison: 0.58178, recall: 0.58619\n",
      "global step 6410, epoch: 5, batch: 302, loss: 0.06935, auc: 0.97195, f1 score: 0.69722, speed: 0.04 step/s\n",
      "global step 6420, epoch: 5, batch: 312, loss: 0.08183, auc: 0.97235, f1 score: 0.67582, speed: 0.23 step/s\n",
      "global step 6430, epoch: 5, batch: 322, loss: 0.05392, auc: 0.97107, f1 score: 0.68344, speed: 0.23 step/s\n",
      "global step 6440, epoch: 5, batch: 332, loss: 0.05851, auc: 0.97097, f1 score: 0.67883, speed: 0.24 step/s\n",
      "eval loss: 0.09116, auc: 0.94636, f1 score: 0.58404, precison: 0.55066, recall: 0.62174\n",
      "global step 6450, epoch: 5, batch: 342, loss: 0.05445, auc: 0.97593, f1 score: 0.70140, speed: 0.04 step/s\n",
      "global step 6460, epoch: 5, batch: 352, loss: 0.05553, auc: 0.97434, f1 score: 0.68589, speed: 0.23 step/s\n",
      "global step 6470, epoch: 5, batch: 362, loss: 0.06428, auc: 0.97237, f1 score: 0.68188, speed: 0.25 step/s\n",
      "global step 6480, epoch: 5, batch: 372, loss: 0.06077, auc: 0.97422, f1 score: 0.68459, speed: 0.24 step/s\n",
      "eval loss: 0.09292, auc: 0.94442, f1 score: 0.58000, precison: 0.56260, recall: 0.59851\n",
      "global step 6490, epoch: 5, batch: 382, loss: 0.06071, auc: 0.96847, f1 score: 0.68892, speed: 0.04 step/s\n",
      "global step 6500, epoch: 5, batch: 392, loss: 0.03652, auc: 0.97275, f1 score: 0.69679, speed: 0.24 step/s\n",
      "global step 6510, epoch: 5, batch: 402, loss: 0.06118, auc: 0.97224, f1 score: 0.69959, speed: 0.23 step/s\n",
      "global step 6520, epoch: 5, batch: 412, loss: 0.07028, auc: 0.96974, f1 score: 0.68558, speed: 0.22 step/s\n",
      "eval loss: 0.09248, auc: 0.94380, f1 score: 0.57365, precison: 0.55402, recall: 0.59472\n",
      "global step 6530, epoch: 5, batch: 422, loss: 0.05620, auc: 0.96474, f1 score: 0.71466, speed: 0.04 step/s\n",
      "global step 6540, epoch: 5, batch: 432, loss: 0.07008, auc: 0.97120, f1 score: 0.70588, speed: 0.23 step/s\n",
      "global step 6550, epoch: 5, batch: 442, loss: 0.06910, auc: 0.96909, f1 score: 0.67862, speed: 0.23 step/s\n",
      "global step 6560, epoch: 5, batch: 452, loss: 0.07730, auc: 0.96968, f1 score: 0.67084, speed: 0.24 step/s\n",
      "eval loss: 0.09117, auc: 0.94550, f1 score: 0.58228, precison: 0.55959, recall: 0.60689\n",
      "global step 6570, epoch: 5, batch: 462, loss: 0.07061, auc: 0.97052, f1 score: 0.66229, speed: 0.04 step/s\n",
      "global step 6580, epoch: 5, batch: 472, loss: 0.05148, auc: 0.97397, f1 score: 0.68863, speed: 0.23 step/s\n",
      "global step 6590, epoch: 5, batch: 482, loss: 0.05183, auc: 0.97258, f1 score: 0.67820, speed: 0.23 step/s\n",
      "global step 6600, epoch: 5, batch: 492, loss: 0.06275, auc: 0.97438, f1 score: 0.67404, speed: 0.23 step/s\n",
      "eval loss: 0.09135, auc: 0.94589, f1 score: 0.58127, precison: 0.55121, recall: 0.61479\n",
      "global step 6610, epoch: 5, batch: 502, loss: 0.08626, auc: 0.97323, f1 score: 0.65663, speed: 0.04 step/s\n",
      "global step 6620, epoch: 5, batch: 512, loss: 0.06464, auc: 0.97096, f1 score: 0.67794, speed: 0.24 step/s\n",
      "global step 6630, epoch: 5, batch: 522, loss: 0.08588, auc: 0.97108, f1 score: 0.67544, speed: 0.23 step/s\n",
      "global step 6640, epoch: 5, batch: 532, loss: 0.05374, auc: 0.97236, f1 score: 0.68462, speed: 0.22 step/s\n",
      "eval loss: 0.09135, auc: 0.94617, f1 score: 0.57559, precison: 0.57076, recall: 0.58050\n",
      "global step 6650, epoch: 5, batch: 542, loss: 0.07098, auc: 0.97116, f1 score: 0.68005, speed: 0.04 step/s\n",
      "global step 6660, epoch: 5, batch: 552, loss: 0.05548, auc: 0.97356, f1 score: 0.69231, speed: 0.23 step/s\n",
      "global step 6670, epoch: 5, batch: 562, loss: 0.07589, auc: 0.97497, f1 score: 0.68827, speed: 0.23 step/s\n",
      "global step 6680, epoch: 5, batch: 572, loss: 0.06727, auc: 0.97249, f1 score: 0.67308, speed: 0.22 step/s\n",
      "eval loss: 0.09035, auc: 0.94711, f1 score: 0.59163, precison: 0.56470, recall: 0.62127\n",
      "global step 6690, epoch: 5, batch: 582, loss: 0.09145, auc: 0.96907, f1 score: 0.68012, speed: 0.04 step/s\n",
      "global step 6700, epoch: 5, batch: 592, loss: 0.08629, auc: 0.97063, f1 score: 0.67466, speed: 0.25 step/s\n",
      "global step 6710, epoch: 5, batch: 602, loss: 0.07983, auc: 0.96962, f1 score: 0.68212, speed: 0.23 step/s\n",
      "global step 6720, epoch: 5, batch: 612, loss: 0.05948, auc: 0.97136, f1 score: 0.68412, speed: 0.23 step/s\n",
      "eval loss: 0.09093, auc: 0.94712, f1 score: 0.58976, precison: 0.57023, recall: 0.61068\n",
      "global step 6730, epoch: 5, batch: 622, loss: 0.07288, auc: 0.96668, f1 score: 0.64319, speed: 0.04 step/s\n",
      "global step 6740, epoch: 5, batch: 632, loss: 0.06960, auc: 0.96566, f1 score: 0.64512, speed: 0.24 step/s\n",
      "global step 6750, epoch: 5, batch: 642, loss: 0.08283, auc: 0.96521, f1 score: 0.64693, speed: 0.22 step/s\n",
      "global step 6760, epoch: 5, batch: 652, loss: 0.06677, auc: 0.96550, f1 score: 0.64505, speed: 0.23 step/s\n",
      "eval loss: 0.09102, auc: 0.94741, f1 score: 0.58891, precison: 0.55023, recall: 0.63343\n",
      "global step 6770, epoch: 5, batch: 662, loss: 0.06074, auc: 0.96849, f1 score: 0.67697, speed: 0.04 step/s\n",
      "global step 6780, epoch: 5, batch: 672, loss: 0.05406, auc: 0.96956, f1 score: 0.66851, speed: 0.24 step/s\n",
      "global step 6790, epoch: 5, batch: 682, loss: 0.06398, auc: 0.97134, f1 score: 0.67559, speed: 0.24 step/s\n",
      "global step 6800, epoch: 5, batch: 692, loss: 0.05726, auc: 0.97252, f1 score: 0.67289, speed: 0.23 step/s\n",
      "eval loss: 0.08989, auc: 0.94858, f1 score: 0.58259, precison: 0.57523, recall: 0.59014\n",
      "global step 6810, epoch: 5, batch: 702, loss: 0.08634, auc: 0.96785, f1 score: 0.68050, speed: 0.04 step/s\n",
      "global step 6820, epoch: 5, batch: 712, loss: 0.07962, auc: 0.96861, f1 score: 0.67409, speed: 0.24 step/s\n",
      "global step 6830, epoch: 5, batch: 722, loss: 0.08278, auc: 0.96926, f1 score: 0.67301, speed: 0.22 step/s\n",
      "global step 6840, epoch: 5, batch: 732, loss: 0.06749, auc: 0.97143, f1 score: 0.67939, speed: 0.23 step/s\n",
      "eval loss: 0.09176, auc: 0.94600, f1 score: 0.58575, precison: 0.55855, recall: 0.61574\n",
      "global step 6850, epoch: 5, batch: 742, loss: 0.06199, auc: 0.96712, f1 score: 0.70408, speed: 0.04 step/s\n",
      "global step 6860, epoch: 5, batch: 752, loss: 0.06575, auc: 0.97176, f1 score: 0.68425, speed: 0.24 step/s\n",
      "global step 6870, epoch: 5, batch: 762, loss: 0.08156, auc: 0.97221, f1 score: 0.68680, speed: 0.23 step/s\n",
      "global step 6880, epoch: 5, batch: 772, loss: 0.06362, auc: 0.97346, f1 score: 0.69568, speed: 0.23 step/s\n",
      "eval loss: 0.09277, auc: 0.94469, f1 score: 0.57138, precison: 0.55961, recall: 0.58366\n",
      "global step 6890, epoch: 5, batch: 782, loss: 0.09577, auc: 0.96871, f1 score: 0.68365, speed: 0.04 step/s\n",
      "global step 6900, epoch: 5, batch: 792, loss: 0.07999, auc: 0.96908, f1 score: 0.68155, speed: 0.24 step/s\n",
      "global step 6910, epoch: 5, batch: 802, loss: 0.06180, auc: 0.96957, f1 score: 0.69070, speed: 0.23 step/s\n",
      "global step 6920, epoch: 5, batch: 812, loss: 0.06945, auc: 0.96941, f1 score: 0.68327, speed: 0.23 step/s\n",
      "eval loss: 0.09125, auc: 0.94708, f1 score: 0.58450, precison: 0.57373, recall: 0.59567\n",
      "global step 6930, epoch: 5, batch: 822, loss: 0.05724, auc: 0.96880, f1 score: 0.64536, speed: 0.04 step/s\n",
      "global step 6940, epoch: 5, batch: 832, loss: 0.05930, auc: 0.96991, f1 score: 0.66102, speed: 0.24 step/s\n",
      "global step 6950, epoch: 5, batch: 842, loss: 0.07745, auc: 0.97123, f1 score: 0.66695, speed: 0.24 step/s\n",
      "global step 6960, epoch: 5, batch: 852, loss: 0.06184, auc: 0.97243, f1 score: 0.67033, speed: 0.23 step/s\n",
      "eval loss: 0.09060, auc: 0.94689, f1 score: 0.58536, precison: 0.56353, recall: 0.60894\n",
      "global step 6970, epoch: 5, batch: 862, loss: 0.05609, auc: 0.97322, f1 score: 0.69666, speed: 0.04 step/s\n",
      "global step 6980, epoch: 5, batch: 872, loss: 0.08507, auc: 0.96993, f1 score: 0.67634, speed: 0.23 step/s\n",
      "global step 6990, epoch: 5, batch: 882, loss: 0.07484, auc: 0.97042, f1 score: 0.67343, speed: 0.23 step/s\n",
      "global step 7000, epoch: 5, batch: 892, loss: 0.08169, auc: 0.97009, f1 score: 0.66817, speed: 0.22 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.08942, auc: 0.94880, f1 score: 0.58556, precison: 0.56472, recall: 0.60799\n",
      "global step 7010, epoch: 5, batch: 902, loss: 0.07339, auc: 0.97425, f1 score: 0.70839, speed: 0.04 step/s\n",
      "global step 7020, epoch: 5, batch: 912, loss: 0.08032, auc: 0.97172, f1 score: 0.68239, speed: 0.23 step/s\n",
      "global step 7030, epoch: 5, batch: 922, loss: 0.08371, auc: 0.97111, f1 score: 0.67987, speed: 0.24 step/s\n",
      "global step 7040, epoch: 5, batch: 932, loss: 0.09725, auc: 0.97033, f1 score: 0.67844, speed: 0.23 step/s\n",
      "eval loss: 0.09167, auc: 0.94661, f1 score: 0.57647, precison: 0.53888, recall: 0.61969\n",
      "global step 7050, epoch: 5, batch: 942, loss: 0.09844, auc: 0.96544, f1 score: 0.67202, speed: 0.04 step/s\n",
      "global step 7060, epoch: 5, batch: 952, loss: 0.07390, auc: 0.97013, f1 score: 0.67351, speed: 0.24 step/s\n",
      "global step 7070, epoch: 5, batch: 962, loss: 0.07489, auc: 0.97105, f1 score: 0.68399, speed: 0.22 step/s\n",
      "global step 7080, epoch: 5, batch: 972, loss: 0.06502, auc: 0.97050, f1 score: 0.68001, speed: 0.23 step/s\n",
      "eval loss: 0.09064, auc: 0.94815, f1 score: 0.58880, precison: 0.56518, recall: 0.61447\n",
      "global step 7090, epoch: 5, batch: 982, loss: 0.07366, auc: 0.97631, f1 score: 0.67516, speed: 0.04 step/s\n",
      "global step 7100, epoch: 5, batch: 992, loss: 0.07204, auc: 0.97329, f1 score: 0.67636, speed: 0.24 step/s\n",
      "global step 7110, epoch: 5, batch: 1002, loss: 0.08686, auc: 0.97213, f1 score: 0.66924, speed: 0.23 step/s\n",
      "global step 7120, epoch: 5, batch: 1012, loss: 0.09242, auc: 0.97116, f1 score: 0.67116, speed: 0.22 step/s\n",
      "eval loss: 0.09027, auc: 0.94821, f1 score: 0.58647, precison: 0.55882, recall: 0.61700\n",
      "global step 7130, epoch: 5, batch: 1022, loss: 0.06408, auc: 0.96889, f1 score: 0.66476, speed: 0.04 step/s\n",
      "global step 7140, epoch: 5, batch: 1032, loss: 0.08469, auc: 0.97258, f1 score: 0.66762, speed: 0.25 step/s\n",
      "global step 7150, epoch: 5, batch: 1042, loss: 0.10839, auc: 0.97199, f1 score: 0.67284, speed: 0.24 step/s\n",
      "global step 7160, epoch: 5, batch: 1052, loss: 0.07505, auc: 0.97174, f1 score: 0.66884, speed: 0.23 step/s\n",
      "eval loss: 0.09155, auc: 0.94657, f1 score: 0.58853, precison: 0.55881, recall: 0.62158\n",
      "global step 7170, epoch: 5, batch: 1062, loss: 0.05819, auc: 0.97660, f1 score: 0.68047, speed: 0.04 step/s\n",
      "global step 7180, epoch: 5, batch: 1072, loss: 0.05786, auc: 0.97396, f1 score: 0.68758, speed: 0.23 step/s\n",
      "global step 7190, epoch: 5, batch: 1082, loss: 0.05641, auc: 0.97426, f1 score: 0.68742, speed: 0.24 step/s\n",
      "global step 7200, epoch: 5, batch: 1092, loss: 0.06860, auc: 0.97326, f1 score: 0.68591, speed: 0.23 step/s\n",
      "eval loss: 0.09109, auc: 0.94762, f1 score: 0.58295, precison: 0.58835, recall: 0.57766\n",
      "global step 7210, epoch: 5, batch: 1102, loss: 0.07236, auc: 0.96928, f1 score: 0.69063, speed: 0.04 step/s\n",
      "global step 7220, epoch: 5, batch: 1112, loss: 0.06295, auc: 0.97147, f1 score: 0.68807, speed: 0.23 step/s\n",
      "global step 7230, epoch: 5, batch: 1122, loss: 0.07385, auc: 0.97189, f1 score: 0.68851, speed: 0.24 step/s\n",
      "global step 7240, epoch: 5, batch: 1132, loss: 0.08935, auc: 0.97050, f1 score: 0.68045, speed: 0.23 step/s\n",
      "eval loss: 0.09124, auc: 0.94677, f1 score: 0.58614, precison: 0.58881, recall: 0.58350\n",
      "global step 7250, epoch: 5, batch: 1142, loss: 0.06421, auc: 0.97699, f1 score: 0.72208, speed: 0.04 step/s\n",
      "global step 7260, epoch: 5, batch: 1152, loss: 0.06153, auc: 0.97217, f1 score: 0.68906, speed: 0.24 step/s\n",
      "global step 7270, epoch: 5, batch: 1162, loss: 0.05367, auc: 0.97266, f1 score: 0.68919, speed: 0.24 step/s\n",
      "global step 7280, epoch: 5, batch: 1172, loss: 0.06134, auc: 0.97249, f1 score: 0.68689, speed: 0.23 step/s\n",
      "eval loss: 0.08976, auc: 0.94911, f1 score: 0.58326, precison: 0.57078, recall: 0.59630\n",
      "global step 7290, epoch: 5, batch: 1182, loss: 0.07686, auc: 0.96842, f1 score: 0.68449, speed: 0.04 step/s\n",
      "global step 7300, epoch: 5, batch: 1192, loss: 0.06854, auc: 0.97087, f1 score: 0.68034, speed: 0.24 step/s\n",
      "global step 7310, epoch: 5, batch: 1202, loss: 0.08460, auc: 0.96859, f1 score: 0.67956, speed: 0.23 step/s\n",
      "global step 7320, epoch: 5, batch: 1212, loss: 0.07133, auc: 0.96956, f1 score: 0.68423, speed: 0.22 step/s\n",
      "eval loss: 0.09078, auc: 0.94786, f1 score: 0.58399, precison: 0.56317, recall: 0.60641\n",
      "global step 7330, epoch: 5, batch: 1222, loss: 0.07099, auc: 0.97250, f1 score: 0.69638, speed: 0.04 step/s\n",
      "global step 7340, epoch: 5, batch: 1232, loss: 0.08010, auc: 0.97154, f1 score: 0.68141, speed: 0.23 step/s\n",
      "global step 7350, epoch: 5, batch: 1242, loss: 0.07076, auc: 0.97115, f1 score: 0.68741, speed: 0.23 step/s\n",
      "global step 7360, epoch: 5, batch: 1252, loss: 0.09683, auc: 0.97123, f1 score: 0.68229, speed: 0.23 step/s\n",
      "eval loss: 0.09117, auc: 0.94772, f1 score: 0.58265, precison: 0.57701, recall: 0.58840\n",
      "global step 7370, epoch: 5, batch: 1262, loss: 0.07567, auc: 0.96701, f1 score: 0.66840, speed: 0.04 step/s\n",
      "global step 7380, epoch: 5, batch: 1272, loss: 0.08594, auc: 0.96867, f1 score: 0.68299, speed: 0.22 step/s\n",
      "global step 7390, epoch: 5, batch: 1282, loss: 0.07785, auc: 0.96798, f1 score: 0.67245, speed: 0.23 step/s\n",
      "global step 7400, epoch: 5, batch: 1292, loss: 0.07070, auc: 0.96890, f1 score: 0.67150, speed: 0.22 step/s\n",
      "eval loss: 0.09031, auc: 0.94816, f1 score: 0.58088, precison: 0.55969, recall: 0.60373\n",
      "global step 7410, epoch: 5, batch: 1302, loss: 0.08036, auc: 0.97386, f1 score: 0.67903, speed: 0.04 step/s\n",
      "global step 7420, epoch: 5, batch: 1312, loss: 0.06309, auc: 0.96899, f1 score: 0.66626, speed: 0.24 step/s\n",
      "global step 7430, epoch: 5, batch: 1322, loss: 0.07566, auc: 0.96884, f1 score: 0.66230, speed: 0.23 step/s\n",
      "global step 7440, epoch: 5, batch: 1332, loss: 0.04821, auc: 0.96908, f1 score: 0.67190, speed: 0.23 step/s\n",
      "eval loss: 0.09133, auc: 0.94733, f1 score: 0.57616, precison: 0.55035, recall: 0.60452\n",
      "global step 7450, epoch: 5, batch: 1342, loss: 0.07271, auc: 0.97306, f1 score: 0.67391, speed: 0.04 step/s\n",
      "global step 7460, epoch: 5, batch: 1352, loss: 0.06649, auc: 0.97394, f1 score: 0.67657, speed: 0.23 step/s\n",
      "global step 7470, epoch: 5, batch: 1362, loss: 0.06869, auc: 0.97365, f1 score: 0.67793, speed: 0.24 step/s\n",
      "global step 7480, epoch: 5, batch: 1372, loss: 0.06041, auc: 0.97365, f1 score: 0.67514, speed: 0.22 step/s\n",
      "eval loss: 0.08991, auc: 0.94816, f1 score: 0.59064, precison: 0.58327, recall: 0.59820\n",
      "global step 7490, epoch: 5, batch: 1382, loss: 0.07962, auc: 0.97565, f1 score: 0.68717, speed: 0.04 step/s\n",
      "global step 7500, epoch: 5, batch: 1392, loss: 0.09538, auc: 0.97477, f1 score: 0.69291, speed: 0.23 step/s\n",
      "global step 7510, epoch: 5, batch: 1402, loss: 0.07623, auc: 0.97323, f1 score: 0.67946, speed: 0.23 step/s\n",
      "global step 7520, epoch: 5, batch: 1412, loss: 0.08917, auc: 0.97236, f1 score: 0.67561, speed: 0.23 step/s\n",
      "eval loss: 0.09015, auc: 0.94810, f1 score: 0.58427, precison: 0.58298, recall: 0.58556\n",
      "global step 7530, epoch: 5, batch: 1422, loss: 0.07008, auc: 0.97262, f1 score: 0.67497, speed: 0.04 step/s\n",
      "global step 7540, epoch: 5, batch: 1432, loss: 0.06076, auc: 0.97504, f1 score: 0.67756, speed: 0.23 step/s\n",
      "global step 7550, epoch: 5, batch: 1442, loss: 0.07237, auc: 0.97357, f1 score: 0.67425, speed: 0.22 step/s\n",
      "global step 7560, epoch: 5, batch: 1452, loss: 0.09069, auc: 0.97178, f1 score: 0.67677, speed: 0.23 step/s\n",
      "eval loss: 0.09129, auc: 0.94713, f1 score: 0.58071, precison: 0.56269, recall: 0.59994\n",
      "global step 7570, epoch: 5, batch: 1462, loss: 0.06939, auc: 0.97402, f1 score: 0.70039, speed: 0.04 step/s\n",
      "global step 7580, epoch: 5, batch: 1472, loss: 0.04357, auc: 0.97287, f1 score: 0.68754, speed: 0.23 step/s\n",
      "global step 7590, epoch: 5, batch: 1482, loss: 0.07115, auc: 0.97162, f1 score: 0.68582, speed: 0.23 step/s\n",
      "global step 7600, epoch: 5, batch: 1492, loss: 0.05611, auc: 0.97080, f1 score: 0.67880, speed: 0.23 step/s\n",
      "eval loss: 0.09123, auc: 0.94818, f1 score: 0.57080, precison: 0.55076, recall: 0.59235\n",
      "global step 7610, epoch: 5, batch: 1502, loss: 0.05706, auc: 0.97171, f1 score: 0.67112, speed: 0.04 step/s\n",
      "global step 7620, epoch: 5, batch: 1512, loss: 0.07434, auc: 0.97066, f1 score: 0.67145, speed: 0.23 step/s\n",
      "global step 7630, epoch: 5, batch: 1522, loss: 0.06323, auc: 0.97119, f1 score: 0.67790, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7640, epoch: 6, batch: 5, loss: 0.06746, auc: 0.96964, f1 score: 0.67040, speed: 0.24 step/s\n",
      "eval loss: 0.08972, auc: 0.94905, f1 score: 0.59175, precison: 0.56996, recall: 0.61526\n",
      "global step 7650, epoch: 6, batch: 15, loss: 0.06324, auc: 0.97588, f1 score: 0.71009, speed: 0.04 step/s\n",
      "global step 7660, epoch: 6, batch: 25, loss: 0.06537, auc: 0.97756, f1 score: 0.71042, speed: 0.24 step/s\n",
      "global step 7670, epoch: 6, batch: 35, loss: 0.05601, auc: 0.97685, f1 score: 0.71756, speed: 0.23 step/s\n",
      "global step 7680, epoch: 6, batch: 45, loss: 0.09013, auc: 0.97613, f1 score: 0.71759, speed: 0.23 step/s\n",
      "eval loss: 0.09263, auc: 0.94594, f1 score: 0.58220, precison: 0.59228, recall: 0.57244\n",
      "global step 7690, epoch: 6, batch: 55, loss: 0.06813, auc: 0.97212, f1 score: 0.69463, speed: 0.04 step/s\n",
      "global step 7700, epoch: 6, batch: 65, loss: 0.04149, auc: 0.97672, f1 score: 0.71906, speed: 0.24 step/s\n",
      "global step 7710, epoch: 6, batch: 75, loss: 0.05336, auc: 0.97851, f1 score: 0.73934, speed: 0.23 step/s\n",
      "global step 7720, epoch: 6, batch: 85, loss: 0.04155, auc: 0.97968, f1 score: 0.74811, speed: 0.22 step/s\n",
      "eval loss: 0.09598, auc: 0.94413, f1 score: 0.58230, precison: 0.57141, recall: 0.59362\n",
      "global step 7730, epoch: 6, batch: 95, loss: 0.06100, auc: 0.98195, f1 score: 0.74218, speed: 0.04 step/s\n",
      "global step 7740, epoch: 6, batch: 105, loss: 0.07868, auc: 0.97810, f1 score: 0.73907, speed: 0.24 step/s\n",
      "global step 7750, epoch: 6, batch: 115, loss: 0.06709, auc: 0.98012, f1 score: 0.74133, speed: 0.22 step/s\n",
      "global step 7760, epoch: 6, batch: 125, loss: 0.05620, auc: 0.97987, f1 score: 0.73558, speed: 0.23 step/s\n",
      "eval loss: 0.09253, auc: 0.94772, f1 score: 0.58307, precison: 0.57084, recall: 0.59583\n",
      "global step 7770, epoch: 6, batch: 135, loss: 0.07886, auc: 0.97646, f1 score: 0.69263, speed: 0.04 step/s\n",
      "global step 7780, epoch: 6, batch: 145, loss: 0.05722, auc: 0.97776, f1 score: 0.70767, speed: 0.25 step/s\n",
      "global step 7790, epoch: 6, batch: 155, loss: 0.05462, auc: 0.97902, f1 score: 0.71978, speed: 0.21 step/s\n",
      "global step 7800, epoch: 6, batch: 165, loss: 0.06315, auc: 0.97965, f1 score: 0.72088, speed: 0.22 step/s\n",
      "eval loss: 0.09573, auc: 0.94352, f1 score: 0.56312, precison: 0.55132, recall: 0.57545\n",
      "global step 7810, epoch: 6, batch: 175, loss: 0.05800, auc: 0.97916, f1 score: 0.72583, speed: 0.04 step/s\n",
      "global step 7820, epoch: 6, batch: 185, loss: 0.06381, auc: 0.97685, f1 score: 0.72254, speed: 0.24 step/s\n",
      "global step 7830, epoch: 6, batch: 195, loss: 0.06460, auc: 0.97717, f1 score: 0.71349, speed: 0.22 step/s\n",
      "global step 7840, epoch: 6, batch: 205, loss: 0.05790, auc: 0.97630, f1 score: 0.71022, speed: 0.23 step/s\n",
      "eval loss: 0.09428, auc: 0.94489, f1 score: 0.57638, precison: 0.59097, recall: 0.56249\n",
      "global step 7850, epoch: 6, batch: 215, loss: 0.06821, auc: 0.97876, f1 score: 0.72977, speed: 0.04 step/s\n",
      "global step 7860, epoch: 6, batch: 225, loss: 0.05721, auc: 0.98028, f1 score: 0.72866, speed: 0.24 step/s\n",
      "global step 7870, epoch: 6, batch: 235, loss: 0.07550, auc: 0.97842, f1 score: 0.71216, speed: 0.23 step/s\n",
      "global step 7880, epoch: 6, batch: 245, loss: 0.05628, auc: 0.97942, f1 score: 0.71939, speed: 0.22 step/s\n",
      "eval loss: 0.09395, auc: 0.94561, f1 score: 0.57524, precison: 0.57129, recall: 0.57924\n",
      "global step 7890, epoch: 6, batch: 255, loss: 0.06587, auc: 0.98328, f1 score: 0.74720, speed: 0.04 step/s\n",
      "global step 7900, epoch: 6, batch: 265, loss: 0.07359, auc: 0.98155, f1 score: 0.74592, speed: 0.23 step/s\n",
      "global step 7910, epoch: 6, batch: 275, loss: 0.04609, auc: 0.98191, f1 score: 0.74198, speed: 0.23 step/s\n",
      "global step 7920, epoch: 6, batch: 285, loss: 0.07820, auc: 0.98079, f1 score: 0.73866, speed: 0.22 step/s\n",
      "eval loss: 0.09435, auc: 0.94494, f1 score: 0.57912, precison: 0.54452, recall: 0.61842\n",
      "global step 7930, epoch: 6, batch: 295, loss: 0.05923, auc: 0.97968, f1 score: 0.72651, speed: 0.04 step/s\n",
      "global step 7940, epoch: 6, batch: 305, loss: 0.06115, auc: 0.97880, f1 score: 0.71955, speed: 0.23 step/s\n",
      "global step 7950, epoch: 6, batch: 315, loss: 0.06689, auc: 0.97788, f1 score: 0.71703, speed: 0.23 step/s\n",
      "global step 7960, epoch: 6, batch: 325, loss: 0.04691, auc: 0.97836, f1 score: 0.71683, speed: 0.23 step/s\n",
      "eval loss: 0.09342, auc: 0.94529, f1 score: 0.57469, precison: 0.57712, recall: 0.57229\n",
      "global step 7970, epoch: 6, batch: 335, loss: 0.07003, auc: 0.97939, f1 score: 0.71148, speed: 0.04 step/s\n",
      "global step 7980, epoch: 6, batch: 345, loss: 0.05122, auc: 0.98081, f1 score: 0.70771, speed: 0.22 step/s\n",
      "global step 7990, epoch: 6, batch: 355, loss: 0.06972, auc: 0.97905, f1 score: 0.69624, speed: 0.22 step/s\n",
      "global step 8000, epoch: 6, batch: 365, loss: 0.06762, auc: 0.97909, f1 score: 0.70569, speed: 0.22 step/s\n",
      "eval loss: 0.09571, auc: 0.94486, f1 score: 0.57215, precison: 0.55635, recall: 0.58888\n",
      "global step 8010, epoch: 6, batch: 375, loss: 0.06052, auc: 0.97954, f1 score: 0.74488, speed: 0.04 step/s\n",
      "global step 8020, epoch: 6, batch: 385, loss: 0.07234, auc: 0.97687, f1 score: 0.73329, speed: 0.22 step/s\n",
      "global step 8030, epoch: 6, batch: 395, loss: 0.04399, auc: 0.97702, f1 score: 0.72516, speed: 0.22 step/s\n",
      "global step 8040, epoch: 6, batch: 405, loss: 0.06426, auc: 0.97731, f1 score: 0.71687, speed: 0.23 step/s\n",
      "eval loss: 0.09375, auc: 0.94625, f1 score: 0.57335, precison: 0.54982, recall: 0.59899\n",
      "global step 8050, epoch: 6, batch: 415, loss: 0.06356, auc: 0.98087, f1 score: 0.72900, speed: 0.04 step/s\n",
      "global step 8060, epoch: 6, batch: 425, loss: 0.06112, auc: 0.97664, f1 score: 0.72488, speed: 0.24 step/s\n",
      "global step 8070, epoch: 6, batch: 435, loss: 0.05756, auc: 0.97545, f1 score: 0.72963, speed: 0.24 step/s\n",
      "global step 8080, epoch: 6, batch: 445, loss: 0.04697, auc: 0.97648, f1 score: 0.72776, speed: 0.23 step/s\n",
      "eval loss: 0.09479, auc: 0.94450, f1 score: 0.57601, precison: 0.55047, recall: 0.60404\n",
      "global step 8090, epoch: 6, batch: 455, loss: 0.06663, auc: 0.97454, f1 score: 0.70027, speed: 0.04 step/s\n",
      "global step 8100, epoch: 6, batch: 465, loss: 0.06834, auc: 0.97404, f1 score: 0.69899, speed: 0.23 step/s\n",
      "global step 8110, epoch: 6, batch: 475, loss: 0.05093, auc: 0.97511, f1 score: 0.70615, speed: 0.23 step/s\n",
      "global step 8120, epoch: 6, batch: 485, loss: 0.05316, auc: 0.97590, f1 score: 0.71160, speed: 0.21 step/s\n",
      "eval loss: 0.09408, auc: 0.94571, f1 score: 0.57005, precison: 0.54443, recall: 0.59820\n",
      "global step 8130, epoch: 6, batch: 495, loss: 0.05765, auc: 0.97996, f1 score: 0.69340, speed: 0.04 step/s\n",
      "global step 8140, epoch: 6, batch: 505, loss: 0.06086, auc: 0.97914, f1 score: 0.71257, speed: 0.24 step/s\n",
      "global step 8150, epoch: 6, batch: 515, loss: 0.06779, auc: 0.97936, f1 score: 0.72604, speed: 0.24 step/s\n",
      "global step 8160, epoch: 6, batch: 525, loss: 0.06530, auc: 0.97739, f1 score: 0.71987, speed: 0.21 step/s\n",
      "eval loss: 0.09428, auc: 0.94637, f1 score: 0.56697, precison: 0.56282, recall: 0.57118\n",
      "global step 8170, epoch: 6, batch: 535, loss: 0.05557, auc: 0.98251, f1 score: 0.71953, speed: 0.04 step/s\n",
      "global step 8180, epoch: 6, batch: 545, loss: 0.04462, auc: 0.97932, f1 score: 0.71358, speed: 0.23 step/s\n",
      "global step 8190, epoch: 6, batch: 555, loss: 0.08838, auc: 0.97721, f1 score: 0.70187, speed: 0.23 step/s\n",
      "global step 8200, epoch: 6, batch: 565, loss: 0.07687, auc: 0.97781, f1 score: 0.70584, speed: 0.24 step/s\n",
      "eval loss: 0.09461, auc: 0.94499, f1 score: 0.57674, precison: 0.57789, recall: 0.57560\n",
      "global step 8210, epoch: 6, batch: 575, loss: 0.07490, auc: 0.97876, f1 score: 0.72340, speed: 0.04 step/s\n",
      "global step 8220, epoch: 6, batch: 585, loss: 0.07696, auc: 0.97819, f1 score: 0.73320, speed: 0.24 step/s\n",
      "global step 8230, epoch: 6, batch: 595, loss: 0.05085, auc: 0.97739, f1 score: 0.72744, speed: 0.23 step/s\n",
      "global step 8240, epoch: 6, batch: 605, loss: 0.06079, auc: 0.97760, f1 score: 0.72137, speed: 0.23 step/s\n",
      "eval loss: 0.09585, auc: 0.94469, f1 score: 0.56237, precison: 0.53854, recall: 0.58840\n",
      "global step 8250, epoch: 6, batch: 615, loss: 0.07341, auc: 0.97933, f1 score: 0.72624, speed: 0.04 step/s\n",
      "global step 8260, epoch: 6, batch: 625, loss: 0.06439, auc: 0.97853, f1 score: 0.72666, speed: 0.24 step/s\n",
      "global step 8270, epoch: 6, batch: 635, loss: 0.07084, auc: 0.97871, f1 score: 0.71982, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8280, epoch: 6, batch: 645, loss: 0.08299, auc: 0.97508, f1 score: 0.70331, speed: 0.23 step/s\n",
      "eval loss: 0.09513, auc: 0.94519, f1 score: 0.56465, precison: 0.53917, recall: 0.59267\n",
      "global step 8290, epoch: 6, batch: 655, loss: 0.04748, auc: 0.97428, f1 score: 0.67513, speed: 0.04 step/s\n",
      "global step 8300, epoch: 6, batch: 665, loss: 0.05666, auc: 0.97867, f1 score: 0.69686, speed: 0.22 step/s\n",
      "global step 8310, epoch: 6, batch: 675, loss: 0.04745, auc: 0.97850, f1 score: 0.69722, speed: 0.25 step/s\n",
      "global step 8320, epoch: 6, batch: 685, loss: 0.08808, auc: 0.97749, f1 score: 0.70199, speed: 0.22 step/s\n",
      "eval loss: 0.09595, auc: 0.94500, f1 score: 0.57610, precison: 0.55516, recall: 0.59867\n",
      "global step 8330, epoch: 6, batch: 695, loss: 0.06370, auc: 0.98045, f1 score: 0.71351, speed: 0.04 step/s\n",
      "global step 8340, epoch: 6, batch: 705, loss: 0.06247, auc: 0.98235, f1 score: 0.71410, speed: 0.24 step/s\n",
      "global step 8350, epoch: 6, batch: 715, loss: 0.06358, auc: 0.98266, f1 score: 0.72551, speed: 0.24 step/s\n",
      "global step 8360, epoch: 6, batch: 725, loss: 0.06908, auc: 0.98114, f1 score: 0.72583, speed: 0.23 step/s\n",
      "eval loss: 0.09552, auc: 0.94527, f1 score: 0.57576, precison: 0.55305, recall: 0.60041\n",
      "global step 8370, epoch: 6, batch: 735, loss: 0.04642, auc: 0.97412, f1 score: 0.73113, speed: 0.04 step/s\n",
      "global step 8380, epoch: 6, batch: 745, loss: 0.07044, auc: 0.97700, f1 score: 0.72668, speed: 0.25 step/s\n",
      "global step 8390, epoch: 6, batch: 755, loss: 0.06389, auc: 0.97668, f1 score: 0.71581, speed: 0.23 step/s\n",
      "global step 8400, epoch: 6, batch: 765, loss: 0.06494, auc: 0.97666, f1 score: 0.71337, speed: 0.23 step/s\n",
      "eval loss: 0.09454, auc: 0.94506, f1 score: 0.57036, precison: 0.56057, recall: 0.58050\n",
      "global step 8410, epoch: 6, batch: 775, loss: 0.05781, auc: 0.97363, f1 score: 0.73569, speed: 0.04 step/s\n",
      "global step 8420, epoch: 6, batch: 785, loss: 0.04773, auc: 0.97734, f1 score: 0.72889, speed: 0.24 step/s\n",
      "global step 8430, epoch: 6, batch: 795, loss: 0.05993, auc: 0.97789, f1 score: 0.72802, speed: 0.23 step/s\n",
      "global step 8440, epoch: 6, batch: 805, loss: 0.06542, auc: 0.97895, f1 score: 0.73177, speed: 0.23 step/s\n",
      "eval loss: 0.09411, auc: 0.94628, f1 score: 0.57013, precison: 0.53932, recall: 0.60468\n",
      "global step 8450, epoch: 6, batch: 815, loss: 0.07070, auc: 0.97924, f1 score: 0.72727, speed: 0.04 step/s\n",
      "global step 8460, epoch: 6, batch: 825, loss: 0.07125, auc: 0.97803, f1 score: 0.70775, speed: 0.24 step/s\n",
      "global step 8470, epoch: 6, batch: 835, loss: 0.06937, auc: 0.97570, f1 score: 0.70379, speed: 0.24 step/s\n",
      "global step 8480, epoch: 6, batch: 845, loss: 0.06078, auc: 0.97665, f1 score: 0.71223, speed: 0.23 step/s\n",
      "eval loss: 0.09429, auc: 0.94595, f1 score: 0.56619, precison: 0.56610, recall: 0.56628\n",
      "global step 8490, epoch: 6, batch: 855, loss: 0.05241, auc: 0.98090, f1 score: 0.75000, speed: 0.04 step/s\n",
      "global step 8500, epoch: 6, batch: 865, loss: 0.05576, auc: 0.97771, f1 score: 0.73082, speed: 0.25 step/s\n",
      "global step 8510, epoch: 6, batch: 875, loss: 0.06429, auc: 0.97750, f1 score: 0.72004, speed: 0.24 step/s\n",
      "global step 8520, epoch: 6, batch: 885, loss: 0.04568, auc: 0.97810, f1 score: 0.72240, speed: 0.23 step/s\n",
      "eval loss: 0.09651, auc: 0.94385, f1 score: 0.56501, precison: 0.52315, recall: 0.61416\n",
      "global step 8530, epoch: 6, batch: 895, loss: 0.04989, auc: 0.97055, f1 score: 0.71773, speed: 0.04 step/s\n",
      "global step 8540, epoch: 6, batch: 905, loss: 0.05189, auc: 0.97390, f1 score: 0.72392, speed: 0.24 step/s\n",
      "global step 8550, epoch: 6, batch: 915, loss: 0.06746, auc: 0.97485, f1 score: 0.71233, speed: 0.22 step/s\n",
      "global step 8560, epoch: 6, batch: 925, loss: 0.08248, auc: 0.97425, f1 score: 0.71390, speed: 0.22 step/s\n",
      "eval loss: 0.09611, auc: 0.94268, f1 score: 0.56832, precison: 0.57086, recall: 0.56581\n",
      "global step 8570, epoch: 6, batch: 935, loss: 0.08069, auc: 0.97865, f1 score: 0.71731, speed: 0.04 step/s\n",
      "global step 8580, epoch: 6, batch: 945, loss: 0.06875, auc: 0.97426, f1 score: 0.70845, speed: 0.23 step/s\n",
      "global step 8590, epoch: 6, batch: 955, loss: 0.05262, auc: 0.97522, f1 score: 0.70286, speed: 0.23 step/s\n",
      "global step 8600, epoch: 6, batch: 965, loss: 0.05631, auc: 0.97547, f1 score: 0.70210, speed: 0.23 step/s\n",
      "eval loss: 0.09596, auc: 0.94446, f1 score: 0.56657, precison: 0.54989, recall: 0.58429\n",
      "global step 8610, epoch: 6, batch: 975, loss: 0.03463, auc: 0.97510, f1 score: 0.73185, speed: 0.04 step/s\n",
      "global step 8620, epoch: 6, batch: 985, loss: 0.07867, auc: 0.97845, f1 score: 0.72512, speed: 0.24 step/s\n",
      "global step 8630, epoch: 6, batch: 995, loss: 0.04586, auc: 0.97936, f1 score: 0.72271, speed: 0.24 step/s\n",
      "global step 8640, epoch: 6, batch: 1005, loss: 0.05294, auc: 0.97863, f1 score: 0.71499, speed: 0.23 step/s\n",
      "eval loss: 0.09521, auc: 0.94506, f1 score: 0.56815, precison: 0.53199, recall: 0.60957\n",
      "global step 8650, epoch: 6, batch: 1015, loss: 0.06403, auc: 0.97433, f1 score: 0.69287, speed: 0.04 step/s\n",
      "global step 8660, epoch: 6, batch: 1025, loss: 0.06366, auc: 0.97357, f1 score: 0.69920, speed: 0.24 step/s\n",
      "global step 8670, epoch: 6, batch: 1035, loss: 0.05122, auc: 0.97496, f1 score: 0.70333, speed: 0.24 step/s\n",
      "global step 8680, epoch: 6, batch: 1045, loss: 0.06774, auc: 0.97468, f1 score: 0.70107, speed: 0.23 step/s\n",
      "eval loss: 0.09583, auc: 0.94420, f1 score: 0.57692, precison: 0.56647, recall: 0.58777\n",
      "global step 8690, epoch: 6, batch: 1055, loss: 0.06610, auc: 0.97610, f1 score: 0.73846, speed: 0.04 step/s\n",
      "global step 8700, epoch: 6, batch: 1065, loss: 0.04073, auc: 0.97923, f1 score: 0.72544, speed: 0.23 step/s\n",
      "global step 8710, epoch: 6, batch: 1075, loss: 0.04820, auc: 0.97756, f1 score: 0.72327, speed: 0.22 step/s\n",
      "global step 8720, epoch: 6, batch: 1085, loss: 0.05569, auc: 0.97732, f1 score: 0.71894, speed: 0.23 step/s\n",
      "eval loss: 0.09560, auc: 0.94436, f1 score: 0.58016, precison: 0.56275, recall: 0.59867\n",
      "global step 8730, epoch: 6, batch: 1095, loss: 0.08022, auc: 0.98045, f1 score: 0.73232, speed: 0.04 step/s\n",
      "global step 8740, epoch: 6, batch: 1105, loss: 0.05271, auc: 0.97888, f1 score: 0.71875, speed: 0.24 step/s\n",
      "global step 8750, epoch: 6, batch: 1115, loss: 0.06406, auc: 0.97893, f1 score: 0.72524, speed: 0.23 step/s\n",
      "global step 8760, epoch: 6, batch: 1125, loss: 0.06367, auc: 0.97858, f1 score: 0.71963, speed: 0.22 step/s\n",
      "eval loss: 0.09489, auc: 0.94463, f1 score: 0.57857, precison: 0.56004, recall: 0.59836\n",
      "global step 8770, epoch: 6, batch: 1135, loss: 0.06426, auc: 0.97538, f1 score: 0.68758, speed: 0.04 step/s\n",
      "global step 8780, epoch: 6, batch: 1145, loss: 0.05373, auc: 0.97382, f1 score: 0.69282, speed: 0.23 step/s\n",
      "global step 8790, epoch: 6, batch: 1155, loss: 0.04956, auc: 0.97630, f1 score: 0.69924, speed: 0.23 step/s\n",
      "global step 8800, epoch: 6, batch: 1165, loss: 0.06117, auc: 0.97658, f1 score: 0.69993, speed: 0.23 step/s\n",
      "eval loss: 0.09519, auc: 0.94493, f1 score: 0.58499, precison: 0.57795, recall: 0.59219\n",
      "global step 8810, epoch: 6, batch: 1175, loss: 0.08123, auc: 0.97519, f1 score: 0.73352, speed: 0.04 step/s\n",
      "global step 8820, epoch: 6, batch: 1185, loss: 0.06816, auc: 0.97714, f1 score: 0.73786, speed: 0.23 step/s\n",
      "global step 8830, epoch: 6, batch: 1195, loss: 0.04964, auc: 0.97634, f1 score: 0.72895, speed: 0.23 step/s\n",
      "global step 8840, epoch: 6, batch: 1205, loss: 0.04447, auc: 0.97567, f1 score: 0.71860, speed: 0.22 step/s\n",
      "eval loss: 0.09404, auc: 0.94565, f1 score: 0.57130, precison: 0.53854, recall: 0.60831\n",
      "global step 8850, epoch: 6, batch: 1215, loss: 0.05788, auc: 0.98154, f1 score: 0.72601, speed: 0.04 step/s\n",
      "global step 8860, epoch: 6, batch: 1225, loss: 0.05142, auc: 0.98091, f1 score: 0.73066, speed: 0.24 step/s\n",
      "global step 8870, epoch: 6, batch: 1235, loss: 0.06730, auc: 0.97881, f1 score: 0.71297, speed: 0.24 step/s\n",
      "global step 8880, epoch: 6, batch: 1245, loss: 0.05319, auc: 0.97732, f1 score: 0.71399, speed: 0.23 step/s\n",
      "eval loss: 0.09799, auc: 0.94088, f1 score: 0.54897, precison: 0.51547, recall: 0.58714\n",
      "global step 8890, epoch: 6, batch: 1255, loss: 0.05782, auc: 0.97641, f1 score: 0.68861, speed: 0.04 step/s\n",
      "global step 8900, epoch: 6, batch: 1265, loss: 0.07177, auc: 0.97906, f1 score: 0.70443, speed: 0.23 step/s\n",
      "global step 8910, epoch: 6, batch: 1275, loss: 0.07947, auc: 0.97805, f1 score: 0.70099, speed: 0.23 step/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 8920, epoch: 6, batch: 1285, loss: 0.07368, auc: 0.97770, f1 score: 0.70166, speed: 0.21 step/s\n",
      "eval loss: 0.09555, auc: 0.94526, f1 score: 0.58184, precison: 0.55455, recall: 0.61195\n",
      "global step 8930, epoch: 6, batch: 1295, loss: 0.04930, auc: 0.97924, f1 score: 0.73874, speed: 0.04 step/s\n",
      "global step 8940, epoch: 6, batch: 1305, loss: 0.08217, auc: 0.97730, f1 score: 0.70906, speed: 0.23 step/s\n",
      "global step 8950, epoch: 6, batch: 1315, loss: 0.03854, auc: 0.97826, f1 score: 0.71120, speed: 0.23 step/s\n",
      "global step 8960, epoch: 6, batch: 1325, loss: 0.07256, auc: 0.97732, f1 score: 0.70651, speed: 0.23 step/s\n",
      "eval loss: 0.09633, auc: 0.94408, f1 score: 0.56368, precison: 0.56235, recall: 0.56502\n",
      "global step 8970, epoch: 6, batch: 1335, loss: 0.05320, auc: 0.97668, f1 score: 0.69188, speed: 0.04 step/s\n",
      "global step 8980, epoch: 6, batch: 1345, loss: 0.08845, auc: 0.97431, f1 score: 0.68690, speed: 0.24 step/s\n",
      "global step 8990, epoch: 6, batch: 1355, loss: 0.05305, auc: 0.97485, f1 score: 0.68042, speed: 0.23 step/s\n",
      "global step 9000, epoch: 6, batch: 1365, loss: 0.03484, auc: 0.97671, f1 score: 0.69743, speed: 0.22 step/s\n",
      "eval loss: 0.09474, auc: 0.94421, f1 score: 0.58117, precison: 0.55490, recall: 0.61005\n",
      "global step 9010, epoch: 6, batch: 1375, loss: 0.06966, auc: 0.97640, f1 score: 0.70725, speed: 0.04 step/s\n",
      "global step 9020, epoch: 6, batch: 1385, loss: 0.07544, auc: 0.97592, f1 score: 0.70749, speed: 0.24 step/s\n",
      "global step 9030, epoch: 6, batch: 1395, loss: 0.06556, auc: 0.97583, f1 score: 0.70578, speed: 0.24 step/s\n",
      "global step 9040, epoch: 6, batch: 1405, loss: 0.04958, auc: 0.97473, f1 score: 0.70672, speed: 0.22 step/s\n",
      "eval loss: 0.09499, auc: 0.94471, f1 score: 0.57471, precison: 0.55139, recall: 0.60009\n",
      "global step 9050, epoch: 6, batch: 1415, loss: 0.05528, auc: 0.97609, f1 score: 0.69740, speed: 0.04 step/s\n",
      "global step 9060, epoch: 6, batch: 1425, loss: 0.03851, auc: 0.97831, f1 score: 0.69874, speed: 0.23 step/s\n",
      "global step 9070, epoch: 6, batch: 1435, loss: 0.07232, auc: 0.97778, f1 score: 0.69968, speed: 0.22 step/s\n",
      "global step 9080, epoch: 6, batch: 1445, loss: 0.04587, auc: 0.97748, f1 score: 0.70648, speed: 0.24 step/s\n",
      "eval loss: 0.09555, auc: 0.94480, f1 score: 0.58369, precison: 0.56383, recall: 0.60499\n",
      "global step 9090, epoch: 6, batch: 1455, loss: 0.05766, auc: 0.97743, f1 score: 0.73228, speed: 0.04 step/s\n",
      "global step 9100, epoch: 6, batch: 1465, loss: 0.07215, auc: 0.97646, f1 score: 0.71847, speed: 0.24 step/s\n",
      "global step 9110, epoch: 6, batch: 1475, loss: 0.05858, auc: 0.97701, f1 score: 0.71766, speed: 0.23 step/s\n",
      "global step 9120, epoch: 6, batch: 1485, loss: 0.04423, auc: 0.97610, f1 score: 0.71382, speed: 0.23 step/s\n",
      "eval loss: 0.09496, auc: 0.94520, f1 score: 0.56470, precison: 0.54391, recall: 0.58714\n",
      "global step 9130, epoch: 6, batch: 1495, loss: 0.05860, auc: 0.98242, f1 score: 0.71703, speed: 0.04 step/s\n",
      "global step 9140, epoch: 6, batch: 1505, loss: 0.06130, auc: 0.98159, f1 score: 0.72470, speed: 0.25 step/s\n",
      "global step 9150, epoch: 6, batch: 1515, loss: 0.07068, auc: 0.97931, f1 score: 0.71670, speed: 0.22 step/s\n",
      "global step 9160, epoch: 6, batch: 1525, loss: 0.08809, auc: 0.97891, f1 score: 0.71566, speed: 0.23 step/s\n",
      "eval loss: 0.09675, auc: 0.94308, f1 score: 0.57663, precison: 0.54847, recall: 0.60784\n"
     ]
    }
   ],
   "source": [
    "epochs = 6 # training times\n",
    "ckpt_dir = \"ernie_ckpt\" # Folder for saving model parameters during training\n",
    "\n",
    "global_step = 0  # Number of iterations\n",
    "tic_train = time.time()\n",
    "best_f1_score = 0\n",
    "\n",
    "# Model Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch['input_ids'], batch['token_type_ids'], batch['labels']\n",
    "\n",
    "        # Calculate model output, loss function value, classification probability value, accuracy, f1 score.\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        probs = F.sigmoid(logits)\n",
    "        metric.update(probs, labels)\n",
    "        auc, f1_score, _, _ = metric.accumulate()\n",
    "\n",
    "        # Print the loss function value, accuracy, f1 score, and computation speed for each 10 iterations.\n",
    "        global_step += 1\n",
    "        if global_step % 10 == 0:\n",
    "            print(\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, auc: %.5f, f1 score: %.5f, speed: %.2f step/s\"\n",
    "                % (global_step, epoch, step, loss, auc, f1_score,\n",
    "                    10 / (time.time() - tic_train)))\n",
    "            tic_train = time.time()\n",
    "        \n",
    "        # Reverse gradient passback with updated parameters.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.clear_grad()\n",
    "        \n",
    "        # Every 40 iterations, evaluate the current trained model, save the current best model parameters and word list of the word splitter, etc.\n",
    "        if global_step % 40 == 0:\n",
    "            save_dir = ckpt_dir\n",
    "            if not os.path.exists(save_dir):\n",
    "                os.makedirs(save_dir)\n",
    "            eval_f1_score = evaluate(model, criterion, metric, test_data_loader, label_vocab, if_return_results=False)\n",
    "            if eval_f1_score > best_f1_score:\n",
    "                best_f1_score = eval_f1_score\n",
    "                model.save_pretrained(save_dir)\n",
    "                tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bdef08",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad41c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERNIE 3.0 performance on GoEmotions micro-emotion 28 classification test set： eval loss: 0.08623, auc: 0.95069, f1 score: 0.60178, precison: 0.58872, recall: 0.61542\n"
     ]
    }
   ],
   "source": [
    "# Load the optimal parameters of the trained model.\n",
    "model.set_dict(paddle.load('ernie_ckpt/model_state.pdparams'))\n",
    "\n",
    "# Load the parameters of the previously trained model.\n",
    "# model.set_dict(paddle.load('/home/aistudio/work/model_state.pdparams'))\n",
    "\n",
    "# Model Validation.\n",
    "print(\"ERNIE 3.0 performance on GoEmotions micro-emotion 28 classification test set：\", end= \" \")\n",
    "results = evaluate(model, criterion, metric, test_data_loader, label_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133337e7",
   "metadata": {},
   "source": [
    "# Use the model to make predictions about the sentiment contained in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3aa3380f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loading and processing functions.\n",
    "from paddlenlp.data import JiebaTokenizer, Pad, Stack, Tuple, Vocab\n",
    "def convert_example(example, tokenizer, max_seq_length=64, is_test=False):\n",
    "    qtconcat = example[\"text\"]\n",
    "    encoded_inputs = tokenizer(text=qtconcat, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    if not is_test:\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\n",
    "        return input_ids, token_type_ids, label\n",
    "    else:\n",
    "        return input_ids, token_type_ids\n",
    "\n",
    "# Define the model prediction function.\n",
    "def predict(model, data, tokenizer, label_vocab, batch_size=1, max_seq=64):\n",
    "    examples = []\n",
    "    # Process input data (list format) into a format acceptable to the model.\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(\n",
    "            text,\n",
    "            tokenizer,\n",
    "            max_seq_length=max_seq,\n",
    "            is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    # Seperates data into some batches.\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        # The last batch whose size is less than the config batch_size setting.\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.sigmoid(logits)\n",
    "        probs = probs.tolist()\n",
    "        # The results were processed by selecting the sentiment categories with probability greater than 0.5.\n",
    "        for prob in probs:\n",
    "            result = []\n",
    "            for c, pred in enumerate(prob):\n",
    "                if pred > 0.5:\n",
    "                    result.append(label_vocab[c])\n",
    "            results.append(','.join(result))\n",
    "    return results  # Return prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a0bf9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Thats absolutely disgusting. \t Lables: disgust\n",
      "Text: Why would I do that? \t Lables: curiosity\n",
      "Text: You shut your mouth \t Lables: anger\n",
      "Text: Thank you. \t Lables: gratitude\n"
     ]
    }
   ],
   "source": [
    "# Define the text data to be subjected to micro-sentiment analysis.\n",
    "data = [\n",
    "    # 11 disgust\n",
    "    {\"text\": 'Thats absolutely disgusting.'},\n",
    "    # 7 curiosity\n",
    "    {\"text\":'Why would I do that?'},\n",
    "    # 2 anger\n",
    "    {\"text\":\"You shut your mouth\"},\n",
    "    # 15 gratitude\n",
    "    {\"text\":\"Thank you.\"}\n",
    "]\n",
    "\n",
    "# Model Predictions.\n",
    "labels =  predict(model, data, tokenizer, label_vocab, batch_size=1)\n",
    "\n",
    "# Output prediction results\n",
    "for idx, text in enumerate(data):\n",
    "    print('Text: {} \\t Lables: {}'.format(text['text'], labels[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
