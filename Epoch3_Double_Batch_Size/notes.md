# This is a note about the training of this model.
In this training, we doubled the batch_size (32 -> 64) and kept other parameters constant to explore the impact on model training.
